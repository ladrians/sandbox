{
  "dataSetId": "d4c3760a-74c1-4339-aca1-b023762376ed",
  "evaluationPlanId": "e036a498-c36f-42ec-aae7-17d812ec1e1a",
  "evaluationPlanProfileName": "geai",
  "evaluationResultChunckCount": 7,
  "evaluationResultChunckSize": "7000",
  "evaluationResultCost": "0.1339948400",
  "evaluationResultDuration": "322",
  "evaluationResultEndDate": "2025-05-13T11:17:06",
  "evaluationResultId": "d4c99f44-f25b-4475-9fba-bb534fca577f",
  "evaluationResultMaxTokens": "1503",
  "evaluationResultModelName": "gpt-4.1-mini",
  "evaluationResultProviderName": "openai",
  "evaluationResultRetrievalMethod": "multiQuery",
  "evaluationResultScoreAverage": 0.0,
  "evaluationResultStartDate": "2025-05-13T11:11:44",
  "evaluationResultStatus": "F",
  "evaluationResultTemperature": 0.0,
  "evaluationResultUploadFiles": false,
  "rows": [
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "is defined; for example, \"lastUserInput\". This variable contains the user's most recent input and will be used as the basis for the query on the selected documents or tags. Similarly, in the Output section, there is also a Variable field that specifies the variable in which the output generated by that node will be stored. In this case, we are interested in saving the RAG Assistant\u2019s response in a new variable called lastResponse. Therefore, the Use as response option is disabled. Clicking on the pencil icon to the right of the node opens the \"State Configuration\" menu where you can modify the following items: - **Acceptable confidence level**: The default value is 0.2, which indicates the level of confidence in the match of the chunks that the RAG obtains after searching the vector database. Only responses that meet or exceed this confidence level will be considered valid.\n\n- **Include conversation history**: Enabled by default. It ensures that the conversation history is included in the context of the interaction, allowing the assistant to take previous messages into account when generating responses.\n- **Include context variables**: Enabled by default. It allows context variables stored during the conversation to be sent to the assistants so that they can be used within the prompts.\n- **Show sources**: Enabled by default. It allows the assistant to show the sources of the information used to generate the response, which helps to provide transparency and to validate the accuracy of the response.\n- **Handle empty response**: Disabled by default. It handles situations where the assistant is unable to generate a valid response. When enabled, it opens a new flow branch (as shown in the image), allowing you to configure specific actions to follow in case no response is available. In this new branch, you can add any of the components available in the Interactions menu on the left to continue the interaction as needed. ![Handle Empty Response](./images/337.png) In this case, if the RAG Assistant finds the answer in the documents, you want the flow to call an assistant to translate the response to Japanese and store it in the same input variable (lastResponse). If no answer is found, the flow goes to a second RAG Assistant configured with documents used in Globant Enterprise AI courses. The variable that goes into the RAG Assistant is lastUserInput, which contains the user's last input. The response generated by this RAG Assistant is stored in the lastResponse variable. ![RAG Assistant Configuration](./images/338.png) ### Go To Node Finally, the Go To node is used to connect different flows within the interaction. This node allows you to select the flow to which you want to redirect the end user based on their current interaction. ![Go To Node](./images/339.png) Clicking on the blank bar (corresponding to the drop-down menu) displays a list of available flows: ![Available Flows](./images/340.png) Here you can select and define the flow to which you want to redirect the user. To review or modify any of these flows, you can access the Flows menu on the left side of the screen. In addition, \"Pick from canvas\" allows you to select a node directly from the canvas, facilitating navigation and connection between different parts of the Flow. The 'Go' icon next to the node allows you to navigate directly to the node to which it is connected. In this case, there are two 'Go To' nodes: - The first 'Go To': Returns to the 'User Input' node. Since the previous node (which translates to Japanese) does not store the response in a new variable and simply redirects to the 'User Input', the response received by the end user is the answer to their question translated to Japanese.\n- The second 'Go To': Sends the response to the 'Assistant' node, which is responsible for translating the generated response to Japanese using the lastResponse variable. This variable contains the response obtained from either of the two RAG Assistants previously executed in the flow. In this way, the flow ensures that the end user receives the response in Japanese, regardless of whether the response was generated by the first RAG Assistant or the second. ![Flow Diagram](./images/341.png) ## Step 3: Test flow Once you have configured the flow, make sure to click on the 'Save' button located at the top left of the screen. To test the flow you have created, you have three options available from the same design window: ![Test Flow Options](./images/342.png) 1. Click on the arrow icon (similar to the play button) to run the flow ('Run Flow').\n2. Click on the message icon to open the testing assistant (\u201cOpen Testing Assistant\u201d). ![Testing Assistant](./images/343.png)\n3. Click on the icon to open in a new window (the box with an arrow) to access the demo page (\u201cGo to Demo Page\u201d). ![Demo Page](./images/344.png) The first two options will open a window on the right side of the screen, while the third option will open a new window in the browser.\n\n# How to create a Flow This is a step-by-step guide to create and test an Assistants [Flow](321.html). ## Step 1: Create New First, log in to the [Globant Enterprise AI Backoffice](42.html). In the Project Dynamic combo box, select the project you want to work with (in this case, Default is used). Next, on the left side of the screen, you will find the backoffice menu. In this menu, click on Flows. ![Flow Menu](./images/324.png) By clicking on Flows in the Globant Enterprise AI Backoffice, a new window opens in the browser with the Flow Builder, where the Flows associated with the selected Globant Enterprise AI project are created and managed. When accessing Globant Enterprise AI\u2019s Flow Builder for the first time, a welcome screen like the one shown in the image below is displayed. From this screen, you can start creating a new flow by clicking on the \"Create\" button. ![Welcome Screen](./images/325.png) When you click on \"Create\", a pop-up window opens in which you must fill in the following information: 1. **Name**: Descriptive name for the flow, allowing it to be easily identified.\n2. **Description**: This field is optional, but it is recommended to add a brief description of the flow to clarify its purpose or content.\n3. **Type**: Allows you to select the type of flow to create, with the options \"Chat Flow\" or \"Assistant Flow\". Selecting one of these types will determine specific configurations and characteristics of the flow. In this case, Assistant Flow is selected.\n\n4. **Select languages**: Sets the language in which the flow will be configured, and allows you to define the language of the hard-coded messages. Multiple languages can be selected, so the same message can be available in different languages. Once you have completed these fields, you can click on the \"Create\" button. ![Create Flow](./images/326.png) ## Step 2: Manage the Start flow Once you have clicked on \"Create,\" you will return to the welcome screen. There, click on the \"Manage\" button of the newly created flow to add the assistants you want and customize their behavior. ![Manage Flow](./images/327.png) Clicking on the \"Manage\" button displays the first flow, which is created automatically. This initial flow, called \"start,\" establishes a basic configuration that you can customize, adding or modifying interactions, messages and assistants according to your needs. ![Start Flow](./images/328.png) ### Start flow The initial flow, called \"start,\" is automatically generated with the ID input.welcome. ![Start Flow ID](./images/329.png) ### Start (Intent: input.welcome) Node The Start (Intent: input.welcome) node marks the beginning of the conversation flow. This node is activated every time the flow is triggered, and its main purpose is to start the flow and, from there, the other nodes that manage the interaction with the user are connected. To rename the start node, go to the left menu, find Flows > start, click on the three dots next to it, select \"Rename Flow\" and, after typing the new name, press Enter to save the change. ![Rename Flow](./images/330.png) ### Message Node The next node, Message, is in charge of sending the welcome message or any other initial message configured. In this case, the message is: \"Hello {name}, how can I help you?\". Here, {name} represents a dynamic variable that is filled with the user's name. The message displayed in this node is customizable, and the text can be formatted by selecting different styles, such as Normal, Heading 1, among others, from the drop-down menu that appears where it says \"Normal\". This allows you to adjust the text style according to your needs. ![Message Node](./images/331.png) On the top bar of the node, you can see several formatting buttons such as bold, italic, underline, among others, to customize the text. In addition, there are icons that allow you to add lists, links and emojis. The last button, which has the shape of a small drawer, allows you to insert dynamic variables in the message. Clicking on this button displays a list of the variables available in the system, and it is also possible to define new custom variables. This Message node can be deleted by clicking on the trash can icon to the right of the node. It is also possible to add a variant by clicking on the \"plus\" (+) button at the top right. ### User Input Node The flow then proceeds to capture the user input through the User Input node, storing it in the lastUserInput variable for later use. ### Assistant Node In the Assistant node, you must configure the assistant to perform specific actions. This node uses the information stored in the lastUserInput variable, which contains the text entered by the end user in the User Input node. ![Assistant Node](./images/332.png) To configure the assistant, click on the \"Select an assistant\" field. By default, \"No assistant selected\" will be displayed. From the drop-down menu, choose the assistant you wish to use. **Note**: The assistants available are those that have been defined in the associated project, which was configured in the first step. The assistants can be any of those defined in Globant Enterprise AI, including [Data Analyst Assistant](886.html), [API Assistant](110.html), [Chat with Data Assistant](159.html). Once selected, the Prompt field will display the message that defines the task of the assistant. This message is read-only and can\u2019t be edited. The assistant's response can be stored in the lastUserInput variable or in a new variable, depending on your needs. By default, the Use as response option is enabled, which causes the assistant's response to be used directly in the lastUserInput variable. If you prefer to store the response in a new variable, disable Use as response. This will enable the Output field, where you can select or create a new variable. ![Assistant Output](./images/333.png) In addition, you can enable the option to read the assistant's response in JSON format. By enabling this option, you will have the ability to map the fields of that JSON to variables previously defined in your configuration. This allows for more precise integration of the assistant's responses in more complex workflows or in systems that require a specific format for data handling. The pencil icon to the right of the node corresponds to the Edit button. By clicking on it, you can enable or disable the Include conversation history and Include context variables options. By default, Include conversation history is enabled. It allows the assistant to access the conversation history to generate more coherent and contextual responses. The Include context variables option is also enabled by default. This option sends to the assistant variables defined in the flow, such as previously captured data or configured values, so that they can be used within prompts. This node can be deleted by clicking on the trash can icon to the right of the node. In this case, the Assistant node is deleted: ![Delete Assistant Node](./images/334.png) ### Insert RAG Assistant or Assistant Node On the left menu, below the Interactions category, you will find the different components that can be added to the flow. In particular, in the GENERATIVE section, you can choose between adding an Assistant or a RAG Assistant. ![Insert RAG Assistant](./images/335.png) In this case, you need to add a RAG Assistant. To do so, click on the RAG Assistant option in the GENERATIVE section. Next, drag the component to the desired location in the flow. In this case, you can insert it between the User input and Go To nodes. When you add a new RAG node, you must click on the \"Select an assistant\" field. By default, the option \"No assistant selected\" will appear. From the drop-down menu, choose the RAG assistant you wish to use. Once you select the RAG assistant, the next field, \"Select a document\", allows you to choose a specific document that will act as the source for responding to the end user's request. If you select a document, the assistant will only use that document as a reference. ![Select Document](./images/336.png) Alternatively, you can use the \"Indicates the documents' tags\" field. Here you can add the tags defined in [Step 2: Upload RAG documents](60.html), which are used to filter the documents. This allows the assistant to search only in the documents that match the indicated tags to respond to the end user's request. In the Input section, a field called Variable is displayed. There, the variable to be used as input for that node",
          "evaluationResultRowContextDocId": "09ebe65a-fb9d-4f45-8146-084558fef786",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "016d66de-9ee7-4f1c-baab-9fa9c281298a",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?322",
          "evaluationResultRowContextName": "322",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#How to expose a Flow in a web page Once you have created your Flow in Globant Enterprise AI, following the steps in [[322]], you may want to expose it in a web page. To do this, follow the steps below. ## Step 1: Create an application key Go to the Flow you want to expose. From the [[371##Side Navigation Menu|Side Navigation Menu]], select [[371##CONFIGURATION|Configuration]] and then Application Keys. Click on the **Generate Key** button. [[474]] In the Type field, select WEBSITE and in Key Name, enter a name, for example, \"Documentation\". Confirm the operation by clicking on Save. [[475]] Click on the copy icon shown next to the generated key. This will allow you to customize and use the code on any external page that requires integration with the Flow created. [[476]] ## Step 2: Understand the Script's Functionality Once you have clicked on the copy icon next to the generated key, you will be able to view the JavaScript code script. ```\n(function () { var script = document.createElement('script'); script.src = \"https://botbuilder.dev.fluentlab.ai/fluentlab/static/js/webcomponent/fluentlab-websdk.js\"; document.head.appendChild(script); window.addEventListener('FluentLabWebComponentsReady', function () { var fluentlabElement = document.createElement(\"fluentlab-app\"); fluentlabElement.branding = \"globant\"; fluentlabElement.launcher = true; fluentlabElement.locale = \"en\"; fluentlabElement.title = \"Multibot Assistant\"; fluentlabElement.socketUrl = \"https://botbuilder.dev.fluentlab.ai/fluentlab/websocket?x-api-key=API-KEY\"; fluentlabElement.botId = \"BOT-ID\"; fluentlabElement.updateStyles({ '--fluentlab-font-family': '\"Avenir\", \"Helvetica\", \"Arial\", sans-serif', }); fluentlabElement.setExtraData(function(body) { // TODO: Send custom data to bot as requestExtraData return {}; }); fluentlabElement.addEventListener('fluentlab-callback-click', function(event) { // TODO: Implement handlers for the callback actions defined in the flows }); document.body.appendChild(fluentlabElement); });\n}());\n``` This script performs several key functions that ensure that the Flow integrates correctly into your web page: * **Dynamic Script Loading:** The script automatically creates and adds a script element to your web page. This element loads the web SDK needed to integrate the chat from the URL `https://botbuilder.dev.fluentlab.ai/fluentlab/static/js/webcomponent/fluentlab-websdk.js`. * **Component Initialization:** Once the web components are ready (when the FluentLabWebComponentsReady event is fired), the script creates a new element called fluentlab-app. This is the main chat container, where you define how the assistant will appear and behave on the page. * **Custom Configuration:** - Branding: The branding attribute sets the brand that will be displayed in the chat. In this case, \"globant\" is used. - Launcher: The launcher attribute controls whether the chat should be visible or hidden until the user activates it. - Locale and Title: locale defines the language of the assistant (in this case, English), and title sets the visible name of the assistant, which here is \"Multibot Assistant\". - Socket URL and Bot ID: socketUrl specifies the URL of the socket server to handle the communication, and botId is the ID of the bot that manages the chat session. * **Visual Styles and Events:** The script also customizes the appearance of the chat. For example, the fonts used in the chat are configured with updateStyles. In addition, events are handled via addEventListener, including clicks made by users within the chat. This allows you to add more interaction based on the Flows you have configured. ## Step 3: Customize Further Using HTML Attributes In addition to the initial configuration, you can further customize the Flow\u2019s behavior by using HTML attributes. These attributes allow you to fine-tune how the chat behaves on your page. The available attributes and their functions are listed below: * **audioUrl** (String): URL of the audio file to be played with incoming messages.\n* **autoFocus** (Boolean, default `false`): Automatically returns focus to the chat input field after interacting with another component of the chat frame.\n* **delay** (Number): Sets a delay between responses; if not specified, the delay is calculated based on the length of the responses.\n* **disableTextInteraction** (Boolean, default `false`): Allows hiding or showing the user's text input.\n* **geolocation** (Boolean, default `false`): Sends geolocation with each websocket message.\n* **launcher** (Boolean, default `false`): Controls the visibility of the chat view.\n* **preview** (Boolean, default `false`): Shows the first floating message on the launcher if the chat frame is closed.\n* **title** (String): Name of the assistant.\n* **sessionId** (String, default `uuid.v4`): Unique identifier of the session to establish a conversation.\n* **showExpiration** (Boolean, default` false`): Displays an expiration message instead of refreshing the server context.\n* **socketUrl** (String): URL of the socket for the connection.\n* **voiceEnabled** (Boolean, default `false`): Enables voice synthesis by default.\n\n* **voices** (String): Voices to be used when voice synthesis is enabled, allowing you to specify more than one voice and adjust the speed and pitch. ## Step 4: Custom methods Optionally, you can use custom methods to send additional parameters and manage events in the chat component using **fluentlab-app**. These methods provide greater flexibility and customization of the chat behavior, enriching the end user\u2019s experience on your web page. #### **setMessageBuilder** This method allows sending additional parameters every time a message is posted to the socket, using the `setMessageBuilder` method. You can define a callback that receives and processes these extra parameters, in addition to the query and the session identifier (`sessionId`). **Example:** ```javascript var rest = { language: \"english\", platform: \"react\", anotherParam: false, }; var FluentLab = document.querySelector(\"fluentlab-app\"); FluentLab.setMessageBuilder(function ({ query, sessionId, ...rest }) { return { destination: \"/app/websocket/chat\", body: JSON.stringify({ ...query, ...rest, sessionId }), }; }); ``` In this example, the `rest` object contains additional parameters that will be sent with the message every time it is posted to the socket. #### **addRender** This method allows you to customize the rendering of certain types of rich messages. Specify the type of message and the content you want to render. For example, create a custom-styled container every time an HTML type message is received. **Example:** ```javascript var FluentLab = document.querySelector(\"fluentlab-app\"); FluentLab.addRender(\"HTML\", function (data, container, onClick) { var div = document.createElement(\"div\"); div.innerHTML = data.html; div.style.border = \"1px solid red\"; container.appendChild(div); }); ``` This code customizes the way HTML messages are rendered inside the chat container. #### **onIncomingMessage** The `onIncomingMessage` method allows you to define a callback that is executed every time a message is received in the chat. **Example:** ```javascript var FluentLab = document.querySelector(\"fluentlab-app\"); FluentLab.onIncomingMessage(function (messages) { console.log(\"Message received:\", messages); }); ``` With this callback, you can handle any specific logic you want every time the bot sends a message to the client. #### **Slots** Slots allow you to inject HTML into specific sections of the chat component, customizing its structure. Each slot has a name that defines its position. **Example:** <pre class=\"Code\"> `<`fluentlab-app`>` `<`header slot=\"custom-header\"`>` `<`h1`>`This is a custom header`<`/h1`>` `<`/header`>` `<`/fluentlab-app`>` </pre> In this case, the slot called `custom-header` places a custom header at the top of the chat. #### **Styles Overriding** You can modify the CSS styles of the chat component by overriding default values using custom CSS properties that use the `--fluentlab-` prefix. These values are applied to the component's Shadow DOM. **Example:** <pre class=\"Code\"> `<`custom-style`>` `<`style is=\"custom-style\"`>` fluentlab-app { --fluentlab-primary-color-main: #abcdef; } `<`/style`>` `<`/custom-style`>` </pre> This code changes the primary color of the chat to a custom shade (`#abcdef`). #### **Event Handling** It is possible to add listeners to specific events of the chat component. Some examples of events include: - `fluentlab-callback-click`: Fired when the user clicks on a CALL-TO-ACTION component of CALLBACK type. - `fluentlab-user-input`: Fired when the user sends a message (presses the enter key or clicks on the send button). **Example:** ```javascript var FluentLab = document.querySelector(\"fluentlab-app\"); FluentLab.addEventListener(\"fluentlab-user-input\", function(event) { console.log(\"User message:\", event.text); }); ``` This listener logs the incoming messages that the user sends to the bot. ## Step 5: Test the bot Once you have made all the necessary adjustments, you can incorporate it into your web page and start testing to make sure everything works properly. If you prefer to test more immediately without modifying your source code, you can use the Google Chrome developer tool. To do so, follow these steps: 1. Open your browser and go to the top right, where you will see an icon with three dots.\n2. Click on the three dots and select **More tools**.\n3. Next, select **Developer Tools**.\n4. In the window that opens, go to the **Console** tab.\n5. Paste the JavaScript code generated in the previous step into the console and press **Enter**. {{{216| If an error occurs, it may be due to security restrictions or policies defined on the site. To avoid this type of error, add the corresponding domain to a whitelist that allows it to run without restrictions.}}}\n{{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "2379dc60-75e5-4ca1-b766-2440a01ff92d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "a028565b-42b1-4189-ad39-2ad0628cf570",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?473",
          "evaluationResultRowContextName": "473",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "* **Body**: This section allows you to write the content of the email. You can format the text using basic editing options such as bold, italics, underline, lists and links. You can also insert emojis or variables. ### Spreadsheet Allows you to write data into a spreadsheet directly from a flow. You can use this node to store user responses or any other information in real time, organizing the data in specific columns of the spreadsheet. [[508]] To configure the Spreadsheet node, click on the Edit icon located on the right side of the node. Clicking on Edit will open a side menu where you can enter the Spreadsheet ID and the Sheet name. The Spreadsheet ID is obtained from the URL of the Google Spreadsheet, and the Sheet name corresponds to the specific tab within the file where you want to save the data, such as \"Sheet 1\". In addition, make sure to upload the JSON file with the service account credentials and that the service account has permissions on the spreadsheet. The node displays a drop-down field where you can select the variables to be written to the spreadsheet. You must arrange the variables in the order in which they appear in the spreadsheet. You can also add other context variables if necessary. ## RESPONSES The Responses section contains the nodes responsible for generating responses to the end user. [[509]] ### Message Sends custom messages within a flow, such as an initial greeting. For example, you can use: \"Hello {name}, how can I help you?\", where {name} is a dynamic variable that is filled with the user's name. [[510]] This message is fully customizable. You can format the text using different styles, such as Normal or Heading 1, and formatting tools such as bold, italics, lists, links and emojis. In addition, you can insert dynamic variables, and add lists, links and emojis. You can also add message variants. ### Image Adds images to interactions within a flow, displaying a space where you can upload or select an image. [[511]] In addition, you can make the content dynamic by clicking on the nut-shaped icon, allowing you to use Velocity Template Language (VTL) to create dynamic responses, adapting the image content to different contexts. Clicking on Add Image brings up a menu on the left side that allows you to upload an image by dragging it or by browsing for the file on your device. You can also enter a URL directly to use an online image. This menu offers options for adjusting the size of the image, including cropping or scaling the image, and setting its vertical and horizontal position, centering it as needed. ### Link Create rich links with a custom preview that includes a title, description, image and tag, providing additional context to engage the user. [[512]] If you choose to add an image, clicking on Add Image will open a menu on the left side where you can upload an image from your device or enter a URL to use an online image. This menu also allows you to adjust the size, vertical position and horizontal position of the image, in a similar way as with the Image component. In addition, you can make the content of the link dynamic by clicking on the nut-shaped edit button. This will allow you to use Velocity Template Language (VTL) to adapt the preview to different contexts and situations within the flow. ### Card Create interactive cards within a flow, where you can add an image, title, and short description. As in the Image and Link components, you can add an image by clicking on Add Image, which will open a menu on the left side with the same options as the Image and Link components. [[513]] The Card node includes one button by default, and you can add more if necessary. If you add several buttons, each one can be configured to connect to an External function or External link, or to define an event. When all buttons are configured as External function or External link, the flow follows a single branch. However, if any of the buttons is configured for an event, an additional branch will be generated in the flow for each button configured for events. This node allows you to enable or disable text interaction, keep the conversation active, or allow user input. You can also make the content dynamic in order to adapt the card to different contexts within the flow using Velocity Template Language (VTL). ### Video Adds the URL of a video, but YouTube videos are not supported. You can also paste the URL of an image in the Poster field to use it as a thumbnail or cover of the video. [[514]] It is possible to make the link content dynamic using Velocity Template Language (VTL) to adapt the video preview to different contexts and situations within the flow. In addition, the component offers the option of adding buttons to connect to an external link, define a specific event or action, and set variables that can be used as input in other interactions within the flow. Each additional button can create a new branch in the flow, unless it is configured to connect to an External function or External link, in which case it will follow the same branch. ### YouTube Allows you to insert YouTube videos into your flow. [[515]] To set up this node, paste the ID of the video in the \"Id\" field in the left menu, which appears when you click on the edit icon. Make sure to enter only the part of the URL that comes after the equal sign (=). For example, if the URL is `https://www.youtube.com/watch?v=h_5nxFWjmZc`, just enter `h_5nxFWjmZc` in the corresponding field. The video thumbnails are generated automatically, so there is no need to add an additional image. The component also offers the option to add as many buttons as you deem necessary. In addition, you can make the content dynamic using Velocity Template Language (VTL), which allows you to adapt the video presentation to different contexts and situations within the flow. ### Quick Reply Adds buttons that you can use to start actions or redirect to other flows. Each button is configured by entering its name and selecting an action option. The Event option allows you to direct the reply to the desired flow. With External Link, you can redirect the user to a website, taking them out of the current flow. Finally, the External function allows you to execute an external function. [[516]] This node also offers additional behavior settings. You can disable text interaction, keep the conversation active, or enable user input. By default, the Enabled User Input option is active, which means that, if you add multiple buttons, a branch will be generated in the flow for each button configured as an Event. Buttons configured as External function or External link will not create new branches and will follow the same path in the flow. ### Datepicker Allows you to select a date within a flow, displaying fields to choose the month, day and year. [[517]] It is possible to configure the format in which the date will be displayed, select which dates will be available, connect the date picker to a specific event, and assign\n\n#Interactions to Create a Flow The Interactions Section provides the necessary components to build custom [[322|flows]]. Here you will find tools to generate, integrate, and respond effectively to interactions in order to design flows that meet the needs of the end user. [[493]] Below are described the different categories of components available in the Interactions Section, including their functionalities and how they can be used to optimize the creation of flows in Globant Enterprise AI. ## GENERATIVE In the Generative category, you will find the assistants that use generative AI: Assistant and RAG Assistant. [[495]] ### Assistant Configures an assistant, such as [[886|Data Analyst Assistant]], [[110]], [[159]], among others, that have been previously defined in the selected project in the [[42]]. [[496]] Once the assistant is selected, the Prompt field will display the message defining the assigned task. This message is read-only and cannot be edited. To see the complete prompt, click on the configuration icon. This node always requires an input variable, such as `lastUserInput`, which receives the information provided by the user. The output variable, on the other hand, can be configured. By default, the assistant's response is stored in `lastUserInput`, but if you disable the Use as response option at the top of the node, you can set a different output variable. In addition, it is possible to save the response in JSON format. The node also allows you to include the conversation history and context variables to further customize the interaction. ### RAG Assistant It integrates an assistant of [[44|RAG]] type that has been previously defined in the selected project in the Globant Enterprise AI Backoffice. [[497]] After selecting the RAG assistant, you can define a document or tags that will act as a source for the responses. Variables are handled in the same way as in the Assistant node. That is, it always requires an input variable, and it is possible to configure a different output variable that can be in JSON format. You can also set the minimum confidence level for responses, include the conversation history and context variables, and choose to display the sources of information used. In addition, you can configure the actions to take when an answer is not found in the documents provided. ## LOGICAL This section includes logical components that allow you to control the flow of the conversation and manipulate variables or conditions within the flow. [[498]] ### User Input Collects user input and stores it in a variable for use in subsequent nodes. [[499]] ### Conditional Evaluates logical conditions written in [https://velocity.apache.org/engine/1.7/vtl-reference.html#ifelseifelse-output-conditional-on-truth-of-statements|Velocity Template Language (VTL)] to decide which path the flow will take. [[500]] When this component is added to the flow, it is divided into branches, which by default are one for If and another for Else. The If branch node is by default of Expression type, while the Else branch is of Go to type. It is possible to add more If conditions by clicking on Add condition. By doing so, a branch with a node of Expression type is added, just like the first one. These Expression nodes allow evaluating different conditional expressions. Each branch is driven by the result of the evaluated condition. Depending on the value of a variable, this allows the flow to be directed to different assistants or actions. If the condition is not met, the flow continues through the Else branch. Each branch is driven by the result of the evaluated condition. Depending on the value of a variable, this allows the flow to be directed to different assistants or actions. If the condition is not met, the flow continues through the Else branch. To configure the condition, select a variable and apply a relational operator to perform the comparison. The supported relational operators are: * Equal\n* Not equal\n* Less than\n* Less than or equal to\n* Greater than",
          "evaluationResultRowContextDocId": "26b674b4-d387-4cb4-a16f-b97b7e0d8204",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "c7cbcb34-90eb-43ed-8cb4-c8fe60763252",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?494",
          "evaluationResultRowContextName": "494",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.5,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "09f31ba0-c78e-4829-8413-6190aee912d9",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0043378800",
      "evaluationResultRowEndDate": "2025-05-13T11:11:57",
      "evaluationResultRowOutput": "Para integrar un asistente Flow de Globant Enterprise AI en tu aplicaci\u00f3n web, debes seguir estos pasos principales:\n\n1. **Crear una clave de aplicaci\u00f3n (Application Key) </B> \n   - Accede al Flow que quieres exponer.  \n   - Desde el men\u00fa lateral selecciona **Configuration > Application Keys**.  \n   - Genera una nueva clave de tipo **WEBSITE </B>y as\u00edgnale un nombre, por ejemplo, \"Documentation\".  \n   - Copia la clave generada para usarla en la configuraci\u00f3n del script en tu web.  \n\n2. **Incrustar el script JavaScript en tu p\u00e1gina web </B> \n   Usa el c\u00f3digo que se genera al copiar la clave. Este script hace lo siguiente:  \n   - Carga din\u00e1micamente el SDK web necesario para el chat.  \n   - Crea el componente principal `fluentlab-app`, que es el contenedor del chat.  \n   - Configura el branding, idioma, visibilidad, t\u00edtulo, URL del socket y Bot ID para manejar la sesi\u00f3n.  \n   - Permite personalizar estilos y manejar eventos para una mejor experiencia.  \n\n   Un ejemplo b\u00e1sico del script es:  \n   ```javascript\n   (function () { \n     var script = document.createElement('script'); \n     script.src = \"https://botbuilder.dev.fluentlab.ai/fluentlab/static/js/webcomponent/fluentlab-websdk.js\"; \n     document.head.appendChild(script); \n     window.addEventListener('FluentLabWebComponentsReady', function () { \n       var fluentlabElement = document.createElement(\"fluentlab-app\"); \n       fluentlabElement.branding = \"globant\"; \n       fluentlabElement.launcher = true; \n       fluentlabElement.locale = \"en\"; \n       fluentlabElement.title = \"Multibot Assistant\"; \n       fluentlabElement.socketUrl = \"https://botbuilder.dev.fluentlab.ai/fluentlab/websocket?x-api-key=API-KEY\"; \n       fluentlabElement.botId = \"BOT-ID\"; \n       fluentlabElement.updateStyles({ '--fluentlab-font-family': '\"Avenir\", \"Helvetica\", \"Arial\", sans-serif', }); \n       document.body.appendChild(fluentlabElement); \n     }); \n   }());\n   ```  \n   Recuerda reemplazar `\"API-KEY\"` y `\"BOT-ID\"` con los valores correspondientes a tu proyecto.  \n\n3. **Personalizaci\u00f3n opcional </B> \n   - Puedes usar atributos HTML en el componente `fluentlab-app` para ajustar comportamiento (ejemplo: `autoFocus`, `locale`, `voiceEnabled`, etc.).  \n   - M\u00e9todos JavaScript para enviar par\u00e1metros extra, personalizar mensajes, manejar eventos o modificar estilos CSS mediante variables customizadas.  \n   - Se pueden a\u00f1adir *slots* para insertar contenido HTML personalizado en distintas partes del chat.  \n\n4. **Probar la integraci\u00f3n </B> \n   - Puedes probar el asistente directamente en tu p\u00e1gina o usando la consola del navegador (Chrome DevTools).  \n   - Si tienes problemas con restricciones de seguridad, aseg\u00farate de agregar el dominio a una lista blanca para permitir la ejecuci\u00f3n del script.  \n\nEsta informaci\u00f3n se encuentra detallada en la documentaci\u00f3n oficial en:  \nhttps://wiki.genexus.com/enterprise-ai/wiki?473  \n\nAll\u00ed encontrar\u00e1s adem\u00e1s ejemplos completos y detalles sobre c\u00f3mo adaptar a\u00fan m\u00e1s la integraci\u00f3n a tus necesidades.  \n\n---\n\nSi quieres informaci\u00f3n sobre c\u00f3mo crear el Flow previo a esta integraci\u00f3n, puedo ayudarte tambi\u00e9n.  \n\n[Referencia: 3](https://wiki.genexus.com/enterprise-ai/wiki?473)",
      "evaluationResultRowStartDate": "2025-05-13T11:11:44",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "8846"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#RAG Assistants API This API allows you to define different [[44|RAG Assistants]]. To use the API, check the [[20##Generic Variables|generic variables]] needed. In addition, read the explanation about parameters in [[64]]. > The following endpoints require a Globant Enterprise AI API token related to **project** scope. ## Endpoints Below is a summary of the available endpoints for this API: | Method | Path | Description |\n|---|---|---|\n| `GET` | `/profiles` | Gets all RAG assistants from a project |\n| `GET` | `/profile/{name}` | Gets a specific RAG assistant |\n| `POST` | `/profile` | Creates a new RAG assistant |\n| `PUT` | `/profile/{name}` | Updates a RAG assistant |\n| `DELETE` | `/profile/{name}` | Deletes a RAG assistant |\n| `GET` | `/profile/{name}/documents` | Gets documents for a RAG assistant |\n| `PUT` | `/profile/{name}/documents`<sup>(1)</sup> | Reindex all documents for the selected RAG assistant |\n| `DELETE` | `/profile/{name}/documents` | Deletes all documents for a RAG assistant |\n| `GET` | `/profile/{name}/document/{id}` | Retrieves Document information |\n| `POST` | `/profile/{name}/document` | Uploads a Document |\n| `PUT` | `/profile/{name}/document`<sup>(1)</sup> | Reindex a Document |\n| `DELETE` | `/profile/{name}/document/{id}` | Deletes a Document |\n| `POST` | `/execute` | Executes a query against the defined RAG assistant | <sup>(1)</sup> - Supported since `2025-05` release. **Note**: Keep in mind that the `searchProfiles` and `{name}` parameters refer to RAG Assistants. In addition, the `{name}` parameter, which represents the name of the RAG Assistants, must not contain blanks. Instead, they must be replaced by the + symbol. ## GET/profiles Retrieves all the RAG Assistants for a Project. ### Response ```json\n{ \"projectActive\": boolean, \"projectDescription\": \"string\", \"projectId\": \"string\", \"projectName\": \"string\", \"projectStatus\": integer, /* 0:Active, 1:Deleted, 2:Hidden */ \"searchProfiles\": [ { \"description\": \"string\", \"name\": \"string\" }, ... ]\n}\n``` The `description` parameter is required for the [[33|chat]] option. ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/search/profiles\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"Accept: application/json\"\n``` ## GET/profile/{name} Gets RAG Assistants `{name}` details. ### Response ```json\n{ \"name\": \"string\", \"description\": \"string\", \"indexOptions\": { \"chunks\": { \"chunkOverlap\": integer, \"chunkSize\": integer } }, \"searchOptions\": { \"historyCount\": integer, \"llm\": { \"cache\": boolean, \"frequencyPenalty\": decimal, \"maxTokens\": integer, \"modelName\": \"string\", \"n\": integer, \"presencePenalty\": decimal, \"provider\": \"string\", \"stream\": boolean, \"temperature\": decimal, \"topP\": decimal, \"type\": \"string\", \"verbose\": boolean }, \"retriever\": { \"type\": \"string\" /* vectorStore, multiQuery, selfQuery, hyde, contextualCompression */ }, \"search\": { \"k\": integer, \"prompt\": \"string\", \"returnSourceDocuments\": boolean, \"scoreThreshold\": decimal, \"template\": \"string\" } }, \"status\": integer /* 1:Enabled, 2:Disabled */\n}\n``` The `type` parameter is explained in [[70|RAG Assistants - Configuration Retrieval]]. ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/search/profile/{name}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## POST/profile Creates a RAG Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) |\n| `descrition` | `string` | Description of the RAG assistant |\n| `template` | `string` | Name of an existing RAG to base the configuration (optional), empty by default | ### Request Body ```json\n{ \"name\": \"string\", /* Required */ \"description\": \"string\", \"template\": \"string\", \"searchOptions\": { \"embeddings\": { \"provider\": \"string\", \"modelName\": \"string\", \"useProxy\": boolean, \"mode\": \"string\", // \"\" or \"multimodal\" \"type\": \"string\", \"batchSize\": integer, // defaults to 100 \"dimensions\": integer // defaults to 1536 }, \"historyCount\": integer, \"llm\": { \"cache\": boolean, \"frequencyPenalty\": decimal, \"maxTokens\": integer, \"modelName\": \"string\", \"n\": integer, \"presencePenalty\": decimal, \"provider\": \"string\", \"stream\": boolean, \"temperature\": decimal, \"topP\": decimal, \"type\": \"string\", /* empty value (default) or json_object */ \"verbose\": boolean, \"guardrails\": [...] // options available are: \"input-moderation-guardrail\", \"llm-output-guardrail\", \"prompt-injection-guardrail\" }, \"search\": { \"k\": integer, \"type\": \"string\", /* similarity (default) or mmr */ \"fetchK\": number, /* valid when using mmr type */ \"lambda\": decimal, /* valid when using mmr type */ \"prompt\": \"string\", \"returnSourceDocuments\": boolean, \"scoreThreshold\": decimal, \"template\": \"string\" }, \"retriever\": { \"type\": \"string\", /* vectorStore, multiQuery, selfQuery, hyde, contextualCompression */ \"searchType\": \"similarity | similarity_hybrid | semantic_hybrid\" /* Azure AISearch specific, defaults to similarity */ \"step\": \"string\", /* all (default) | documents */ \"prompt\": \"string\" /* not needed when using vectorStore */ } }, \"indexOptions\": { \"chunks\": { \"chunkOverlap\": integer, \"chunkSize\": integer }, \"useParentDocument\": boolean, /* false by default */ \"childDocument\": { /* valid if the previous element is true */ \"childK\": decimal, \"child\": { \"chunkSize\": decimal, \"chunkOverlap\": decimal, \"contentProcessing\": \"string\" /* '' | 'clean' */ } } }, \"welcomeData\": { \"title\": \"string\", \"description\": \"string\", \"features\": [ { \"title\": \"string\", \"description\": \"string\" }, ... ], \"examplesPrompt\": [ { \"title\": \"string\", \"description\": \"string\", \"promptText\": \"string\" }, ... ] }\n}\n``` The LLM type option using *json_object* can be checked [[69##type|here]]. {{{216| If you do not provide the `WelcomeData` section when creating a RAG Assistant, it will be empty}}} ### Response Equivalent to [[29##GET/profile/{name}|Get Response]]. ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/v1/search/profile\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\" \\\n-d '{\n\"name\": \"my RAG assistant\",\n\"description\": \"My awesome profile\",\n\"searchOptions\": {\n\"historyCount\": 2,\n\"llm\": {\n\"temperature\": 0.1,\n\"maxTokens\": 1500,\n\"modelName\": \"gpt-3.5-turbo-16k\"\n},\n\"search\": {\n\"k\": 5\n}\n}\n}'\n``` #### Sample with WelcomeData ```bash\ncurl -X POST \"$BASE_URL/v1/search/profile\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\" \\\n-d '{\n\"name\": \"Test-Profile-WelcomeData\",\n\"description\": \"Test Profile with WelcomeData\",\n\"searchOptions\": {\n\"historyCount\": 2,\n\"llm\": {\n\"cache\": false,\n\"temperature\": 0.1,\n\"maxTokens\": 999,\n\"modelName\": \"gpt-3.5-turbo-16k\",\n\"verbose\": true\n},\n\"search\": {\n\"k\": 5,\n\"returnSourceDocuments\": false,\n\"scoreThreshold\": 0,\n\"prompt\": \"Use {context} and {question}\"\n}\n},\n\"indexOptions\": {\n\"chunks\": {\n\"chunkSize\": 999,\n\"chunkOverlap\": 99\n}\n},\n\"welcomeData\": {\n\"title\": \"Welcome to the RAG\",\n\"description\": \"It is a RAG created with WelcomeData via API\",\n\"features\": [\n{\n\"title\": \"Weather conditions\",\n\n{ \"type\": \"test\", \"domain\": \"Knowledge\", \"year\": 2023, \"quarter\": \"q3\"\n}\n``` The metadata can be uploaded as `Text` or directly from a `File`. #### Metadata special values Check for the following metadata special values | Item | **Description** |\n|---|---|\n| url | If you want that the document point to a specific absolute URL, add a *url* metadata element pointing to the complete url resource. |\n| description | Use this property if you want to personalize how the Frontend shows the sources section; when not present it will show the *name* property. | ### Response Equivalent to [[29##GET/profile/{name}/document/{id}|Get Response]]. Note that, once the document is uploaded, the `indexStatus` will be `Unknown` as it is queued to be ingested. Use the [[29##GET/profile/{name}/document/{id}|Get Response]] API to check the document status; the expected result is `Success`. Possible return errors: - [[23##2027|2027]]\n- [[23##2028|2028]] ### cURL Sample To upload a `SampleFile.pdf` file, you can follow these steps: ```bash\n# binary\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'filename: SampleFile.pdf' \\\n--header 'Content-Type: application/pdf' \\\n--data '@/C:/temp/SampleFile.pdf' # multi-part\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"Content-Type: multipart/form-data\" \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"' # multi-part with metadata as text\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"Content-Type: multipart/form-data\" \\\n--form 'metadata=\"{\\\"type\\\":\\\"test\\\",\\\"domain\\\":\\\"Knowledge\\\",\\\"year\\\":2023,\\\"quarter\\\":\\\"q3\\\"}\"' \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"' # multi-part with metadata as a File\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"Content-Type: multipart/form-data\" \\\n--form 'metadata=@\"/C:/temp/upload_file_metadata.json\"' \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"'\n``` ## PUT/profile/{name}/document Reindex a Document (associated to an existing `documentId` GUID) to the associated `{name}` RAG Assistant. ### Request Headers | **Header** | **Description** |\n|---|---|\n| `documentId` | The GUID associated to an already existing document. | ### Request Form-data Body | **Key** | **Description** |\n|---|---|\n| `file` | The associated file to reindex (`optional`), if not present will use the previous existing file automatically. |\n| `metadata` | Use a metadata to reindex the content (`optional`); if not present will use the previous metadata; force the `{}` value if you want to clean up the associated metadata. | A sample metadata looks like this: ```json\n{ \"type\": \"test\", \"domain\": \"Knowledge\", \"year\": 2023, \"quarter\": \"q3\"\n}\n``` ### Response Equivalent to [[29##GET/profile/{name}/document/{id}|Get Response]]. Possible return errors: - [[23##2021|2021]] ### cURL Samples ```bash\n# reindex existing file\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'documentId: GUID' # Reindex existing file with new content\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'documentId: Document_GUID' \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"' # Reindex existing file with new metadata\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'documentId: Document_GUID' \\\n--form 'metadata=\"{\\\"type\\\":\\\"test\\\",\\\"domain\\\":\\\"Knowledge\\\",\\\"year\\\":2023,\\\"quarter\\\":\\\"q3\\\"}\"' # Reindex existing file with new content and metadata\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'documentId: Document_GUID' \\\n--form 'metadata=@\"/C:/temp/upload_file_metadata.json\"' \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"'\n``` ## DELETE/profile/{name}/document/{id} Deletes a Document. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) |\n| `id` | `string` | Document Id (required) | ### Response StatusCode `200` is shown when successfully deleted; otherwise, `400*` is displayed with a collection of errors. ### cURL Example ```bash\ncurl -X DELETE \"$BASE_URL/v1/search/profile/{name}/document/{id}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## POST/execute Executes a search query. For more details, read [[33]]. {{{12}}}{{{17}}}##MARKDOWN##\n\n\"description\": \"Obtain weather conditions in any country\"\n},\n{\n\"title\": \"Rain probability\",\n\"description\": \"Get the rain probability in any location\"\n}\n],\n\"examplesPrompt\": [\n{\n\"title\": \"First Prompt Example\",\n\"description\": \"First Prompt Example Description\",\n\"promptText\": \"You are an assistant specializing in...\"\n}\n]\n}\n}'\n``` ## PUT/profile/{name} Updates a RAG Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) | ### Request Body Equivalent to [[29##POST/profile|Post Request]], but the following elements are discarded: - `name` element\n- `indexOptions` section In addition, `status` element can be specified by taking values: - 1: Enabled\n- 2: Disabled If a RAG Assistants had `WelcomeData` and an update does not include this section, the existing information is deleted and becomes empty. Therefore, if you want to add new information to the `WelcomeData`, you must include both the old and the new information. If you only send the new one, it will completely replace the old one. ### Response Equivalent to Get Response. ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\" \\\n-d '{\n\"description\": \"Updated RAG assistant\",\n\"status\": 1,\n\"searchOptions\": {\n\"historyCount\": 4,\n\"llm\": {\n\"temperature\": 0.5,\n\"maxTokens\": 1000\n},\n\"search\": {\n\"k\": 2,\n\"prompt\": \"You are an Assistant, only reply using the following context:\\n{context}\\n Question is: {question}\\n\",\n\"scoreThreshold\": 0.2\n}\n}\n}'\n``` #### Sample with Welcome Data Update ```bash\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\" \\\n-d '{\n\"name\": \"Test-Profile-WelcomeData\",\n\"welcomeData\": {\n\"title\": \"Welcome to RAG Update\",\n\"description\": \"It is a RAG created with WelcomeData via API\",\n\"features\": [\n{\n\"title\": \"Updated weather conditions\",\n\"description\": \"Get the state of the weather in any country\"\n},\n{\n\"title\": \"Rain probability updated\",\n\"description\": \"Get the rain probability in any location\"\n}\n],\n\"examplesPrompt\": [\n{\n\"title\": \"First Prompt Example\",\n\"description\": \"First Prompt Example Description\",\n\"promptText\": \"You are an assistant specializing in...\"\n}\n]\n}\n}'\n``` ## DELETE/profile/{name} Deletes a RAG Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) | ### Response StatusCode `200` is shown when successfully deleted; otherwise, StatusCode `400*` is shown with a collection of errors. ### cURL Sample ```bash\ncurl -X DELETE \"$BASE_URL/v1/search/profile/{name}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## GET/profile/{name}/documents Lists the documents for a RAG Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name |\n| `skip` | `integer` | Number of documents to skip |\n| `count` | `integer` | Number of documents to return (defaults to 10) | ### Response ```json\n{ \"documents\": [ { \"extension\": \"string\", \"id\": \"string\", \"name\": \"string\", \"timestamp\": \"timestamp\", \"url\": \"string\", \"indexStatus\": \"string\", /* Unknown, Starting, Failed, Pending, Success */ \"indexDetail\": \"string\" }, ... ], \"count\": \"integer\" /* Total number of documents */\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/search/profile/{name}/documents\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n# Use the optional skip and count parameters\n$BASE_URL/v1/search/profile/{name}/documents?skip={skip}&count={count}\n``` ## PUT/profile/{name}/documents Reindex all existing documents for the RAG assistant. ### Response An empty Json with statusCode: 200. ```json\n{}\n``` ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/documents\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n``` ## DELETE/profile/{name}/documents Deletes all documents associated to the specified RAG assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) | ### Response StatusCode `200` is shown when successfully deleted; otherwise, StatusCode `400*` is shown with a collection of errors. ### cURL Sample ```bash\ncurl -X DELETE \"$BASE_URL/v1/search/profile/{name}/documents\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## GET/profile/{name}/document/{id} Using the `{name}` RAG Assistants, it gets details about the `{id}` document. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) |\n| `id` | `string` | Document Id (required) | ### Response ```json\n{ \"extension\": \"string\", \"id\": \"string\", \"indexStatus\": \"string\", /* Unknown, Starting, Failed, Pending, Success */ \"indexDetail\": \"string\", \"keyName\": \"string\", \"metadata\": [ { \"key\": \"string\", \"value\": \"string\" }, ... ], \"name\": \"string\", \"timestamp\": \"timestamp\", \"url\": \"string\"\n}\n``` For the *Failed* status, check the returned *Index Detail* field for further information. | **Index Detail** | **Description** |\n|---|---|\n| Invalid content | The associated file does not have text content; the common case is a PDF file composed of images. These files are not able to be used in a RAG Assistant. | ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/search/profile/{name}/document/{id}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## POST/profile/{name}/document Uploads a Document to the associated `{name}` RAG Assistants. Note that the file extension must be a [[137|supported one]]. ### Request Body The supported options are `binary` or `multipart/form-data` including a `File` type. #### Binary It is useful for its simplicity and encodes the binary data directly in the request body. Set the request with the associated `Content-Type` header to indicate the type of data being sent (e.g., `application/pdf`, `text/plain`). It is mandatory to set a `filename` header value with the document name and extension. For example: ```\nfilename: SampleFile.pdf\n``` Note that this option does not enable to upload [[60##Step 3: Upload documents|metadata]], for this case use the other upload option below (Form-Data). #### Form-Data This format allows you to include both binary data and other form fields in a single request. Each part of the data (binary file, text fields, etc.) is separated by a boundary and sent as separate parts. It is expected to be used for large files. You can also specify different [[581|ingestion options]] that will decide how files are processed. If you want to attach metadata to the file to be processed during ingestion, add a `metadata` form-data variable with the desired value; remember that the expected format is a `key/value` JSON list. For example, the following is a valid metadata for a Document: ```json",
          "evaluationResultRowContextDocId": "886dd74f-79ea-4603-afb3-b7ca8298555b",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "2c490c3b-fca7-48e7-b1ae-64fec3c9d8d2",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?29",
          "evaluationResultRowContextName": "29",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#RAG File Formats By default the following upload file formats are supported: | **Category** | **Extension** |\n|---|---|\n|Text|`.txt`|\n|Portable|`.pdf`<sup>2</sup>, `.md`, `.msg`<sup>2</sup>, `.org`<sup>2</sup>, `.rst`<sup>2</sup>, `.csv`<sup>1</sup>, `.tsv`|\n|General|`.eml`<sup>2</sup>, `.html`<sup>2</sup>, `.xml`|\n|Microsoft Office|`.doc`<sup>2</sup>, `.docx`<sup>2</sup>, `.ppt`<sup>2</sup>, `.pptx`<sup>2</sup>, `.xls`, `.xlsx`, `.rtf`|\n|Open Document|`.odt`<sup>2</sup>, `.odp`<sup>2</sup>, `.ods`<sup>2</sup>|\n|Ebook|`.epub`<sup>2</sup>|\n|JSON|`.json`, `.jsonl`|\n|Images|`.png`, `.jpeg/.jpg`, `.tiff`, `.bmp`, `.gif`|\n|Audio<sup>3</sup>|`.mp3`, `.ogg`, `.wav`, `.flac`, `.aac`, `.m4a`|\n|Video<sup>3</sup>|`.mp4`, `.avi`, `.mov`, `.mkv`|\n|Custom|`.custom`, `.web`| <sup>1</sup> - When using the [[581|legacy provider]], the separator must be a comma (`,`) and not a semicolon (`;`). <sup>2</sup> - supports the `pageNumber` element when using the [[581|GEAI provider]]. <sup>3</sup> - Supported since `2025-05` release. Take into account that simple files like `.csv`, `.txt` are expected to have **utf-8** encoding. For [[441]], you can extend the supported file extensions by changing the `FILE_TYPES` parameter. ## Custom Formats - [[201|.custom]]: use it to manually configure your desired chunks and metadata.\n- [[202|.web]]: use it to crawl a web site. ## Changelog ### Revision #8 * _New extensions_: `.mp4`, `.avi`, `.mov`, `.mkv`, `.mp3`, `.wav`, `.ogg`, `.aac`, `.flac`, `.m4a`, `.wma`. ### Revision #5 * _New extensions_: `doc`, `ppt`, `xls`, `msg`, `org`, `rtf`, `rst`, `tsv`, `eml`, `tiff`, `bmp`, `epub`.\n* _new options_ when processing `csv`, `xls*`. ## On-Premises Check the System parameters as detailed for the RAG module: | **Revision** | **Parameter** | **Value** |\n|---|---|---|\n| 2 | `FILE_TYPES` | `txt,pdf,docx,pptx,xlsx,odt,odp,ods,xlsx,epub,json,jsonl,csv,java,cs,py,js,ts,xml,html,web,custom,md` |\n| 5 | `FILE_TYPES` | `txt,pdf,doc,docx,ppt,pptx,xls,xlsx,odt,odp,ods,rtf,rst,epub,json,jsonl,csv,tsv,java,cs,py,js,ts,web,custom,md,png,gif,jpeg,jpg,mp3,mp4,msg,org,tiff,bmp,eml,xml,html` |\n| 8 | `FILE_TYPES` | `txt,pdf,doc,docx,ppt,pptx,xls,xlsx,odt,odp,ods,rtf,rst,epub,json,jsonl,csv,tsv,java,cs,py,js,ts,web,custom,md,png,gif,jpeg,jpg,mp3,m4a,mkv,aac,ogg,flac,wav,mp4,avi,mov,msg,org,tiff,bmp,eml,xml,html` | ## See Also [[581]] {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "84a8ad9b-723e-45dd-99c1-7383a327b4c3",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "7c72ef69-3739-440b-8cfd-cc97abf00592",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?137",
          "evaluationResultRowContextName": "137",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Supported models and file formats This table provides a detailed overview of the multimodal models supported by Globant Enterprise AI and their compatibility with different file formats such as `pdf`<sup>(1)</sup>, `txt`, `png`, `jpg`, `jpeg`, `gif`, `mp3` and `mp4`. The size of the supported files will depend on the context window of each model. <div style=\"background-color: rgb(242,242,242);border-left: 6.0px solid rgb(128,128,128);margin-bottom: 15.0px;padding: 4.0px 12.0px;\"><b>Note</b>:All non-multimodal models (not included in the table) accept text (`txt`) and PDF (`pdf`) files.</div> | **Provider** | **Model** | **Supported formats** | **Context window (tokens)**|\n|---|---|---|---|\n| anthropic | claude-3-5-sonnet-20241022 | txt, pdf, jpeg, jpg, png, gif | 200000 |\n| anthropic | claude-3-haiku-20240307 | txt, pdf, png, gif | 200000 |\n| anthropic | claude-3-opus-20240229 | txt, pdf, png, gif | 200000 |\n| anthropic | claude-3-5-sonnet-20240620 | txt, pdf, png, gif | 200000 |\n| anthropic | claude-3-7-sonnet-latest | txt, pdf, png, gif, jpeg | 200000 |\n| awsbedrock | anthropic.claude-3-opus | txt, pdf | 200000 |\n| awsbedrock | meta.llama3-2-11b | txt, pdf | 128000 |\n| awsbedrock | meta.llama3-2-90b | txt, pdf | 128000 |\n| awsbedrock | anthropic.claude-3-7-sonnet | txt, pdf, png, gif, jpeg | 200000 |\n| awsbedrock | anthropic.claude-3.5-sonnet-v2 | txt, pdf, jpeg, jpg, png, gif | 200000 |\n| awsbedrock | anthropic.claude-3-haiku | txt, pdf, png, gif | 200000 |\n| awsbedrock | anthropic.claude-3.5-sonnet | txt, pdf, png, gif | 200000 |\n| awsbedrock | amazon.nova-lite-v1:0 | txt, pdf, png, gif | 300000 |\n| awsbedrock | amazon.nova-pro-v1:0 | txt, pdf, png, gif | 300000 |\n| azure | gpt-4o | txt, pdf, gif, jpg, png | 128000 |\n| azure | gpt-4o-mini | txt, pdf, gif, jpg, png | 128000 |\n| azure | o1 | txt, pdf, jpg, png, gif | 200000 |\n| gemini | gemini-2.0-flash-thinking-exp-01-21 | txt, pdf, png, mp3 | 1048576 |\n| gemini | gemini-1.5-pro-latest | txt, pdf, jpg | 2097152 |\n| gemini | gemini-exp-1206 | txt, pdf, jpg | 2097152 |\n| gemini | gemini-2.0-pro-exp | txt, pdf, jpg | 2097152 |\n| gemini | gemini-1.5-flash-8b-exp-0827 | txt, pdf, jpg, png, gif, mp3 | 1048576 |\n| gemini | gemini-1.5-flash-latest | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| gemini | gemini-2.0-flash-exp | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| gemini | gemini-2.0-flash-lite-preview | txt, pdf, jpg, png, mp3 | 1048576 |\n| gemini | gemma-3-27b-it | txt, pdf | 131072 |\n| gemini | gemini-2.5-pro-exp-03-25 | txt, pdf, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| groq | meta-llama-4-scout-17b-16e-instruct | txt, pdf, jpg, jpeg, png | 131072 |\n| groq | meta-llama-4-maverick-17b-128e-instruct | txt, pdf, jpg, jpeg, png | 131072 |\n| groq | llama-3.2-11b-vision-preview | txt, pdf | 128000 |\n| groq | llama-3.2-90b-vision-preview | txt, pdf | 128000 |\n| nvidia | meta-llama-4-scout-17b-16e-instruct | txt, pdf, png, jpeg, jpg, gif | 131072 |\n| nvidia | meta-llama-4-maverick-17b-128e-instruct | txt, pdf, png, jpeg, jpg, gif | 32768 |\n| openai | chatgpt-4o-latest | txt, pdf, jpg, png, gif | 128000 |\n| openai | o1 | txt, pdf, jpg, png, gif | 200000 |\n| openai | o1-pro | pdf, jpg, jpeg, png, gif, webp | 200000 |\n| openai | o3 | txt, pdf, jpg, jpeg, png, gif | 200000 |\n| openai | o4-mini | txt, pdf, jpg, jpeg, png, gif | 200000 |\n| openai | gpt-4.1 | txt, pdf, gif, jpg, png, jpeg | 1047576 |\n| openai | gpt-4.1-mini | txt, pdf, gif, jpg, png, jpeg | 1047576 |\n| openai | gpt-4.1-nano | txt, pdf, gif, jpg, png, jpeg | 1047576 |\n| openai | gpt-4.5-preview | txt, pdf, gif, jpg, png, jpeg | 128000 |\n| openai | gpt-4o | txt, pdf, gif, jpg, png | 128000 |\n| openai | gpt-4o-mini | txt, pdf, gif, jpg, png | 128000 |\n| openai | gpt-4o-2024-11-20 | txt, pdf, gif, jpg, png | 128000 |\n| openai | gpt-4o-search-preview | txt, pdf | 128000 |\n| openai | gpt-4o-mini-search-preview | txt, pdf | 128000 |\n| openrouter | [https://openrouter.ai/openrouter/auto|auto] | txt, pdf, gif, jpg, jpeg, png | depends on the selected model |\n| sambanova | Llama-3.2-11B-Vision-Instruct | txt, pdf, png, jpg | 4096 |\n| sambanova | Llama-3.2-90B-Vision-Instruct | txt, pdf | 4096 |\n| vertex_ai | claude-3-7-sonnet-20250219 | txt, pdf, png, gif, jpeg | 200000 |\n| vertex_ai | claude-3-5-sonnet-20240620 | txt, pdf, pdf, png, gif | 200000 |\n| vertex_ai | claude-3-haiku-20240307 | txt, pdf, png, gif | 200000 |\n| vertex_ai | claude-3-opus-20240229 | txt, pdf, png, gif | 200000 |\n| vertex_ai | meta.llama-3.2-90b-vision-instruct-maas | txt, pdf | 128000 |\n| vertex_ai | meta-llama-4-scout-17b-16e-instruct-maas | txt, pdf, jpeg, jpg, png, gif | 10000000 |\n| vertex_ai | meta-llama-4-maverick-17b-128e-instruct-maas | txt, pdf, jpeg, jpg, png, gif | 1000000 |\n| vertex_ai | claude-3-5-sonnet-v2-20241022 | txt, pdf, jpeg, jpg, png, gif | 200000 |\n| vertex_ai | gemini-2.0-flash-thinking-exp-01-21 | txt, pdf, jpg, png, mp3, mp4 | 32000 |\n| vertex_ai | gemini-1.5-pro | txt, pdf, jpg, png, gif, mp3, mp4, ogg | 2097152 |\n| vertex_ai | gemini-1.5-pro-002 | txt, pdf, jpg, png, gif, mp3, mp4, ogg | 2097152 |\n| vertex_ai | gemini-2.0-flash-exp | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| vertex_ai | gemini-2.0-flash-001 | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| vertex_ai | gemini-2.0-flash-lite | txt, pdf, jpg, jpeg, png, mp3, mp4, ogg | 1048576 |\n| vertex_ai | gemini-2.0-flash | txt, pdf, jpg, jpeg, png, mp3, mp4, ogg | 1048576 |\n| vertex_ai | gemini-2.5-pro-exp-03-25 | txt, pdf, jpg, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| vertex_ai | gemini-2.5-flash-preview-04-17 | txt, pdf, jpg, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| vertex_ai | gemini-2.5-pro-preview-05-06 | txt, pdf, jpg, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| xai | grok-vision-beta | txt, pdf, png | 8192 |\n| xai | grok-2-vision-1212 | txt, pdf, png | 32768 | <sup>(1)</sup> - Encrypted files are not supported. ## See Also [[200]] [[843]] {{{12}}} {{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "2021f0ec-69d2-4e24-a755-4628261a3c84",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "8e1fea4d-a14a-4db2-8824-8b75e4ba1c9a",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?406",
          "evaluationResultRowContextName": "406",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# RAG Assistants Introduction [Retrieval Augmented Generation](https://www.promptingguide.ai/techniques/rag) (RAG) is an approach that combines information retrieval from unstructured data and text generation to improve performance on tasks such as question answering. In the retrieval phase, a selective search is performed on a set of documents, identifying related information and efficiently reducing the search space. This approach ensures that focus is placed on the most relevant and meaningful information. The information retrieved is effectively integrated and expanded in prompting, providing deeper connections and insights into unstructured data. This process enriches the information with additional details and relationships. In this way, it encompasses not only immediate relevance, but also broader contextual connections and helps in understanding the retrieved information. In the subsequent text generation phase, this expanded data set is used to produce coherent and contextual responses. The generative model, by working on the information previously retrieved and added to the prompting (in-context learning), not only improves the system's ability to generate accurate and relevant responses, but also effectively incorporates the enriched details from the textual information. This process enhances the system's ability to generate accurate and relevant content, effectively completing the cycle in the comprehensive RAG approach. ## RAG Assistants in Globant Enterprise AI ![Globant Enterprise AI](./images/43.png) Globant Enterprise AI makes it possible to use RAG assistants to chat with or search for information stored in documents (unstructured data). This functionality is enabled through the [RAG Assistants API](29.html) or RAG Assistants section of the Globant Enterprise AI backend. The different phases of this process are described below: ### Data Ingestion The initial phase, known as the Ingestion stage, involves loading various types of documents from multiple sources. This phase is not only limited to data acquisition but also includes the configuration of chunks. #### Documents and other data sources Data can be loaded in different formats and from different sources. In addition, other data sources generated by end users are considered. #### Configuration of Chunks In parallel with document loading, the configuration of chunks is performed to optimize information management. These chunks act as organizational fragments, enabling efficient data segmentation. This process goes beyond simple data partitioning, as it integrates with the Index Profile and its chunking strategy. This connection ensures that data segmentation is consistent and aligned with the specific requirements of the index, thus optimizing data preparation for subsequent processing. The default chunking strategy is as follows: - chunkSize: 1000 characters\n- chunkOverlap: 100 characters ### Retrieval In the Retrieval stage, the data retrieval process is started, leveraging the previously ingested and organized information. During this phase, the vector database is accessed and the documents loaded during Ingestion are indexed and stored efficiently. The main component of this stage is data extraction through embeddings and access to the vector store. #### Embeddings Embeddings, derived from the context and represented by a numeric array, capture the contextual essence of the documents in terms of chunks and the user's query. #### Vector Store The Vector Store, powered by embeddings and metadata, connects with providers, index parameters, and distance metrics to ensure accurate and contextualized retrieval of data chunks relevant to the given query. ### Generation The Generation stage marks the point at which the Globant Enterprise AI architecture focuses on generating relevant and contextually consistent responses. In this process, the system uses the RAG Assistant configuration to know which model to access and with which parameters. #### RAG Assistant This assistant incorporates key elements such as Prompts, LLMs, and search retrieval parameters to define the search strategy. Prompts act as guides to contextualize the responses, while the LLM contributes to the consistency and relevance of the generated content. In addition, the assistant makes it possible to add variable-based adjustments and filters, enhancing the customization of the generated responses. This adaptability allows for responses to be specific and relevant to the particular needs of the end user. ### End user interaction Finally, the Globant Enterprise AI architecture enables interaction with the user through the API. This interface facilitates smooth and efficient communication between end users and RAG Assistants, completing the cycle and providing answers to queries in an efficient manner.",
          "evaluationResultRowContextDocId": "29364b9b-2163-4296-b051-ecf6ef244703",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "c96d31b6-525f-4485-985b-a5c44611537c",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?44",
          "evaluationResultRowContextName": "44",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# File support for multimodal models Multimodal models allow interactions that go beyond traditional text-based prompts. These models can understand and process information from a variety of sources, including text, file documents, images, audio, and video. This means they can analyze, process, and generate content based on a richer and more complete set of inputs. Globant Enterprise AI implements multimodal models, allowing you to interact with models such as Gemini and OpenAI in a completely new way. By combining text with files, a world of possibilities opens up for analyzing, processing, and generating content. These models can directly receive files alongside your prompts, eliminating the need to manually extract and inject file content. This direct file integration offers several advantages. - Simplifies file handling. You no longer need to extract and inject file content into the prompt.\n- Files are sent directly to the model, streamlining the process and enabling direct model interaction.\n- Empowers you to interact directly with files within your prompts, making it easier to analyze, process, and generate content based on their contents. This eliminates the need for manual extraction and injection, simplifying your workflow.\n- Leverages the information contained in files to provide more comprehensive and insightful responses, enhancing their capabilities. ## File References in Prompts You can reference files in your prompts using a specific format: `{file:filename}`. For example, to reference a file named \"report.pdf\" you would write: `{file:report}`. > **Note**: This reference format only works for files that have already been uploaded to your project. If you're uploading a file for the first time, you'll need to use the standard file upload mechanism within the platform. ## How Files are Managed in Your Project 1. **File Storage**: All files uploaded to your project are stored securely and can be accessed by your prompts.\n2. **File Access**: When you reference a file in your prompt, the system automatically retrieves it and sends it to the multimodal model.\n3. **File Types**: The system supports a wide range of file types, including text documents, images, audio files, and more. ## Sample Suppose you have a document called \"product_specs.txt\" loaded in your project. You can use it in your prompt as follows: ```plaintext\n\"Please summarize the key features of the product described in {file:product_specs}\".\n``` The multimodal model will then access the \"product_specs.txt\" file and use its contents to generate a summary of the product features. ## See Also [Supported models and file formats](406.html)",
          "evaluationResultRowContextDocId": "381b4a5d-cb99-40b8-a58e-e67f82082418",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "ef277805-8dd8-4a9a-9bce-043495f9778f",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?345",
          "evaluationResultRowContextName": "345",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.7,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        }
      ],
      "dataSetRowId": "1087fb22-925e-4dfe-945f-c33608db39c7",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0048454800",
      "evaluationResultRowEndDate": "2025-05-13T11:12:06",
      "evaluationResultRowOutput": "The types of files supported in Globant Enterprise AI for upload and use, especially with RAG Assistants and multimodal models, include a wide range of text, document, image, audio, and video formats:\n\n| **Category </B>       | **Extensions </B>                                                                                    |\n|---------------------|--------------------------------------------------------------------------------------------------|\n| Text                | `.txt`                                                                                           |\n| Portable            | `.pdf`, `.md`, `.msg`, `.org`, `.rst`, `.csv`, `.tsv`                                           |\n| General             | `.eml`, `.html`, `.xml`                                                                           |\n| Microsoft Office     | `.doc`, `.docx`, `.ppt`, `.pptx`, `.xls`, `.xlsx`, `.rtf`                                       |\n| Open Document        | `.odt`, `.odp`, `.ods`                                                                            |\n| Ebook               | `.epub`                                                                                          |\n| JSON                | `.json`, `.jsonl`                                                                                |\n| Images              | `.png`, `.jpeg`, `.jpg`, `.tiff`, `.bmp`, `.gif`                                                |\n| Audio               | `.mp3`, `.ogg`, `.wav`, `.flac`, `.aac`, `.m4a`                                                 |\n| Video               | `.mp4`, `.avi`, `.mov`, `.mkv`                                                                  |\n| Custom              | `.custom`, `.web`                                                                                |\n\nAdditional notes:\n- Support for audio and video formats started from the 2025-05 release.\n- Simple files like `.csv` or `.txt` are expected to have UTF-8 encoding.\n- Some document extensions support features like `pageNumber` element with certain providers.\n- Multimodal models support these files directly for integrated processing.\n- Encrypted PDF files are not supported.\n\nThis support allows interacting with multimodal models such as Gemini, OpenAI, Anthropic, AWS Bedrock, and others using various file formats, enabling text, image, audio, and video inputs as detailed in the Supported Models table.\n\nFor full details, see the list of supported extensions and file formats here:  \nhttps://wiki.genexus.com/enterprise-ai/wiki?137  \nhttps://wiki.genexus.com/enterprise-ai/wiki?406  \nhttps://wiki.genexus.com/enterprise-ai/wiki?345\n\n[4](https://wiki.genexus.com/enterprise-ai/wiki?137)  \n[1](https://wiki.genexus.com/enterprise-ai/wiki?406)  \n[3](https://wiki.genexus.com/enterprise-ai/wiki?345)",
      "evaluationResultRowStartDate": "2025-05-13T11:11:57",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "10722"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# .web File Format The `.web` file format is a JSON file with a `.web` extension used to configure website crawling for [RAG Assistant](44.html). ### .web File Format Parameters The following parameters are available in the `.web` file format: | **Element Name** | **DataType** | **Description** |\n|---|---|---|\n| url | string | Mandatory: the base URL to crawl. |\n| excludeDirs | string[] | Optional list of web page directories to exclude; they must be absolute references. Here you can exclude unnecessary dependencies referenced in the HTML. |\n| maxDepth | number | The maximum depth to crawl. By default, it is set to 3; the system controls it cannot have a higher value. |\n| timeout | number | Timeout for each request, in the unit of milliseconds. By default, it is set to 10000 (10 seconds). |\n| preventOutside | boolean | Whether to prevent crawling outside the root base URL; _true_ by default. |\n| selectors | string[] | List of page selectors to consider; it uses the _body_ tag by default, and can be used in the following combinations: <ul><li>`*` - universal selector;</li><li>`div` - tag name;</li><li>`.foo` - class name;</li><li>`#bar` - id;</li><li>`[baz]` - attribute presence;</li><li>`[baz=buzz]` - attribute value (with any operators and also quotes and case sensitivity modifiers - [syntax](https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors#syntax);</li><li>`+` and `>` combinators (other combinators are not supported).</li></ul> It is recommended to analyze some of the web pages of the site and select the main container where the valuable text is set. |\n| linkBrackets | boolean | Add link brackets when processing an anchor tag; _false_ by default. |\n| ignoreHref | boolean | Remove href references; _false_ by default. | ### Using the .web File Format Once the file is uploaded (as shown in [Upload documents](60.html)), the ingestion process will crawl the site starting in the base URL (_url_ parameter) considering the _maxDepth_ parameter (to follow associated links in each page up-to _maxDepth_) and will exclude all base URLs (specified in the _excludeDirs_ parameter). When the process is finished, the crawl status will be displayed as usual, in this case associated to the uploaded file (in general as _Success_): ![Crawl Status](./images/292.png) You can check the [Chunks file](60.html) with the result of the processing, and validate that the detected _plain text_ by the crawler makes sense (to be used as semantic information for a RAG assistant). ### Sample Suppose you want to crawl a Wikipedia article. In this case, a possible `wikipedia_sample.web` file to upload could be configured as follows: ```json\n{ \"url\": \"https://en.wikipedia.org/wiki/GeneXus\", \"maxDepth\": 1, \"preventOutside\": true, \"timeout\": 10000, \"selectors\": [ \"#firstHeading\", \"#bodyContent\" ], \"linkBrackets\": false, \"ignoreHref\": true, \"excludeDirs\": [ \"https://en.wikipedia.org/static/\", \"https://en.wikipedia.org/w/\" ]\n}\n``` Note the following: - **url:** The base URL of the Wikipedia article.\n- **maxDepth:** Only crawl one level of the site.\n- **selectors:** Use the selectors by ID (`firstHeading`, `bodyContent`) to only process the important content from the site.\n- **linkBrackets:** Remove link references to keep the plain text as clean as possible.\n- **excludeDirs:** Exclude a couple of directories. #### Resulting Chunks File The resulting Chunks file will look something like this: ```json\n[ ... { \"pageContent\": \"GENEXUS From Wikipedia, the free encyclopedia Computer programming tool\\nThis article is about a computer programming tool. For the Fear Factory album, see Genexus (album). GeneXusDeveloper(s)GeneXusInitial release1988; 36 years ago (1988)Stable release\\nGeneXus 17\\n...\", \"metadata\": { \"source\": \"https://en.wikipedia.org/wiki/GeneXus\", \"language\": \"en\", \"description\": \"GeneXus - Wikipedia\", \"id\": \"someGUID\" } }, { \"pageContent\": \"...can be generated include COBOL, Java, Objective-C, RPG, Ruby, Visual Basic, and Visual FoxPro.[3] Some of the DBMSs supported are\\nMicrosoft SQL Server, Oracle, IBM Db2, Informix, PostgreSQL, and MySQL. GeneXus was developed by Uruguayan company ARTech Consultores SRL which later renamed to Genexus SA.[4] The latest version is...\", \"metadata\": { \"source\": \"https://en.wikipedia.org/wiki/GeneXus\", \"language\": \"en\", \"description\": \"GeneXus - Wikipedia\", \"id\": \"someGUID\" } } ...\n]\n``` ### See Also [.custom File Format](201.html)",
          "evaluationResultRowContextDocId": "9020065f-bb77-40f1-8f88-9aeeac036331",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "047e7fd6-c630-45f6-9eeb-65e528699443",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?202",
          "evaluationResultRowContextName": "202",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#RAG Assistants API This API allows you to define different [[44|RAG Assistants]]. To use the API, check the [[20##Generic Variables|generic variables]] needed. In addition, read the explanation about parameters in [[64]]. > The following endpoints require a Globant Enterprise AI API token related to **project** scope. ## Endpoints Below is a summary of the available endpoints for this API: | Method | Path | Description |\n|---|---|---|\n| `GET` | `/profiles` | Gets all RAG assistants from a project |\n| `GET` | `/profile/{name}` | Gets a specific RAG assistant |\n| `POST` | `/profile` | Creates a new RAG assistant |\n| `PUT` | `/profile/{name}` | Updates a RAG assistant |\n| `DELETE` | `/profile/{name}` | Deletes a RAG assistant |\n| `GET` | `/profile/{name}/documents` | Gets documents for a RAG assistant |\n| `PUT` | `/profile/{name}/documents`<sup>(1)</sup> | Reindex all documents for the selected RAG assistant |\n| `DELETE` | `/profile/{name}/documents` | Deletes all documents for a RAG assistant |\n| `GET` | `/profile/{name}/document/{id}` | Retrieves Document information |\n| `POST` | `/profile/{name}/document` | Uploads a Document |\n| `PUT` | `/profile/{name}/document`<sup>(1)</sup> | Reindex a Document |\n| `DELETE` | `/profile/{name}/document/{id}` | Deletes a Document |\n| `POST` | `/execute` | Executes a query against the defined RAG assistant | <sup>(1)</sup> - Supported since `2025-05` release. **Note**: Keep in mind that the `searchProfiles` and `{name}` parameters refer to RAG Assistants. In addition, the `{name}` parameter, which represents the name of the RAG Assistants, must not contain blanks. Instead, they must be replaced by the + symbol. ## GET/profiles Retrieves all the RAG Assistants for a Project. ### Response ```json\n{ \"projectActive\": boolean, \"projectDescription\": \"string\", \"projectId\": \"string\", \"projectName\": \"string\", \"projectStatus\": integer, /* 0:Active, 1:Deleted, 2:Hidden */ \"searchProfiles\": [ { \"description\": \"string\", \"name\": \"string\" }, ... ]\n}\n``` The `description` parameter is required for the [[33|chat]] option. ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/search/profiles\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"Accept: application/json\"\n``` ## GET/profile/{name} Gets RAG Assistants `{name}` details. ### Response ```json\n{ \"name\": \"string\", \"description\": \"string\", \"indexOptions\": { \"chunks\": { \"chunkOverlap\": integer, \"chunkSize\": integer } }, \"searchOptions\": { \"historyCount\": integer, \"llm\": { \"cache\": boolean, \"frequencyPenalty\": decimal, \"maxTokens\": integer, \"modelName\": \"string\", \"n\": integer, \"presencePenalty\": decimal, \"provider\": \"string\", \"stream\": boolean, \"temperature\": decimal, \"topP\": decimal, \"type\": \"string\", \"verbose\": boolean }, \"retriever\": { \"type\": \"string\" /* vectorStore, multiQuery, selfQuery, hyde, contextualCompression */ }, \"search\": { \"k\": integer, \"prompt\": \"string\", \"returnSourceDocuments\": boolean, \"scoreThreshold\": decimal, \"template\": \"string\" } }, \"status\": integer /* 1:Enabled, 2:Disabled */\n}\n``` The `type` parameter is explained in [[70|RAG Assistants - Configuration Retrieval]]. ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/search/profile/{name}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## POST/profile Creates a RAG Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) |\n| `descrition` | `string` | Description of the RAG assistant |\n| `template` | `string` | Name of an existing RAG to base the configuration (optional), empty by default | ### Request Body ```json\n{ \"name\": \"string\", /* Required */ \"description\": \"string\", \"template\": \"string\", \"searchOptions\": { \"embeddings\": { \"provider\": \"string\", \"modelName\": \"string\", \"useProxy\": boolean, \"mode\": \"string\", // \"\" or \"multimodal\" \"type\": \"string\", \"batchSize\": integer, // defaults to 100 \"dimensions\": integer // defaults to 1536 }, \"historyCount\": integer, \"llm\": { \"cache\": boolean, \"frequencyPenalty\": decimal, \"maxTokens\": integer, \"modelName\": \"string\", \"n\": integer, \"presencePenalty\": decimal, \"provider\": \"string\", \"stream\": boolean, \"temperature\": decimal, \"topP\": decimal, \"type\": \"string\", /* empty value (default) or json_object */ \"verbose\": boolean, \"guardrails\": [...] // options available are: \"input-moderation-guardrail\", \"llm-output-guardrail\", \"prompt-injection-guardrail\" }, \"search\": { \"k\": integer, \"type\": \"string\", /* similarity (default) or mmr */ \"fetchK\": number, /* valid when using mmr type */ \"lambda\": decimal, /* valid when using mmr type */ \"prompt\": \"string\", \"returnSourceDocuments\": boolean, \"scoreThreshold\": decimal, \"template\": \"string\" }, \"retriever\": { \"type\": \"string\", /* vectorStore, multiQuery, selfQuery, hyde, contextualCompression */ \"searchType\": \"similarity | similarity_hybrid | semantic_hybrid\" /* Azure AISearch specific, defaults to similarity */ \"step\": \"string\", /* all (default) | documents */ \"prompt\": \"string\" /* not needed when using vectorStore */ } }, \"indexOptions\": { \"chunks\": { \"chunkOverlap\": integer, \"chunkSize\": integer }, \"useParentDocument\": boolean, /* false by default */ \"childDocument\": { /* valid if the previous element is true */ \"childK\": decimal, \"child\": { \"chunkSize\": decimal, \"chunkOverlap\": decimal, \"contentProcessing\": \"string\" /* '' | 'clean' */ } } }, \"welcomeData\": { \"title\": \"string\", \"description\": \"string\", \"features\": [ { \"title\": \"string\", \"description\": \"string\" }, ... ], \"examplesPrompt\": [ { \"title\": \"string\", \"description\": \"string\", \"promptText\": \"string\" }, ... ] }\n}\n``` The LLM type option using *json_object* can be checked [[69##type|here]]. {{{216| If you do not provide the `WelcomeData` section when creating a RAG Assistant, it will be empty}}} ### Response Equivalent to [[29##GET/profile/{name}|Get Response]]. ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/v1/search/profile\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\" \\\n-d '{\n\"name\": \"my RAG assistant\",\n\"description\": \"My awesome profile\",\n\"searchOptions\": {\n\"historyCount\": 2,\n\"llm\": {\n\"temperature\": 0.1,\n\"maxTokens\": 1500,\n\"modelName\": \"gpt-3.5-turbo-16k\"\n},\n\"search\": {\n\"k\": 5\n}\n}\n}'\n``` #### Sample with WelcomeData ```bash\ncurl -X POST \"$BASE_URL/v1/search/profile\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\" \\\n-d '{\n\"name\": \"Test-Profile-WelcomeData\",\n\"description\": \"Test Profile with WelcomeData\",\n\"searchOptions\": {\n\"historyCount\": 2,\n\"llm\": {\n\"cache\": false,\n\"temperature\": 0.1,\n\"maxTokens\": 999,\n\"modelName\": \"gpt-3.5-turbo-16k\",\n\"verbose\": true\n},\n\"search\": {\n\"k\": 5,\n\"returnSourceDocuments\": false,\n\"scoreThreshold\": 0,\n\"prompt\": \"Use {context} and {question}\"\n}\n},\n\"indexOptions\": {\n\"chunks\": {\n\"chunkSize\": 999,\n\"chunkOverlap\": 99\n}\n},\n\"welcomeData\": {\n\"title\": \"Welcome to the RAG\",\n\"description\": \"It is a RAG created with WelcomeData via API\",\n\"features\": [\n{\n\"title\": \"Weather conditions\",\n\n{ \"type\": \"test\", \"domain\": \"Knowledge\", \"year\": 2023, \"quarter\": \"q3\"\n}\n``` The metadata can be uploaded as `Text` or directly from a `File`. #### Metadata special values Check for the following metadata special values | Item | **Description** |\n|---|---|\n| url | If you want that the document point to a specific absolute URL, add a *url* metadata element pointing to the complete url resource. |\n| description | Use this property if you want to personalize how the Frontend shows the sources section; when not present it will show the *name* property. | ### Response Equivalent to [[29##GET/profile/{name}/document/{id}|Get Response]]. Note that, once the document is uploaded, the `indexStatus` will be `Unknown` as it is queued to be ingested. Use the [[29##GET/profile/{name}/document/{id}|Get Response]] API to check the document status; the expected result is `Success`. Possible return errors: - [[23##2027|2027]]\n- [[23##2028|2028]] ### cURL Sample To upload a `SampleFile.pdf` file, you can follow these steps: ```bash\n# binary\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'filename: SampleFile.pdf' \\\n--header 'Content-Type: application/pdf' \\\n--data '@/C:/temp/SampleFile.pdf' # multi-part\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"Content-Type: multipart/form-data\" \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"' # multi-part with metadata as text\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"Content-Type: multipart/form-data\" \\\n--form 'metadata=\"{\\\"type\\\":\\\"test\\\",\\\"domain\\\":\\\"Knowledge\\\",\\\"year\\\":2023,\\\"quarter\\\":\\\"q3\\\"}\"' \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"' # multi-part with metadata as a File\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"Content-Type: multipart/form-data\" \\\n--form 'metadata=@\"/C:/temp/upload_file_metadata.json\"' \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"'\n``` ## PUT/profile/{name}/document Reindex a Document (associated to an existing `documentId` GUID) to the associated `{name}` RAG Assistant. ### Request Headers | **Header** | **Description** |\n|---|---|\n| `documentId` | The GUID associated to an already existing document. | ### Request Form-data Body | **Key** | **Description** |\n|---|---|\n| `file` | The associated file to reindex (`optional`), if not present will use the previous existing file automatically. |\n| `metadata` | Use a metadata to reindex the content (`optional`); if not present will use the previous metadata; force the `{}` value if you want to clean up the associated metadata. | A sample metadata looks like this: ```json\n{ \"type\": \"test\", \"domain\": \"Knowledge\", \"year\": 2023, \"quarter\": \"q3\"\n}\n``` ### Response Equivalent to [[29##GET/profile/{name}/document/{id}|Get Response]]. Possible return errors: - [[23##2021|2021]] ### cURL Samples ```bash\n# reindex existing file\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'documentId: GUID' # Reindex existing file with new content\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'documentId: Document_GUID' \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"' # Reindex existing file with new metadata\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'documentId: Document_GUID' \\\n--form 'metadata=\"{\\\"type\\\":\\\"test\\\",\\\"domain\\\":\\\"Knowledge\\\",\\\"year\\\":2023,\\\"quarter\\\":\\\"q3\\\"}\"' # Reindex existing file with new content and metadata\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'documentId: Document_GUID' \\\n--form 'metadata=@\"/C:/temp/upload_file_metadata.json\"' \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"'\n``` ## DELETE/profile/{name}/document/{id} Deletes a Document. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) |\n| `id` | `string` | Document Id (required) | ### Response StatusCode `200` is shown when successfully deleted; otherwise, `400*` is displayed with a collection of errors. ### cURL Example ```bash\ncurl -X DELETE \"$BASE_URL/v1/search/profile/{name}/document/{id}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## POST/execute Executes a search query. For more details, read [[33]]. {{{12}}}{{{17}}}##MARKDOWN##\n\n\"description\": \"Obtain weather conditions in any country\"\n},\n{\n\"title\": \"Rain probability\",\n\"description\": \"Get the rain probability in any location\"\n}\n],\n\"examplesPrompt\": [\n{\n\"title\": \"First Prompt Example\",\n\"description\": \"First Prompt Example Description\",\n\"promptText\": \"You are an assistant specializing in...\"\n}\n]\n}\n}'\n``` ## PUT/profile/{name} Updates a RAG Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) | ### Request Body Equivalent to [[29##POST/profile|Post Request]], but the following elements are discarded: - `name` element\n- `indexOptions` section In addition, `status` element can be specified by taking values: - 1: Enabled\n- 2: Disabled If a RAG Assistants had `WelcomeData` and an update does not include this section, the existing information is deleted and becomes empty. Therefore, if you want to add new information to the `WelcomeData`, you must include both the old and the new information. If you only send the new one, it will completely replace the old one. ### Response Equivalent to Get Response. ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\" \\\n-d '{\n\"description\": \"Updated RAG assistant\",\n\"status\": 1,\n\"searchOptions\": {\n\"historyCount\": 4,\n\"llm\": {\n\"temperature\": 0.5,\n\"maxTokens\": 1000\n},\n\"search\": {\n\"k\": 2,\n\"prompt\": \"You are an Assistant, only reply using the following context:\\n{context}\\n Question is: {question}\\n\",\n\"scoreThreshold\": 0.2\n}\n}\n}'\n``` #### Sample with Welcome Data Update ```bash\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\" \\\n-d '{\n\"name\": \"Test-Profile-WelcomeData\",\n\"welcomeData\": {\n\"title\": \"Welcome to RAG Update\",\n\"description\": \"It is a RAG created with WelcomeData via API\",\n\"features\": [\n{\n\"title\": \"Updated weather conditions\",\n\"description\": \"Get the state of the weather in any country\"\n},\n{\n\"title\": \"Rain probability updated\",\n\"description\": \"Get the rain probability in any location\"\n}\n],\n\"examplesPrompt\": [\n{\n\"title\": \"First Prompt Example\",\n\"description\": \"First Prompt Example Description\",\n\"promptText\": \"You are an assistant specializing in...\"\n}\n]\n}\n}'\n``` ## DELETE/profile/{name} Deletes a RAG Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) | ### Response StatusCode `200` is shown when successfully deleted; otherwise, StatusCode `400*` is shown with a collection of errors. ### cURL Sample ```bash\ncurl -X DELETE \"$BASE_URL/v1/search/profile/{name}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## GET/profile/{name}/documents Lists the documents for a RAG Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name |\n| `skip` | `integer` | Number of documents to skip |\n| `count` | `integer` | Number of documents to return (defaults to 10) | ### Response ```json\n{ \"documents\": [ { \"extension\": \"string\", \"id\": \"string\", \"name\": \"string\", \"timestamp\": \"timestamp\", \"url\": \"string\", \"indexStatus\": \"string\", /* Unknown, Starting, Failed, Pending, Success */ \"indexDetail\": \"string\" }, ... ], \"count\": \"integer\" /* Total number of documents */\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/search/profile/{name}/documents\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n# Use the optional skip and count parameters\n$BASE_URL/v1/search/profile/{name}/documents?skip={skip}&count={count}\n``` ## PUT/profile/{name}/documents Reindex all existing documents for the RAG assistant. ### Response An empty Json with statusCode: 200. ```json\n{}\n``` ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/documents\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n``` ## DELETE/profile/{name}/documents Deletes all documents associated to the specified RAG assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) | ### Response StatusCode `200` is shown when successfully deleted; otherwise, StatusCode `400*` is shown with a collection of errors. ### cURL Sample ```bash\ncurl -X DELETE \"$BASE_URL/v1/search/profile/{name}/documents\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## GET/profile/{name}/document/{id} Using the `{name}` RAG Assistants, it gets details about the `{id}` document. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) |\n| `id` | `string` | Document Id (required) | ### Response ```json\n{ \"extension\": \"string\", \"id\": \"string\", \"indexStatus\": \"string\", /* Unknown, Starting, Failed, Pending, Success */ \"indexDetail\": \"string\", \"keyName\": \"string\", \"metadata\": [ { \"key\": \"string\", \"value\": \"string\" }, ... ], \"name\": \"string\", \"timestamp\": \"timestamp\", \"url\": \"string\"\n}\n``` For the *Failed* status, check the returned *Index Detail* field for further information. | **Index Detail** | **Description** |\n|---|---|\n| Invalid content | The associated file does not have text content; the common case is a PDF file composed of images. These files are not able to be used in a RAG Assistant. | ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/search/profile/{name}/document/{id}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## POST/profile/{name}/document Uploads a Document to the associated `{name}` RAG Assistants. Note that the file extension must be a [[137|supported one]]. ### Request Body The supported options are `binary` or `multipart/form-data` including a `File` type. #### Binary It is useful for its simplicity and encodes the binary data directly in the request body. Set the request with the associated `Content-Type` header to indicate the type of data being sent (e.g., `application/pdf`, `text/plain`). It is mandatory to set a `filename` header value with the document name and extension. For example: ```\nfilename: SampleFile.pdf\n``` Note that this option does not enable to upload [[60##Step 3: Upload documents|metadata]], for this case use the other upload option below (Form-Data). #### Form-Data This format allows you to include both binary data and other form fields in a single request. Each part of the data (binary file, text fields, etc.) is separated by a boundary and sent as separate parts. It is expected to be used for large files. You can also specify different [[581|ingestion options]] that will decide how files are processed. If you want to attach metadata to the file to be processed during ingestion, add a `metadata` form-data variable with the desired value; remember that the expected format is a `key/value` JSON list. For example, the following is a valid metadata for a Document: ```json",
          "evaluationResultRowContextDocId": "886dd74f-79ea-4603-afb3-b7ca8298555b",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "0a08ae60-385c-401f-ae88-779f084e3961",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?29",
          "evaluationResultRowContextName": "29",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# File Support in Assistants File Support in Assistants allows you to work with various file types, including documents, images, videos, and audio, to perform tasks like summarizing documents, extracting information, comparing files, and generating text based on file content. This opens up a world of possibilities for using assistants in new and innovative ways. ## Scenarios The end user should be able to interact with assistants using files in the following ways: - Summarize a document: Provide a concise summary of the document's content.\n- Extract specific information: Extract specific data points from a document, such as the amount, client name, and deadline from a contract.\n- Compare documents: Compare the information in two documents, identifying matching and differing clauses.\n- Generate text based on document content: Create new text based on the content of a document.\n- Ask questions based on uploaded documents: Similar to [RAG Assistants](44.html), but with the ability to upload new documents from the [Playground Interface](349.html) to add to the assistant's context. ## Functionality File Support in Assistants provides a robust set of features for managing and using files: - File Management: You can manage files associated with each project using CRUD operations (create, read, update, delete). A project-level API is available for file management, using the project identifier as the key. You can also configure a storage limit per project to manage storage space effectively.\n- File Referencing: You can reference files within assistant prompts using variables like `{file:filename}` or `{file:*}`. For example, you can use the prompt: \"Compare `{file:id1}` with `{file:id2}`\". The system seamlessly handles file uploads, context window size, and summarization, regardless of whether the model is multimodal or not.\n- File Types: File support extends beyond documents to include images and videos, using a similar syntax: `{file:image1}` or `{file:video1}`. For example, you can use the prompt: \"Extract the text from image `{file:id1}` and summarize its content\".\n- Frontend Upload: End users can easily select existing files or upload new ones when interacting with file-handling assistants. ## How to Use File Support in Assistants 1. Choose your assistant: Select the assistant that best suits your needs.\n2. Upload your file: Click on the \u201cUpload File\u201d button and select the file you want to use. The assistant will recognize the file type and process it accordingly.\n3. Write your prompt: Enter your instructions for the assistant in the prompt box. Be clear and specific about what you want the assistant to do.\n4. Use file references: To reference the uploaded file in your prompt, use the following syntax: - Documents: `{file:filename}` (for example, {file:contract}) - Images: `{file:image1}` - Videos: `{file:video1}` - Audio: `{file:audio1}`\n5. Submit your prompt: Click on the arrow icon (or press Enter on your keyboard) to send your prompt to the assistant.\n6. Review the results: The assistant will process your request and display the results. ## Samples ### Document: - Prompt: \u201cSummarize the key points of the document `{file:contract}`\u201d.\n- Response: \u201cThe contract outlines the terms of service for a software subscription, including pricing, payment terms, and support options\u201d. ### Image: - Prompt: \u201cDescribe the image `{file:product_image}`\u201d.\n- Response: \u201cThe image shows a red sports car with black leather interior\u201d. ### Video: - Prompt: \u201cSummarize the main points of the video `{file:tutorial}`\u201d.\n- Response: \u201cThe video demonstrates how to use the new software feature for creating custom reports\u201d. ### Audio: - Prompt: \u201cTranscribe the audio file `{file:meeting_recording}`\u201d.\n- Response: The assistant will provide a text transcript of the audio recording.",
          "evaluationResultRowContextDocId": "97680bd3-a82d-4d62-87ac-fba252965c5b",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "31ccb187-b0fb-4181-8afd-643cd5c4a848",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?346",
          "evaluationResultRowContextName": "346",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#How to upload and manage Files via API It is possible to work with previously uploaded files to perform various operations such as summaries or analysis. To perform these operations, first it is necessary to upload the file using the [[450]], confirm the success of the operation, and then reference it in the requests to the models that require the file. Here is a step-by-step guide on how to upload and manage files using the File API. ## Step 1: Uploading a File Using the File API\nThe first step is to upload the file you want to manage. To do this, use the [[450##POST /files|POST /files]] endpoint of the File API. This endpoint allows you to upload a file and receive a confirmation that the file has been successfully uploaded. The name of the file, which will be used to reference it, can be specified in the `fileName` parameter or, if not provided, it will be the name of the uploaded file. ### Getting your projectId and organizationId You can get your `$PROJECT_ID` and `$ORGANIZATION_ID` using the [[22##GET /accessControl/apitoken/validate|GET /accessControl/apitoken/validate]] endpoint of the [[22]]. Keep in mind that you can replace `$SAIA_ORGANIZATION_APITOKEN` with your `$SAIA_PROJECT_APITOKEN` if needed. ### Sample cURL Command\nTo load a file, use the following sample:\n```bash\ncurl -X POST \"$BASE_URL/v1/files\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: multipart/form-data\" \\ -H \"fileName: $FILE_NAME\" \\ -H \"folder: testFolder\" \\ -H \"organizationId: $ORGANIZATION_ID\" \\ -H \"projectId: $PROJECT_ID\" \\ -F \"file=@/path/to/your_file.mp4\" ``` If you include the `fileName` parameter, that will be the name you will use to reference the file later. If you don't include it, you will need to use the original name of the uploaded file. For example, if you upload `\"file=@/path/to/your_file.mp4\"`, the reference name will be exampleFile (without the extension). ## Step 2: Validating the File Upload\nOnce the file has been successfully uploaded, the response will include information confirming the success of the operation. You don't need to use a dataFileId, but you can check the response to ensure the file was uploaded correctly. ### Sample response\n```json\n{ \"dataFileId\": \"string\", \"dataFileUrl\": \"string\", \"success\": true\n}\n```\n## Step 3: Create a Chat Assistant (if not created already) After uploading the file, you need to create a [[708]] to process the uploaded content, if you haven't already created one. This is done through the [[35##POST/assistant|POST /assistant]] endpoint of the [[35]]. ### Sample cURL Command ```bash\ncurl -X POST \"$BASE_URL/v1/assistant\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"type\": \"chat\", \"name\": \"vertex_ai/gemini-2.0-flash-exp -Test File Upload\", \"prompt\": \"You are a helpful assistant\", \"llmSettings\": { \"providerName\": \"vertex_ai\", \"modelName\": \"gemini-2.0-flash-exp\", \"temperature\": 0.10, \"maxTokens\": 8192, \"uploadFiles\": true } }'\n``` ## Step 4: Referencing the File Once the Chat Assistant is set up, you can reference the file in a request to the Assistant for analysis or summary, using the [[34]] and specifying the file name in the request body. ### Sample cURL Command to Summarize a File: ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"saia:assistant:vertex_ai/gemini-2.0-flash-exp -Test File Upload\", /* Using the assistant created in Step 3 named 'vertex_ai/gemini-2.0-flash-exp -Test File Upload' */ \"messages\": ~~[~~ { \"role\": \"user\", \"content\": \"Provide the content {file:$FILE_NAME}\" } ~~]~~, \"stream\": false }'\n``` ## Alternative: Inquire About Files - Interact Directly with LLMs You can also interact with LLMs directly, asking questions about files like images or videos. [regions]\n[region|cURL]\nSample 1: Ask GPT-4o About an Image\n```bash\ncurl --location '&lt;BASE_URL&gt;/chat' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;SAIA_PROJECT_APITOKEN&gt;' \\\n--data '{ \"model\": \"openai/gpt-4o\", \"max_tokens\": 1024, \"temperature\": 0.1, \"messages\": ~~[~~ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": ~~[~~ { \"text\": \"Tell me what this is about?\", \"type\": \"text\" }, { \"image_url\": { \"url\": \"&lt;IMAGE_FILE_URL&gt;\" }, \"type\": \"image_url\" } ~~]~~ } ~~]~~\n}'\n```\n[/region]\n[region|Python]\nSample 2: Python Code - Sending Gemini a Video File for Analysis\n```python\nimport requests\nimport json\nimport base64 # Define the API endpoint and authentication headers\nurl = \"&lt;BASE_URL&gt;/chat/completions\"\nheaders = { 'Content-Type': 'application/json', 'Authorization': 'Bearer &lt;SAIA_PROJECT_APITOKEN&gt;'\n} # Function to encode a file in Base64 format\ndef encode_file(file_path): \"\"\"Encodes a file in Base64 format for API submission.\"\"\" with open(file_path, 'rb') as file: return base64.b64encode(file.read()).decode('utf-8') # Replace 'path_to_your_file.mp4' with the actual file path\nfile_path = \"path_to_your_file.mp4\"\nencoded_file = encode_file(file_path) # Construct the API request payload\npayload = { \"model\": \"vertex_ai/gemini-1.5-pro-002\", \"messages\": ~~[~~ { \"role\": \"user\", \"content\": ~~[~~ { \"type\": \"text\", \"text\": \"What's this file about?\" }, { \"type\": \"image_url\", \"image_url\": f\"data:video/mp4;base64,{encoded_file}\" } ~~]~~ } ~~]~~\n} # Send the request to the API\nresponse = requests.post(url, headers=headers, json=payload) # Print response details\nprint(\"Status Code:\", response.status_code)\nprint(\"\\nResponse:\")\nprint(json.dumps(response.json(), indent=2))\n```\n[/region]\n[/regions]\n{{{12}}}{{{17}}} ##MARKDOWN##",
          "evaluationResultRowContextDocId": "94fd1b65-0ca6-4b3c-8acf-eed9148e7023",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "4a7b304b-a310-45e5-8b65-e3efa10e145a",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?454",
          "evaluationResultRowContextName": "454",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Responses API This API provides access to the Responses endpoint from Globant Enterprise AI (GEAI). It includes plain text inputs, function calling, and file inputs (PDFs and images). Check the [[20]] for generic variables needed to use the API. **Note:** This endpoint currently supports only OpenAI models. ## Endpoint\n| Method | Path |\n| --- | --- |\n| **POST** | `/responses` | </br>\nThis endpoint has the same interface (Request/Response) as the [https://platform.openai.com/docs/api-reference/responses|OpenAI Responses API]. ## 1. Basic text completion\n#### Request\n```\ncurl --request POST \\ --url $BASE_URL/responses \\ -H \"Authorization: Bearer $YOUR_SAIA_API_TOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"openai/o1-pro\", \"input\": \"Tell me a three\u2011sentence bedtime story about a unicorn.\", \"stream\": false, \"max_tokens\": 2000\n}'\n```\n</br>\n#### Expected Response ```\n{ \"instructions\": null, \"metadata\": {}, \"reasoning\": { \"effort\": \"medium\" }, \"usage\": { \"completion_tokens\": 272, \"prompt_tokens\": 18, \"total_cost\": 0.1659, \"completion_tokens_details\": { \"reasoning_tokens\": 192 }, \"prompt_tokens_details\": { \"cached_tokens\": 0 }, \"total_tokens\": 290, \"currency\": \"USD\", \"completion_cost\": 0.1632, \"prompt_cost\": 0.0027 }, \"created_at\": 1746473821, \"error\": null, \"tools\": [], \"output\": [ { \"summary\": [], \"id\": \"rs_6819...\", \"type\": \"reasoning\", \"status\": null }, { \"role\": \"assistant\", \"id\": \"msg_68191...\", \"type\": \"message\", \"content\": [ { \"annotations\": [], \"text\": \"Once upon a time, in a hidden meadow where flowers swayed like colored ribbons, a unicorn named Moonlight pranced gracefully beneath the twinkling stars. By day, she roamed among the sun-kissed hills, her soft mane shimmering with every gentle breeze. At night, Moonlight\u2019s silvery horn glowed softly, granting sweet, soothing dreams to all who believed in magic.\", \"type\": \"output_text\" } ], \"status\": \"completed\" } ], \"top_p\": 1, \"previous_response_id\": null, \"parallel_tool_calls\": true, \"temperature\": 1, \"tool_choice\": \"auto\", \"model\": \"o1-pro-2025-03-19\", \"id\": \"resp_bGl...\", \"text\": { \"format\": { \"type\": \"text\" } }, \"incomplete_details\": null, \"truncation\": \"disabled\", \"user\": null, \"object\": \"response\", \"max_output_tokens\": null, \"status\": \"completed\"\n}\n``` ## 2. Function calling\n#### Request\n```\ncurl --request POST \\ --url $BASE_URL/responses \\ -H \"Authorization: Bearer $YOUR_SAIA_API_TOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"openai/o1-pro\", \"input\": \"What is the weather like in Paris today?\", \"tools\": [ { \"type\": \"function\", \"name\": \"get_weather\", \"description\": \"Get current temperature for a given location.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"City and country e.g. Bogot\u00e1, Colombia\" } }, \"required\": [ \"location\" ], \"additionalProperties\": false } } ]\n}'\n```\n</br>\n#### Expected Response\n```\n{ \"instructions\": null, \"metadata\": {}, \"reasoning\": { \"effort\": \"medium\" }, \"usage\": { \"total_tokens\": 0, \"currency\": \"USD\" }, \"created_at\": 1745243167, \"error\": null, \"tools\": [ { \"name\": \"get_weather\", \"description\": \"Get current temperature for a given location.\", \"strict\": true, \"type\": \"function\", \"parameters\": { \"additionalProperties\": false, \"type\": \"object\", \"properties\": { \"location\": { \"description\": \"City and country e.g. Bogot\u00e1, Colombia\", \"type\": \"string\" } }, \"required\": [ \"location\" ] } } ], \"output\": [ { \"summary\": [], \"id\": \"rs_6806...\", \"type\": \"reasoning\", \"status\": null }, { \"name\": \"get_weather\", \"arguments\": \"{\\\"location\\\":\\\"Paris, France\\\"}\", \"id\": \"fc_6806...\", \"type\": \"function_call\", \"call_id\": \"call_ruYPRIF9qcOxft5XrfjCBQg8\", \"status\": \"completed\" } ], \"top_p\": 1, \"previous_response_id\": null, \"parallel_tool_calls\": true, \"temperature\": 1, \"tool_choice\": \"auto\", \"model\": \"o1-pro-2025-03-19\", \"id\": \"resp_6806...\", \"text\": { \"format\": { \"type\": \"text\" } }, \"incomplete_details\": null, \"truncation\": \"disabled\", \"user\": null, \"object\": \"response\", \"max_output_tokens\": null, \"status\": \"completed\"\n}\n``` ## 3. Sending files\n### 3.1 Image input There are two ways to attach an image: * URL \u2013 pass a publicly reachable image_url. Recommended when the file is already hosted and you want a minimal payload.\n* Base\u201164 inline \u2013 embed the binary data in a data:image/...;base64, URI. Recommended for local files, private content, or CI pipelines with no public storage. </br>\n**Option A \u2014 Public image URL (cURL)**\n```\ncurl --request POST \\ --url $BASE_URL/responses \\ -H \"Authorization: Bearer $YOUR_SAIA_API_TOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"openai/gpt-4.1\", \"input\": [ { \"role\": \"user\", \"content\": [ {\"type\": \"input_text\", \"text\": \"What is in this image?\"}, { \"type\": \"input_image\", \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" } ] } ] }'\n```\n</br>\n**Option B \u2014 Inline base\u201164 (Python)**\n```\nimport base64, json, requests token = \"YOUR_SAIA_API_TOKEN\"\nurl = \"$BASE_URL/responses\"\nheaders = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"} def encode_file(file_path): with open(file_path, 'rb') as file: return base64.b64encode(file.read()).decode('utf-8') file_path = \"/path/to/your/image.png\"\nencoded_file = encode_file(file_path) payload = { \"model\": \"openai/gpt-4.1\", \"input\": [ { \"role\": \"user\", \"content\": [{ \"type\": \"input_text\", \"text\": \"What's this image about?\" }, { \"type\": \"input_image\", \"image_url\": f\"data:image/png;base64,{encoded_file}\" } ] } ]\n}\nr = requests.post(url, headers=headers, json=payload)\nprint(json.dumps(r.json(), indent=2))\n```\n</br>\n**Supported MIME types for images** | Extension(s) | MIME type | Base64 prefix |\n| --- | --- | --- |\n| .png | image/png | data:image/png;base64, |\n| .jpg, .jpeg | image/jpeg | data:image/jpeg;base64, |\n| .gif | image/gif | data:image/gif;base64, |\n| .webp | image/webp | data:image/webp;base64, ### 3.2 PDF input (Python example)\n```\nimport base64, json, requests token = \"YOUR_SAIA_API_TOKEN\"\nurl = \"$BASE_URL/responses\"\nheaders = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"} def encode_file(file_path): with open(file_path, 'rb') as file: return base64.b64encode(file.read()).decode('utf-8') file_path = \"/path/to/your/sample.pdf\"\nencoded_file = encode_file(file_path) payload = { \"model\": \"openai/o4-mini\", \"input\": [ { \"role\": \"user\", \"content\": [ { \"type\": \"input_text\", \"text\": \"What's this file about?\" }, { \"type\": \"input_file\", \"filename\": \"sample.pdf\", \"file_data\": f\"data:application/pdf;base64,{encoded_file}\" } ] } ]\n}\nr = requests.post(url, headers=headers, json=payload)\nprint(json.dumps(r.json(), indent=2))\n``` **Note:** Only models that support both text and image inputs, such as gpt-4o, gpt-4.1, or o1, can accept PDF files as input. ## Availability Since May 2025 release. {{{12}}} {{{17}}}\n##MARKDOWN##",
          "evaluationResultRowContextDocId": "60900ab1-258b-445f-9a0c-ec190a5d4cc8",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "93d584e4-ba60-4437-a5ce-5452733cedd7",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?1180",
          "evaluationResultRowContextName": "1180",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataSetRowId": "12bb908b-602f-45c6-8b6e-24ac5ff1f97b",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0051706200",
      "evaluationResultRowEndDate": "2025-05-13T11:12:18",
      "evaluationResultRowOutput": "To upload a file using the File API in Globant Enterprise AI, follow these steps:\n\n### Step 1: Upload the File\n\nUse the `POST /files` endpoint of the File API to upload your file. You can specify the name of the file in the `fileName` header or omit it to use the original file name by default.\n\n#### Required Headers:\n- `Authorization: Bearer $SAIA_PROJECT_APITOKEN`\n- `Content-Type: multipart/form-data`\n- `fileName: $FILE_NAME` (optional, name you want to assign to the uploaded file)\n- `folder: <optional folder>`\n- `organizationId: $ORGANIZATION_ID`\n- `projectId: $PROJECT_ID`\n\n#### Example cURL command:\n```bash\ncurl -X POST \"$BASE_URL/v1/files\" \\\n  -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -H \"fileName: $FILE_NAME\" \\\n  -H \"folder: testFolder\" \\\n  -H \"organizationId: $ORGANIZATION_ID\" \\\n  -H \"projectId: $PROJECT_ID\" \\\n  -F \"file=@/path/to/your_file.mp4\"\n```\n\n- Replace `$FILE_NAME` with the desired file name.\n- Replace `/path/to/your_file.mp4` with the actual path to your file.\n- Replace `$SAIA_PROJECT_APITOKEN`, `$ORGANIZATION_ID`, and `$PROJECT_ID` with your real tokens and IDs.\n\n### Step 2: Validate Upload Success\n\nThe response will contain confirmation of a successful upload and provide details like `dataFileId` and `dataFileUrl`.\n\nExample response:\n```json\n{\n  \"dataFileId\": \"string\",\n  \"dataFileUrl\": \"string\",\n  \"success\": true\n}\n```\n\n### Additional Information\n\n- You can get your `projectId` and `organizationId` via the access token validation endpoint.\n- Once uploaded, this file can be referenced in assistant prompts or for further file operations.\n\nThis procedure ensures the file is uploaded and ready for use in your assistants or other AI workflows. \n\nFor full details, see the documentation here: [How to upload and manage Files via API](https://wiki.genexus.com/enterprise-ai/wiki?454) [6](https://wiki.genexus.com/enterprise-ai/wiki?454)",
      "evaluationResultRowStartDate": "2025-05-13T11:12:06",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "11563"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# LLM Troubleshooting Below you can find known issues when working with [Supported Chat Models](200.html). ### [OpenAI's \"o\" models restrictions](#OpenAI%27s+%22o%22+models+restrictions) The parameters *temperature*, *top_p* and *n* must be set to 1, while *presence_penalty* and *frequency_penalty* are fixed at 0. Any other configuration change may generate errors such as the following: ```\nUnsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported\n``` Check your assistant configuration accordingly, related information [here](https://platform.openai.com/docs/guides/reasoning/quickstart). ### [Empty responses when using any of OpenAI\u2019s reasoning models](#Empty+responses+when+using+any+of+OpenAI%E2%80%99s+reasoning+models) When creating a [Chat Assistant](708.html) using any of OpenAI\u2019s reasoning models\u2014such as **o1**, **o1-mini**, or **o3-mini**\u2014you might encounter a scenario where the response status shows **\u201cSucceeded\u201d** but the actual response content is empty. One common cause is an insufficient **Max Output Tokens** setting. According to [OpenAI\u2019s documentation on reasoning models](https://platform.openai.com/docs/guides/reasoning/quickstart?api-mode=chat#how-reasoning-works), even though reasoning tokens are not visible to you, they still consume space in the model\u2019s context window. If **Max Output Tokens** is set too low, the model may not generate any user-visible output. To resolve this, try configuring the assistant with the maximum **Max Output Tokens** allowed by each model: - **o1** and **o3-mini**: up to 100k tokens\n- **o1-mini**: up to 65k tokens Increasing the **Max Output Tokens** to these values should prevent empty responses when the status indicates \u201cSucceeded.\u201d ### [Invalid 'max_tokens': integer below minimum value](#Invalid+%27max_tokens%27%3A+integer+below+minimum+value) The following error appears when executing an assistant where the *max_tokens* parameter is set to -1. ```\nError code: 400\nInvalid 'max_tokens': integer below minimum value. Expected a value >= 1, but got -1 instead.\ntype: invalid_request_error\nparam: max_tokens\ncode: integer_below_min_value\n``` The case was reproduced using OpenAI provider. Assign a maximum value according to the [selected model](https://platform.openai.com/docs/api-reference/chat#chat-create-max_tokens) as -1 is not detailed as supported. ### [max_tokens is too large](#max_tokens+is+too+large) The following error appears when executing an assistant ```\nError connecting to the SAIA service cause: 400\nmax_tokens is too large: 12000. This model supports at most 4096 completion tokens, whereas you provided 12000\n``` Check the *max_token* parameter supported for your assistant configured model; the selected *max_token* parameter is greater than the maximum supported. ### [The response was filtered due to the prompt triggering Azure OpenAI's content management policy](#The+response+was+filtered+due+to+the+prompt+triggering+Azure+OpenAI%27s+content+management+policy) The following error appears when executing an assistant with a complex query using Azure OpenAI endpoints ```\nThe response was filtered due to the prompt triggering Azure OpenAIs content management policy.\nPlease modify your prompt and retry.\nTo learn more about our content filtering policies please read our documentation\nhttps://go.microsoft.com/fwlink/?linkid=2198766\n``` Check the deployment made for the associated endpoint, make sure to set the *content filter* to the empty value (default); do not use the [Microsoft.Default.v2](https://learn.microsoft.com/en-us/azure/ai-services/openai/whats-new#new-responsible-ai-default-content-filtering-policy) configuration. Go to the [Azure AI Foundry portal](https://ai.azure.com/), locate the *deployments* section; and for each completion model (such as gpt-4o, gpt-4o-mini), use the *Update Deployment* option to set the Content Filter as \"Default\". ![Image](./images/883.png) ### [Service: BedrockRuntime, Status Code: 403, Request ID: GUID](#Service%3A+BedrockRuntime%2C+Status+Code%3A+403%2C+Request+ID%3A+GUID) The following error appears when using a model in AWS Bedrock ```\nYou don't have access to the model with the specified model ID\nReceived Model Group=awsbedrock/modelname\n``` Make sure you have access to the model *modelname*, follow these steps to enable it: [Add or remove access to Amazon Bedrock foundation models](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html) ### [Empty Prompt for Anthropic Models](#Empty+Prompt+for+Anthropic+Models) When creating a [Chat Assistant](708.html) and selecting an Anthropic model (for example, *anthropic/claude-3-7-sonnet-latest*), the **Prompt** field is mandatory and cannot be left empty. If you try to configure the assistant without any content in this field, you will encounter an error similar to the following: ```\n{\"error\":{\"message\":\"litellm.BadRequestError: AnthropicException - {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"invalid_request_error\\\",\\\"message\\\":\\\"system: text content blocks must be non-empty\\\"}}. Received Model Group=anthropic/claude-3-7-sonnet-latest\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n``` Make sure that the **Prompt** field contains at least one valid text content block. Review the assistant's configuration and fill in the prompt with appropriate information to avoid this error.",
          "evaluationResultRowContextDocId": "02b5107e-351f-42fd-91b5-715e37300386",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "24a68aac-5560-4c60-841c-195655a8d425",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?290",
          "evaluationResultRowContextName": "290",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Supported Embeddings Models **List of supported embeddings models** | **Provider** | **Model** | **Multimodal** | **Description** | **Environment Support** |\n|---|---|---|---|---|\n| **openai** | text-embedding-ada-002<br>text-embedding-3-small<br>text-embedding-3-large|No<br>No<br>No<br>No|Must set dimensions to 1536, other values are not enabled by default<sup>(1)</sup> | Beta, QA, Production|\n| **azureopenai** | * | | Same models as openai, configure the associated URL endpoints | Beta, QA, Production |\n| **google/googlevertexai/vertex_ai**<sup>(3)</sup>|multimodalembedding@001<sup>(2)</sup><br>text-embedding-004<br>text-embedding-005<br>text-multilingual-embedding-002<br>|Yes<br>No<br>No<br>No|The dimension parameter must be configured to 768<sup>(1)</sup> or 1408<sup>(2)</sup> | Beta, QA, Production |\n| **awsbedrock** | amazon.titan-embed-image-v1<br>cohere.embed-english-v3<br>amazon.titan-embed-v2<br>cohere.embed-english<br>cohere.embed-multilingual|Yes<br>Yes<br>No<br>Yes<br>No|Check the model dimensions configuration<sup>(1)</sup>| Beta, QA, Production |\n| **nvidia** | baai.bge-m3<br>nv-embed-v1<br>nvclip<br>embed-qa-4<br>nv-embedqa-e5-v5<br>snowflake.arctic-embed-l<br>nv-embedqa-mistral-7b-v2|No<br>No<br>Yes<br>No<br>No<br>No<br>No|Check the model dimensions configuration<sup>(1)</sup> | Beta, QA, Production |\n| **cohere** | embed-english-v3.0<br>embed-multilingual-v3.0<br>embed-english-light-v3.0<br>embed-multilingual-light-v3.0|Yes<br>No<br>No<br>No| 1024 dimensions except for the \"*light*\" models using 384. | Beta, QA, Production |\n| **pinecone**<sup>(2)</sup> | multilingual-e5-large<br>llama-text-embed-v2|No<br>No| 1024 dimensions. | Beta, QA | <sup>(1)</sup> - For [[441]], the associated module must be installed and configured. <sup>(2)</sup> - The configuration must be done in the [[70|Retrieval tab]] by configuring the [[298|Profile Metadata]] parameter. For more information, refer to [[298##Not registered Providers and Models|Not registered Providers and Models]]. Please note that the associated `Embeddings costs` are not recorded, and no related `Requests` are logged as they are processed by the provider. <sup>(3)</sup> - Check the [https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#supported-models|Provider recommendations]. {{{12}}}{{{17}}}\n##MARKDOWN##",
          "evaluationResultRowContextDocId": "9d067949-05dd-404f-8c5a-55e97750d270",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "52232fee-7897-42ac-9f6c-c53d6d9b533e",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?447",
          "evaluationResultRowContextName": "447",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Embeddings API Gets a vector representation of a given input that can be easily consumed by machine learning models and algorithms. Check the [[20##Generic Variables|generic variables]] needed to use the API and the [[447|supported models]]. ## Endpoints |**Method**|**Path**|**Description**|\n|---|---|---|\n|`POST`|`/embeddings`|Creates an embedding vector representing the input| ## POST/embeddings Creates an embedding vector representing the input provided. |**Parameter**|**Type**|**Description**|\n|---|---|---|\n|`input`|`string` or `array`|Input to embed, encoded as a string<sup>(4)</sup>. To embed multiple inputs in a single request, pass an array of strings. The input must not exceed the max input tokens for the model and cannot be an empty string|\n|`model`|`string`|`provider`/`modelId` to use|\n|`encoding_format`<sup>(1)</sup>|`string`|The format to return the embeddings. It can be either `float` (default) or `base64` (optional)|\n|`dimensions`<sup>(1)</sup>|`integer`|The number of dimensions the resulting output embeddings should have. Only supported in `text-embedding-3*` and later models (optional)|\n|`user`<sup>(3)</sup>|`string`|A unique identifier representing your `end-user`|\n|`input_type`<sup>(2)</sup>|`string`|Defines how the input data will be used when generating embeddings (optional)|\n|`timeout`|`integer`|The maximum time, in seconds, to wait for the API to respond. Defaults to 600 seconds| Check the valid providers in the Providers column of the [[200]]. <sup>(1)</sup>: Only supported by `openAI` provider. The maximum input token is 8191 (roughly ten standard pages of text). <sup>(2)</sup>: Check if the selected embeddings model supports this option. <sup>(3)</sup>: `openAI` specific. <sup>(4)</sup>: If the embedding model support image embeddings; pass in a base64 encoded image string to the input parameter with the associated mime type (check the samples section). You can use the `X-Saia-Cache-Enabled` (`true` or `false` default) to cache the embeddings for the model; it applies by Organization/Project. ### Request Body [regions]\n[region|Using a string]\n<pre class=\"Code\">\n{ \"model\": \"openai/text-embedding-3-small\", \"input\" : \"Help me with Globant Enterprise AI!\"\n}\n</pre> [/region]\n[region|Using an array]\n<pre class=\"Code\">\n{ \"model\": \"openai/text-embedding-3-small\", \"input\" : [\"Help me with\", \"Globant Enterprise AI!\"]\n}\n</pre>\n[/region]\n[/regions] ### Response The expected response is similar to the following: <pre class=\"Code\">\n{ \"model\": \"text-embedding-3-small\", \"data\": [ { \"embedding\": [...], \"index\": 0, \"object\": \"embedding\" },... ], \"object\": \"list\", \"usage\": { \"completion_tokens\": number, \"prompt_tokens\": number, \"total_tokens\": number, \"completion_tokens_details\": null }\n}\n</pre> ### Samples [regions]\n[region|OpenAI using one string]\n```bash\ncurl --location '$BASE_URL/embeddings' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n--data '{ \"model\": \"openai/text-embedding-3-small\", \"input\" : \"Help me with Globant Enterprise AI!\"\n}'\n```\n[/region]\n[region|OpenAI and cache]\n```bash\ncurl --location '$BASE_URL/embeddings' \\\n--header 'X-Saia-Cache-Enabled: true' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n--data '{ \"model\": \"openai/text-embedding-3-small\", \"input\" : [\"Help me with\", \"Globant Enterprise AI!\"] }'\n```\n[/region]\n[region|VertexAI and optional parameters]\n```bash\ncurl --location '$BASE_URL/embeddings' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n--data '{ \"model\": \"vertex_ai/text-embedding-004\", \"input_type\": \"SEMANTIC_SIMILARITY\", // RETRIEVAL_QUERY, RETRIEVAL_DOCUMENT and other options \"input\" : [\"Help me with\", \"Globant Enterprise AI!\"]\n}'\n```\n[/region]\n[region|Nvidia and optional parameters]\n```bash\ncurl --location '$BASE_URL/embeddings' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n--data '{ \"model\": \"nvidia/nv-embedqa-e5-v5\", \"input\" : [\"identify\", \"yourself!\"], \"input_type\": \"passage\" // passage or query\n}'\n```\n[/region]\n[region|Bedrock text and Image content]\n```bash\ncurl --location '$BASE_URL/embeddings' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n--data '{ \"model\": \"awsbedrock/amazon.titan-embed-image-v1\", \"input\" : [ \"Help me with Globant Enterprise AI!\", \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAIAAACQd1PeAAAAEElEQVR4nGK6HcwNCAAA//8DTgE8HuxwEQAAAABJRU5ErkJggg==\" ]\n}'\n```\n[/region]\n[/regions] ## See Also\n[[20|API Reference]] [https://platform.openai.com/docs/api-reference/embeddings|openAI Embeddings] [https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api|VertexAI Text Embeddings] {{{12}}} {{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "1dc9a910-de7a-4e43-bc70-f0bfd94d38f7",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "8d087ff0-759e-4a5b-b1b6-9c640fdd2156",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?422",
          "evaluationResultRowContextName": "422",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "- New Average Request Time metric added in the Project Dashboard. ### January 8th - The option formerly known as 'Search Documents' has been improved and renamed to RAG Assistant (Retrieval Augmented Generation) to provide an optimized experience when searching and generating information. #### Frontend - Feedback is provided during conversations with RAG Assistants, indicating where you are in the process.\n- 'Response streaming' support for RAG Assistants.\n- Settings are hidden when selecting an assistant, except when 'Chat with LLMs' is selected. ## 2023 ### December 19th - Fixed: Too Many Redirects when accessing Playground using a browser in Spanish language. ### December 6th - New backoffice design.\n- Access to the Playground from the backoffice to chat with the assistants defined in the project.\n- Upload images for analysis with GPT-4 Vision.\n- Google Analytics support at the frontend.\n- Keep a conversation thread when chatting with documents.\n- An email notification is sent automatically when a new member is invited to join the organization or project. ### November 28th - First version officially released!! ### November 6th - The following OpenAI models are supported: GTP-4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), and GPT-4 Vision (gpt-4-vision-preview). ### October 18th - [Manage quotas per project](71.html): It is possible to set quota limits to control project expenses or usage. ### October 11th - [AI-Driven Load Balancing](32.html): The platform automatically manages the Load Balancing process when you work with generative AI providers, efficiently addressing the limits imposed by LLM platforms.\n\n- In the [Organization API](22.html), the ability to set and manage usage limits on projects through the [POST /project](22.html) and [GET /project/{id}](22.html) endpoints has been added.\n- [Quota Limit](71.html) now includes improvements such as highlighting the active quota in green, offering options to cancel active quotas, among others. ### October 17th - Rebranding to Globant Enteprise AI\n- Improvements in RAG - New option not to [compact the conversation history](297.html) - Support for Redis, MongoDB Atlas and PG Vector [vectorstores](267.html) - New endpoint to get an embedding. Check details here: [Embeddings API](422.html)\n- Playground improvements\n- File management improvements\n- New [LLMs supported](200.html) - NVIDIA provider with new models supported - nvidia.nemotron-mini-4b-instruct - meta.llama-3.1-8b-instruct - meta.llama-3.1-70b-instruct - meta.llama-3.1-405b-instruct - meta.llama-3.2-3b-instruct - Groq provider supported - groq/llama-3.1-70b-versatile - groq/llama-3.2-11b-vision-preview - groq/llama-3.2-3b-previewgroq/llama-3.2-1b-preview\n- New [embeddings models](447.html) added - Vertex AI: - vertex_ai/textembedding-gecko - vertex_ai/text-embedding-004 - vertex_ai/textembedding-gecko-multilingual - Nvidia: - nvidia/nvclip - nvidia/nv-embed-v1 - nvidia/baai.bge-m3 - nvidia/snowflake.arctic-embed-l - nvidia/nv-embedqa-mistral-7b-v2 - nvidia/embed-qa-4 - nvidia/nv-embedqa-e5-v5 ### September 25th - [Support for file](346.html) processing with prompt-based assistants. This will enable many scenarios, such as uploading documents and summarizing, extracting, and checking information, etc. Also, depending on the model used by the assistant, it will be able to process audio, video, or images.\n- [Support for multi-modal](345.html) LLMs allow processing docs, audio, video, and images in models like GPT-4o or Gemini Pro.\n- Chat with data assistants - The model used to build the queries was updated with GPT-4o, which improves the quality of the generated query. - Configure the query builder server by organization and project. This means you can connect with different DBMS from each project when building Chat with data assistants. - Show an explanation of how the query was built.\n- New [Playground Interface](349.html) design - New design - Upload documents from the front end to chat with them.\n- [Flows builder](321.html) - There will be two types of Flows, one more oriented to build a conversational UI and the other to build assistant flows. Access to these flows will only be available through [Chat API](34.html) or through the [channels](384.html) offered by Flows.\n- New models hosted in AWS Bedrock added: - Amazon Titan Express v1 - Amazon Titan Lite v1 - Anthropic Claude 3 Haiku - Anthropic Claude 3 Sonnet - Anthropic Claude 3.5 Sonnet - Cohere Command - Meta Llama 3 8B - Meta LLama 3 70B\n- It is now possible to provide clear guidance on the assistant's capabilities, allowing you to add information such as descriptions, features, and example prompts. This configuration can be done from the Backoffice, [Start Page](408.html), or WelcomeData section of the [Assistant API](35.html) and [RAG Assistants API](29.html) endpoints.\n- RAG Assistants - New endpoint in [RAG Assistants API](29.html): using [DELETE/profile/{name}/documents](29.html), all documents associated with the specified RAG Assistant are deleted. ### August 9th - Support of new models - GPT-4o mini\n- RAG Assistants - New option called [CLEANUP](60.html) allows to delete the documents associated to a RAG Assistant. - When creating a new assistant, the following defaults are updated: - [Chunk Count](69.html) updated from 2 to 5. - [Model Name](68.html) from gpt-3.5-turbo to gpt-4o-mini.\n- Data Analyst Assistant - New [Conversational Module](291.html) for Data Analyst Assistant.\n- Considerations - **Important**: [max_tokens integer below minimum value](290.html) error may occur if you are using the _max_tokens_ value as -1; those Assistants must be updated to use a correct positive value. ### July 4th - Permissions by assistants in role-based access control. A new [role and permissions management system](242.html) has been implemented to provide greater flexibility and control. Please read [Migration to the new Roles and Permissions Management System](264.html).\n- Dashboard performance optimizations. Please read [Using the Request Summary Table for Improved Dashboard Performance](273.html).\n- RAG Improvements - New option to customize the [context](205.html). - Support for [Max Marginal Relevance](206.html) - Support for _similarity_hybrid_ and _semantic_hybrid_ for [Azure AISearch](207.html) - Advanced options for SelfQuery retriever prompt case. See [Self Query Use Case](150.html) - Support for Parent Document retrieval option. See [Parent Document Retrieval Strategy](204.html) ### June 10th - [Enterprise AI Proxy](19.html) is deprecated. Use [Chat API](34.html) instead.\n- Support for new LLMs - OpenAI new model GPT-4o - Models in Google Vertex - Gemini 1.0 Pro - Gemini 1.5 Flash preview-0514 - Gemini 1.5 Pro preview-0514 - Claude 3 Haiku - Claude 3 Opus - Claude 3 Sonnet\n- RAG Improvements - New option to initialize RAG Assistant based on another when creating a new RAG Assistant. - New option to export document list in View Documents over a RAG Assistant. - Added filter options when browsing Documents. - SelfQuery RAG retriever partial support for a [customized Prompt](150.html). - Support for text-embedding-004 in Google models to generate the embeddings.\n- Deprecated [Assistant API](35.html) endpoints. - /assistant/text/begin - /assistant/text\n- Support to deploy in Google Cloud Platform. ### May 8th - New [Chat with Data Assistant](159.html).\n- New [Ingestion SDK](172.html) to automate document ingestion in RAG assistants.\n- New models hosted in NVIDIA platform supported. See [Supported Chat Models](200.html) for more details.\n- New option to export information about projects and members available for the organization administrator.\n- New API to extend dataset for [Data Analyst Assistant 1.0](89.html).\n- New filter by user email in Requests.\n- Update default to use _text-embedding-3-small_ OpenAI Embeddings for new RAG assistants.\n- Support for gemini-1.5-pro-preview-0409 model added. ### April 3rd - New [API Assistant](110.html).\n- New [Data Analyst Assistant 1.0](89.html).\n- The user Dashboard displays the email to identify the most active user or the one who generates the most costs. ### March 11th - GeneXus Identity Provider is implemented, expanding the login options in the [Backoffice](42.html) of the production environment. This allows for login not only with Google but also with Apple or GeneXus Account.\n- It is possible to customize the icon for each assistant. ### February 29th - [Frontend](82.html) improvements in UI/UX.\n- Option to get feedback from end users when interacting with [RAG Assistant](44.html).\n- Gemini Pro LLM support.\n- New [Dashboard](562.html) with user metrics.\n\n# Updates Log This log shows the most important fixes or features added to the platform. ## Log ## 2025 ### April 10th - New [Globant Enterprise AI Lab](972.html) - The Lab is designed for defining, managing, and orchestrating autonomous AI agents. It provides a standardized model for representing agents, their capabilities, and their interactions within complex workflows. The core components of the Lab include: - Agents & Tools: This module allows for the definition and management of individual agents and their resources, such as skills and tools. It serves as a central hub for cataloging and managing the agent workforce. - Agentic Processes: This component enables the definition of processes based on tasks executed by Agents. These workflows facilitate collaboration among agents to achieve larger objectives. More details at [How to create an Agentic Process](985.html). - Agent RunTime: This module provides the execution environment for agentic workflows, where agents perform tasks based on their skills and interact with artifacts, driven by events and the flow of knowledge. - The Lab aims to meet the growing demand for intelligent, self-sufficient AI agents capable of collaborating and solving complex problems with minimal human intervention. It offers a flexible and adaptable model, allowing for the creation and management of a diverse range of agents, from co-pilots working alongside humans to fully automated agents executing complex tasks. Implemented as a module of Globant Enterprise AI, the Lab supports the development of intelligent agents that can work autonomously or in collaboration with humans and other agents.\n- New features in Flows - Agent Integration Component: You can now directly integrate agents created with the AI Lab into a Flow. These agents can be exposed through platforms like WhatsApp, Teams, or Slack. - [File Upload Support](1053.html) from WhatsApp: Flows now support receiving file attachments such as documents, images, audio, and video directly from WhatsApp interactions. - Audio and Video Attachment in Web Chat: The Web Chat component now allows users to attach audio and video files, enhancing the interaction experience.\n- New Features for the Data Analyst Agent - Reduced Configuration Requirements: The setup needed to enable the assistant to respond to a wide range of questions has been minimized. - Enhanced Analysis Module: An additional analysis module has been incorporated to complement the responses with relevant business conclusions and interpretations of the obtained data.\n- New metrics to track processed tokens. - A new view has been added to each project dashboard to monitor token-based consumption. More details at [Tokens tab in Dashboard](965.html). - To enable this feature, you must run the initialization process described in [Reprocessing Requests and Loading the Summary Table](1015.html).\n- LLMs: - New Gemini 2.5 Pro (via providers Vertex AI and Gemini): Gemini 2.5 is Google\u2019s latest reasoning model, engineered to tackle increasingly complex challenges. This model is designed for tasks that demand advanced analytical thinking and robust problem-solving capabilities. More details at [LLM API](663.html). - Migration to Gemini 2.0 series (Vertex AI): Based on recommendations from Vertex AI, we have migrated from the legacy Gemini 1.0 and 1.5 models to the more advanced Gemini 2.0 series, offering improved performance, scalability, and integration capabilities. For comprehensive information, please refer to the [Deprecated Models](Deprecated Models). - New Azure OpenAI models (o1, o1-mini and o3-mini): We have expanded our model availability by introducing these models via Azure, providing the same high-quality capabilities as those offered through the OpenAI provider. - DeepSeek-R1 via AWS Bedrock: Recently added through a Serverless API, DeepSeek-R1 offers reliable inference with a substantial 128K token context window and up to 32K maximum output tokens. - OpenAI's new models with built-in web search tool: These specialized models integrate web search capabilities directly into the Chat Completions API, enabling them to both interpret and execute search queries in real time. - openai/gpt-4o-search-preview - openai/gpt-4o-mini-search-preview - New models - Beta only: - gemini/gemma-3-27b-it: Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 has a large, 128K context window, and multilingual support in over 140 languages. - SambaNova: - DeepSeek-R1: This Provider offers the fastest performance for running DeepSeek, processing up to 198 tokens per second per user, with a 16K token context window. The model is hosted in US data centers; privately and securely. - DeepSeek-V3-0324: This model significantly outperforms its predecessor with enhanced reasoning benchmarks, improved code executability and refined web aesthetics, and superior Chinese writing aligned with the R1 style. It also offers better multi-turn interactive rewriting, translation quality, detailed report analysis, and more accurate function calling. - QwQ-32B: SambaNova provides access to QwQ-32B-Preview, the best open source test-time compute model released by Alibaba. - Llama 4 collection by Meta: - Llama 4 Scout: A 17B-parameter multimodal MoE model with 16 experts that excels in text and image understanding. The model is currently in beta and is available via providers Nvidia, Groq and SambaNova. Via Groq, it supports a 128k tokens context window with fast inference at 460 tokens/sec, while SambaNova Cloud runs at 697 tokens/second/user. - Llama 4 Maverick: Available via Nvidia, this 17-billion-parameter model featuring 128 experts supports a 32k-token context window. ### March 10th - New LLMs: - GPT-4.5 - Claude 3.7 Sonnet (Providers Anthropic, Vertex AI and AWS Bedrock) - Updates in Gemini 2.0 series: - vertex_ai/gemini-2.0-flash-lite-preview-02-05 - vertex_ai/gemini-2.0-flash-thinking-exp-01-21\n- RAG Revision #6 - Support for o3-mini, gpt-4.5-preview, claude-3-7-sonnet-20250219, new DeepSeek, Gemini2* and sambanova LLM providers. - New pinecone provider available for [embeddings](447.html) and [rerankers](638.html). - The [CleanUp](60.html) action message has been corrected to clearly specify that it will permanently delete the RAG Assistant files and update the information in the RDS. - Added [usage](712.html) element on every response. - Improvements when changing the [LLM](69.html)/[Embeddings](68.html) settings; all models and providers are normalized to be selected from standard combo-box items; use the [override mechanism](209.html) if you need other options. - Support for [guardrails](840.html). - New [documentAggregation](298.html) property to decide how sources are grouped and returned.\n- It is possible to [provide feedback](170.html) on the response of the Chat with Data Assistant in the Frontend.\n\n- The new [Evaluation APIs](893.html) introduce key functionalities through three interconnected APIs: [DataSet API](839.html), [Evaluation Plan API](849.html), and [Evaluation Result API](856.html). This version is primarily designed for users with a data science profile and is mainly accessed via APIs, complemented by a series of Jupyter notebooks that demonstrate their use. For a comprehensive guide on how to use these APIs, you can refer to [How to evaluate an AI Assistant](881.html) and the [EvaluationAPITutorial.ipynb](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?887,,) notebook, which provide practical examples and code for working through the evaluation process.\n- File attachment support in Flows (version 0.9).\n- Support for Full Story integration in the Workspace/Playground to generate user access statistics in Full Story.\n- In the [LLM API](663.html), for models that have descriptions in the specified languages, the `descriptions` property is included in the Response, which contains the descriptions in the available languages, such as `Spanish`, `English` and `Japanese`.\n- [Data Analyst Assistant 2.0](886.html) version presents important improvements, simplifying the interaction with the data by reducing the main components to just two: **Dispatcher** and **Thinker**. In addition, the metadata structure is automatically generated when loading the datasets, streamlining the setup process. For more information, see [How to create a Data Analyst Assistant](105.html).\n- The option to consult version-specific documentation is now available. Articles with versions show the option \u201cOther document versions\u201d in the header. Clicking on \u201cOther document versions\u201d brings up a menu that allows you to choose between the most recent version (\u201cLatest\u201d) or earlier versions (e.g. \u201c2025-02 or prior\u201d). If you select a version other than \u201cLatest\u201d, a message appears: \u201cThis is not the latest version of this document; to access the latest version, click here\u201d. This message provides a direct link to the most up-to-date documentation. [Components Version Update](822.html) ### February 10th - New documentation with details about [Supported Chart Types](731.html).\n- New [Usage Limits API](802.html).\n- Flows - Logs associated with conversations are accessible from the [conversation history](378.html).\n- RAG Revision #5 - Ingestion supports new [file formats](137.html). For on-premises installations, update the _FILE_TYPES_ parameter. - New optional _cleanProcessing_ parameter when using [Parent Document Retrieval Strategy](204.html). - [Embeddings API](422.html) cache support; disabled by default from a [RAG assistant](68.html). - New [mode](68.html) property for using multimodal embeddings and creating a [Multimodal RAG](783.html).\n- New endpoint [GET /accessControl/apitoken/validate](22.html) returns information about the organization and project associated with the provided apitoken.\n- New LLMs: - Already in production - **OpenAI**: o3-mini - Already in Beta - **DeepSeek**: - deepseek/deepseek-reasoner - deepseek/deepseek-chat - azure/deepseek-r1 - nvidia/deepseek-ai-deepseek-r1 - groq/deepseek-r1-distill-llama-70b - sambanova/DeepSeek-R1-Distill-Llama-70B - Updates in Gemini 2.0 series: - gemini-2.0-flash-thinking-exp-01-21 (Via Providers Gemini and Vertex AI) - gemini/gemini-2.0-flash-lite-preview - gemini/gemini-2.0-pro-exp - vertex_ai/gemini-2.0-flash-001 - sambanova/Llama-3.1-Tulu-3-405B [Components Version Update](822.html) ### January 13th - Internationalization, [Backoffice](42.html), and frontend support for Japanese.\n- Invitations now include information about the organization and project in the subject.\n- New LLMs - Already in Production - **OpenAI**: o1 (2024-12-17 version) - Already in Beta - New Providers: **Cohere** - **Cohere**: Cohere-r\n- [Guardrails](682.html) configured by assistant.\n- [Rerank API](625.html) to semantically order a list of document chunks given a query.\n- New optional RAG [Retrieve and Rerank](620.html) adds an extra layer of precision to ensure that only the most relevant information reaches the model used in the generation step. ## 2024 ### December 12th - [Automatic Creation of Default Assistant](640.html)\n- [Organization Usage Limits](666.html): It is possible to set quota limits to control organization expenses or usage.\n- Chat with Data Assistant - [Show details](170.html) about the generated query in the Playground. - Support in [Chat API](34.html) to interact with Chat with Data Assistant.\n- Flows - Support for markdown when showing the response on the different channels supported by Flows (web, Slack, WhatsApp, and Teams). - New component for connecting flows to the agent overflow console (Human-in-the-loop) via B2Chat. Please read [How to connect a Flow to B2Chat](674.html).\n- RAG - [Chroma](537.html), [Elasticsearch](579.html), [OpenSearch](580.html) vectorstore support. - [Improvements](581.html) when processing PDF documents (update to RAG module revision #4 or higher). - Integration with [AWS Knowledge Bases](577.html) at query time.\n- Data Analyst Assistant - Option to update metadata options. - New version by default in new Data Analyst assistants.\n- New LLMs - OpenAI: gpt-4o-2024-11-20 - AWS Bedrock: Anthropic Claude 3.5 Haiku - Amazon Nova models (Micro, Lite, and Pro) - Llama 3.1 405B on Vertex AI - Beta: - Support for providers Cerebras, SambaNova and xAI (Grok models). - All new Gemini Experimental models.\n- Security - It is possible to define a user 'all-members@domain.com' to give access to the Frontend to all users of the domain '@domain.com'. Permissions are enforced based on the roles assigned to this user. Please read [Allowing Frontend Access for Users from Specific Domains](657.html) ### November 12th - Flows execution integrated into the Playground\n- New [LLMs support](200.html) - **OpenAI**: o1-preview and o1-mini - **Claude Sonnet 3.5 v2 - Providers**: Anthropic, Vertex AI, and AWS Bedrock - **Llama 3.2 models - Providers**: Vertex AI and AWS Bedrock\n- Chat with data assistants - Possibility to edit metadata, [entities](170.html), and attribute descriptions. - The Properties tab has been renamed to [Settings](170.html) along with the options that can be configured in it.\n- RAG - New [returnSourceDocuments](298.html) option to disable returning the documents section used to answer the question. - New [step](298.html) option to use the assistant as a retrieval tool. - Support for [custom history](85.html) in conversations using the _chat_history_ variable.\n- Stand-alone Frontend based on the new Playground UI - Options to customize the Frontend to use the client logo, color palette, welcome message, and descriptions. - Feature to collect feedback (thumbs up/down) in each response. - Google Analytics support.\n- Data Analyst Assistant - Support to upload large CSV files.",
          "evaluationResultRowContextDocId": "68ace6a6-8b1b-4736-a466-9ed0a67b97a9",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "ed1a8137-be1b-4f2d-813d-1af5ac6f0af1",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?31",
          "evaluationResultRowContextName": "31",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.7,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        }
      ],
      "dataSetRowId": "13d613d9-3925-4ae8-a7e8-be635d4af41c",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0038406800",
      "evaluationResultRowEndDate": "2025-05-13T11:12:22",
      "evaluationResultRowOutput": "The embedding models supported by AWS Bedrock are:\n\n- amazon.titan-embed-image-v1  \n- cohere.embed-english-v3  \n- amazon.titan-embed-v2  \n- cohere.embed-english  \n- cohere.embed-multilingual  \n\nSome of these models support multimodal embeddings. Make sure to check the model dimensions configuration for these models when using them.\n\nThis information is detailed in the **Supported Embeddings Models </B>section. You can find it at [Supported Embeddings Models Documentation](https://wiki.genexus.com/enterprise-ai/wiki?447) [6].",
      "evaluationResultRowStartDate": "2025-05-13T11:12:18",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "9369"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#LLMs with Reasoning Capabilities This article explains how to enable and control the reasoning features available in three leading Large Language Model (LLM) providers\u2014OpenAI, Anthropic, and Google Vertex AI\u2014when calling the `chat/completions` endpoint. For each provider, you will find an overview of the relevant parameters and a cURL example that follows the standard request format used in GEAI projects. ## Providers\n### 1. OpenAI\nOpenAI\u2019s o-series reasoning models expose an optional [https://platform.openai.com/docs/api-reference/chat/create#chat-create-reasoning_effort|`reasoning_effort`] parameter that lets you choose how many tokens the model may devote to internal reasoning. This gives you explicit control over latency and cost. | Allowed values | Effect |\n| --- | --- |\n| `low` | Minimal extra reasoning (fastest, cheapest) |\n| `medium` | Balanced reasoning vs. speed/cost |\n| `high` | Maximum reasoning effort (slowest, highest cost) | #### **Sample cURL** ```\ncurl -X POST \"$BASE_URL/chat/completions\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"openai/o4-mini\", \"messages\": [ { \"role\": \"user\", \"content\": \"Could you explain and compare in detail the relationships between category theory and homotopy theory in modern algebraic topology, describe concrete applications to the classification of fibrations, then outline how these ideas extend to theoretical physics\u2014especially quantum field theory and quantum gravity\u2014considering AdS/CFT correspondence and renormalisation methods in non\u2011commutative geometry, and finally discuss the philosophical consequences of these advances on the unity of physical laws?\" } ], \"stream\": false, \"temperature\": 1, \"max_completion_tokens\": 100000, \"reasoning_effort\": \"high\"\n}' ``` ### 2. Anthropic Claude 3.7 Sonnet offers an [https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking|Extended Thinking] mode. Enable it by adding a top\u2011level `thinking` object with a `budget_tokens` field that sets the token budget for reasoning. * Range: 0 to \u2248\u202f63000 (Claude\u2019s max output tokens = 64\u202f000).\n* Note: Above 32000 tokens, the model may not consume the entire budget. #### **Sample cURL**\n```\ncurl -X POST \"$BASE_URL/chat/completions\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"anthropic/claude-3-7-sonnet-latest\", \"max_tokens\": 20000, \"thinking\": { \"type\": \"enabled\", \"budget_tokens\": 16000 }, \"messages\": [ { \"role\": \"user\", \"content\": \"Are there infinitely many prime numbers that leave a remainder of 2 when divided by 3?\" } ]\n}' ``` ### 3. Google Vertex AI Google\u2019s latest model, [https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash|Gemini 2.5 Flash Preview], also supports a reasoning budget via a `thinking` object. * Range: 1 to 24\u202f000 tokens. #### **Sample cURL** ```\ncurl -X POST \"$BASE_URL/chat/completions\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"vertex_ai/gemini-2.5-flash-preview-04-17\", \"messages\": [ { \"role\": \"user\", \"content\": \"A farmer buys 30 animals consisting only of chickens and cows, and when he counts their legs he gets 74\u2014how many of the animals are cows?\" } ], \"thinking\": { \"type\": \"enabled\", \"budget_tokens\": 10000 }, \"stream\": false, \"temperature\": 0.1\n}'\n``` {{{216| Although [https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro|Gemini 2.5 Pro Preview] is also a reasoning\u2011capable model, its reasoning feature is currently fixed and cannot be enabled, disabled, or tuned|||}}} ##Availability Since April 2025 release. {{{12}}}{{{17}}} ##MARKDOWN##",
          "evaluationResultRowContextDocId": "2d0746ec-0c30-4a2f-882c-733bfb8b8b46",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "0329093f-97c2-4bfb-a240-30cebf5d1879",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?1168",
          "evaluationResultRowContextName": "1168",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "- New Average Request Time metric added in the Project Dashboard. ### January 8th - The option formerly known as 'Search Documents' has been improved and renamed to RAG Assistant (Retrieval Augmented Generation) to provide an optimized experience when searching and generating information. #### Frontend - Feedback is provided during conversations with RAG Assistants, indicating where you are in the process.\n- 'Response streaming' support for RAG Assistants.\n- Settings are hidden when selecting an assistant, except when 'Chat with LLMs' is selected. ## 2023 ### December 19th - Fixed: Too Many Redirects when accessing Playground using a browser in Spanish language. ### December 6th - New backoffice design.\n- Access to the Playground from the backoffice to chat with the assistants defined in the project.\n- Upload images for analysis with GPT-4 Vision.\n- Google Analytics support at the frontend.\n- Keep a conversation thread when chatting with documents.\n- An email notification is sent automatically when a new member is invited to join the organization or project. ### November 28th - First version officially released!! ### November 6th - The following OpenAI models are supported: GTP-4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), and GPT-4 Vision (gpt-4-vision-preview). ### October 18th - [Manage quotas per project](71.html): It is possible to set quota limits to control project expenses or usage. ### October 11th - [AI-Driven Load Balancing](32.html): The platform automatically manages the Load Balancing process when you work with generative AI providers, efficiently addressing the limits imposed by LLM platforms.\n\n- In the [Organization API](22.html), the ability to set and manage usage limits on projects through the [POST /project](22.html) and [GET /project/{id}](22.html) endpoints has been added.\n- [Quota Limit](71.html) now includes improvements such as highlighting the active quota in green, offering options to cancel active quotas, among others. ### October 17th - Rebranding to Globant Enteprise AI\n- Improvements in RAG - New option not to [compact the conversation history](297.html) - Support for Redis, MongoDB Atlas and PG Vector [vectorstores](267.html) - New endpoint to get an embedding. Check details here: [Embeddings API](422.html)\n- Playground improvements\n- File management improvements\n- New [LLMs supported](200.html) - NVIDIA provider with new models supported - nvidia.nemotron-mini-4b-instruct - meta.llama-3.1-8b-instruct - meta.llama-3.1-70b-instruct - meta.llama-3.1-405b-instruct - meta.llama-3.2-3b-instruct - Groq provider supported - groq/llama-3.1-70b-versatile - groq/llama-3.2-11b-vision-preview - groq/llama-3.2-3b-previewgroq/llama-3.2-1b-preview\n- New [embeddings models](447.html) added - Vertex AI: - vertex_ai/textembedding-gecko - vertex_ai/text-embedding-004 - vertex_ai/textembedding-gecko-multilingual - Nvidia: - nvidia/nvclip - nvidia/nv-embed-v1 - nvidia/baai.bge-m3 - nvidia/snowflake.arctic-embed-l - nvidia/nv-embedqa-mistral-7b-v2 - nvidia/embed-qa-4 - nvidia/nv-embedqa-e5-v5 ### September 25th - [Support for file](346.html) processing with prompt-based assistants. This will enable many scenarios, such as uploading documents and summarizing, extracting, and checking information, etc. Also, depending on the model used by the assistant, it will be able to process audio, video, or images.\n- [Support for multi-modal](345.html) LLMs allow processing docs, audio, video, and images in models like GPT-4o or Gemini Pro.\n- Chat with data assistants - The model used to build the queries was updated with GPT-4o, which improves the quality of the generated query. - Configure the query builder server by organization and project. This means you can connect with different DBMS from each project when building Chat with data assistants. - Show an explanation of how the query was built.\n- New [Playground Interface](349.html) design - New design - Upload documents from the front end to chat with them.\n- [Flows builder](321.html) - There will be two types of Flows, one more oriented to build a conversational UI and the other to build assistant flows. Access to these flows will only be available through [Chat API](34.html) or through the [channels](384.html) offered by Flows.\n- New models hosted in AWS Bedrock added: - Amazon Titan Express v1 - Amazon Titan Lite v1 - Anthropic Claude 3 Haiku - Anthropic Claude 3 Sonnet - Anthropic Claude 3.5 Sonnet - Cohere Command - Meta Llama 3 8B - Meta LLama 3 70B\n- It is now possible to provide clear guidance on the assistant's capabilities, allowing you to add information such as descriptions, features, and example prompts. This configuration can be done from the Backoffice, [Start Page](408.html), or WelcomeData section of the [Assistant API](35.html) and [RAG Assistants API](29.html) endpoints.\n- RAG Assistants - New endpoint in [RAG Assistants API](29.html): using [DELETE/profile/{name}/documents](29.html), all documents associated with the specified RAG Assistant are deleted. ### August 9th - Support of new models - GPT-4o mini\n- RAG Assistants - New option called [CLEANUP](60.html) allows to delete the documents associated to a RAG Assistant. - When creating a new assistant, the following defaults are updated: - [Chunk Count](69.html) updated from 2 to 5. - [Model Name](68.html) from gpt-3.5-turbo to gpt-4o-mini.\n- Data Analyst Assistant - New [Conversational Module](291.html) for Data Analyst Assistant.\n- Considerations - **Important**: [max_tokens integer below minimum value](290.html) error may occur if you are using the _max_tokens_ value as -1; those Assistants must be updated to use a correct positive value. ### July 4th - Permissions by assistants in role-based access control. A new [role and permissions management system](242.html) has been implemented to provide greater flexibility and control. Please read [Migration to the new Roles and Permissions Management System](264.html).\n- Dashboard performance optimizations. Please read [Using the Request Summary Table for Improved Dashboard Performance](273.html).\n- RAG Improvements - New option to customize the [context](205.html). - Support for [Max Marginal Relevance](206.html) - Support for _similarity_hybrid_ and _semantic_hybrid_ for [Azure AISearch](207.html) - Advanced options for SelfQuery retriever prompt case. See [Self Query Use Case](150.html) - Support for Parent Document retrieval option. See [Parent Document Retrieval Strategy](204.html) ### June 10th - [Enterprise AI Proxy](19.html) is deprecated. Use [Chat API](34.html) instead.\n- Support for new LLMs - OpenAI new model GPT-4o - Models in Google Vertex - Gemini 1.0 Pro - Gemini 1.5 Flash preview-0514 - Gemini 1.5 Pro preview-0514 - Claude 3 Haiku - Claude 3 Opus - Claude 3 Sonnet\n- RAG Improvements - New option to initialize RAG Assistant based on another when creating a new RAG Assistant. - New option to export document list in View Documents over a RAG Assistant. - Added filter options when browsing Documents. - SelfQuery RAG retriever partial support for a [customized Prompt](150.html). - Support for text-embedding-004 in Google models to generate the embeddings.\n- Deprecated [Assistant API](35.html) endpoints. - /assistant/text/begin - /assistant/text\n- Support to deploy in Google Cloud Platform. ### May 8th - New [Chat with Data Assistant](159.html).\n- New [Ingestion SDK](172.html) to automate document ingestion in RAG assistants.\n- New models hosted in NVIDIA platform supported. See [Supported Chat Models](200.html) for more details.\n- New option to export information about projects and members available for the organization administrator.\n- New API to extend dataset for [Data Analyst Assistant 1.0](89.html).\n- New filter by user email in Requests.\n- Update default to use _text-embedding-3-small_ OpenAI Embeddings for new RAG assistants.\n- Support for gemini-1.5-pro-preview-0409 model added. ### April 3rd - New [API Assistant](110.html).\n- New [Data Analyst Assistant 1.0](89.html).\n- The user Dashboard displays the email to identify the most active user or the one who generates the most costs. ### March 11th - GeneXus Identity Provider is implemented, expanding the login options in the [Backoffice](42.html) of the production environment. This allows for login not only with Google but also with Apple or GeneXus Account.\n- It is possible to customize the icon for each assistant. ### February 29th - [Frontend](82.html) improvements in UI/UX.\n- Option to get feedback from end users when interacting with [RAG Assistant](44.html).\n- Gemini Pro LLM support.\n- New [Dashboard](562.html) with user metrics.\n\n# Updates Log This log shows the most important fixes or features added to the platform. ## Log ## 2025 ### April 10th - New [Globant Enterprise AI Lab](972.html) - The Lab is designed for defining, managing, and orchestrating autonomous AI agents. It provides a standardized model for representing agents, their capabilities, and their interactions within complex workflows. The core components of the Lab include: - Agents & Tools: This module allows for the definition and management of individual agents and their resources, such as skills and tools. It serves as a central hub for cataloging and managing the agent workforce. - Agentic Processes: This component enables the definition of processes based on tasks executed by Agents. These workflows facilitate collaboration among agents to achieve larger objectives. More details at [How to create an Agentic Process](985.html). - Agent RunTime: This module provides the execution environment for agentic workflows, where agents perform tasks based on their skills and interact with artifacts, driven by events and the flow of knowledge. - The Lab aims to meet the growing demand for intelligent, self-sufficient AI agents capable of collaborating and solving complex problems with minimal human intervention. It offers a flexible and adaptable model, allowing for the creation and management of a diverse range of agents, from co-pilots working alongside humans to fully automated agents executing complex tasks. Implemented as a module of Globant Enterprise AI, the Lab supports the development of intelligent agents that can work autonomously or in collaboration with humans and other agents.\n- New features in Flows - Agent Integration Component: You can now directly integrate agents created with the AI Lab into a Flow. These agents can be exposed through platforms like WhatsApp, Teams, or Slack. - [File Upload Support](1053.html) from WhatsApp: Flows now support receiving file attachments such as documents, images, audio, and video directly from WhatsApp interactions. - Audio and Video Attachment in Web Chat: The Web Chat component now allows users to attach audio and video files, enhancing the interaction experience.\n- New Features for the Data Analyst Agent - Reduced Configuration Requirements: The setup needed to enable the assistant to respond to a wide range of questions has been minimized. - Enhanced Analysis Module: An additional analysis module has been incorporated to complement the responses with relevant business conclusions and interpretations of the obtained data.\n- New metrics to track processed tokens. - A new view has been added to each project dashboard to monitor token-based consumption. More details at [Tokens tab in Dashboard](965.html). - To enable this feature, you must run the initialization process described in [Reprocessing Requests and Loading the Summary Table](1015.html).\n- LLMs: - New Gemini 2.5 Pro (via providers Vertex AI and Gemini): Gemini 2.5 is Google\u2019s latest reasoning model, engineered to tackle increasingly complex challenges. This model is designed for tasks that demand advanced analytical thinking and robust problem-solving capabilities. More details at [LLM API](663.html). - Migration to Gemini 2.0 series (Vertex AI): Based on recommendations from Vertex AI, we have migrated from the legacy Gemini 1.0 and 1.5 models to the more advanced Gemini 2.0 series, offering improved performance, scalability, and integration capabilities. For comprehensive information, please refer to the [Deprecated Models](Deprecated Models). - New Azure OpenAI models (o1, o1-mini and o3-mini): We have expanded our model availability by introducing these models via Azure, providing the same high-quality capabilities as those offered through the OpenAI provider. - DeepSeek-R1 via AWS Bedrock: Recently added through a Serverless API, DeepSeek-R1 offers reliable inference with a substantial 128K token context window and up to 32K maximum output tokens. - OpenAI's new models with built-in web search tool: These specialized models integrate web search capabilities directly into the Chat Completions API, enabling them to both interpret and execute search queries in real time. - openai/gpt-4o-search-preview - openai/gpt-4o-mini-search-preview - New models - Beta only: - gemini/gemma-3-27b-it: Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 has a large, 128K context window, and multilingual support in over 140 languages. - SambaNova: - DeepSeek-R1: This Provider offers the fastest performance for running DeepSeek, processing up to 198 tokens per second per user, with a 16K token context window. The model is hosted in US data centers; privately and securely. - DeepSeek-V3-0324: This model significantly outperforms its predecessor with enhanced reasoning benchmarks, improved code executability and refined web aesthetics, and superior Chinese writing aligned with the R1 style. It also offers better multi-turn interactive rewriting, translation quality, detailed report analysis, and more accurate function calling. - QwQ-32B: SambaNova provides access to QwQ-32B-Preview, the best open source test-time compute model released by Alibaba. - Llama 4 collection by Meta: - Llama 4 Scout: A 17B-parameter multimodal MoE model with 16 experts that excels in text and image understanding. The model is currently in beta and is available via providers Nvidia, Groq and SambaNova. Via Groq, it supports a 128k tokens context window with fast inference at 460 tokens/sec, while SambaNova Cloud runs at 697 tokens/second/user. - Llama 4 Maverick: Available via Nvidia, this 17-billion-parameter model featuring 128 experts supports a 32k-token context window. ### March 10th - New LLMs: - GPT-4.5 - Claude 3.7 Sonnet (Providers Anthropic, Vertex AI and AWS Bedrock) - Updates in Gemini 2.0 series: - vertex_ai/gemini-2.0-flash-lite-preview-02-05 - vertex_ai/gemini-2.0-flash-thinking-exp-01-21\n- RAG Revision #6 - Support for o3-mini, gpt-4.5-preview, claude-3-7-sonnet-20250219, new DeepSeek, Gemini2* and sambanova LLM providers. - New pinecone provider available for [embeddings](447.html) and [rerankers](638.html). - The [CleanUp](60.html) action message has been corrected to clearly specify that it will permanently delete the RAG Assistant files and update the information in the RDS. - Added [usage](712.html) element on every response. - Improvements when changing the [LLM](69.html)/[Embeddings](68.html) settings; all models and providers are normalized to be selected from standard combo-box items; use the [override mechanism](209.html) if you need other options. - Support for [guardrails](840.html). - New [documentAggregation](298.html) property to decide how sources are grouped and returned.\n- It is possible to [provide feedback](170.html) on the response of the Chat with Data Assistant in the Frontend.",
          "evaluationResultRowContextDocId": "68ace6a6-8b1b-4736-a466-9ed0a67b97a9",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "382c43a1-5c42-4ccc-b97f-dd80d6cfe13b",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?31",
          "evaluationResultRowContextName": "31",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#How to create an API Assistant Here is a step-by-step guide on how to use [[110]] and start chatting with the assistant from the Playground. ## Step 1: Get the OpenAPI 3.0 Specification The first step is to get the OpenAPI 3.0 Specification for the API you want to call. Make sure to give it the right structure, taking the [[124|Weather API]] sample as a reference. ## Step 2: Create API Assistant Enter the [[42]]. On the left side of the screen, you can find the Backoffice menu. In this menu, click on Assistants. Then click on CREATE API ASSISTANT. Next, in the Project Dynamic Combo Box, select the project you want to work with (in this case, the Default one is used). [[918]] ## Step 3: Fill General Information In this step, you must provide the description of your API. Even though this description is optional, it may be helpful for the Assistant to understand the purpose and functionality of your API. You can include any other relevant instructions you deem necessary to guide the Assistant effectively. For example, for the Weather API, it can be something like the following: ```\nThe final response must strictly comply with the following:\nIt must be in the same language as the user's request.\nThe response text must be formatted as a Table, with 2 columns and rows.\nIn case it exists, it must return the LINK to view more details of the Weather.\nDraw an emoji that roughly describes the weather condition, in the Condition column along with the description.\n``` ## Step 4: Fill OpenAPI JSON This step involves completing the OpenAPI JSON file with the OpenAPI 3.0 specification or higher. This is where you specify the technical details of your API, including paths, HTTP methods, query parameters, request and response bodies, among others. This file acts as a detailed specification of your API, allowing for accurate and consistent interaction with the Assistant. [[124|Here]], you can check an example of how to fill OpenAPI JSON. [[112]] Then, you can click on the VALIDATE API button for a basic check of the JSON, which checks if it is properly formed and if it has any services defined. In addition, it also checks if the URL of the service is correctly defined. It should be noted that this validation does not run a test against the API. ## Step 5: Set Authentication In this step, it is possible to define whether the API requires authentication, and if so, what type of authentication will be used. Globant Enterprise AI offers support for authentication based on ApiKeys, which can be Basic or Bearer. [[113]] If the API does not require authentication, select the None option. Otherwise, you must set the API Key value. When setting Authentication Type with the API Key value, make sure to provide the service's API key in the ApiKey field. This unique key gives you exclusive access to the API. In addition, you must specify the Auth Type, where you will have the option to select between the Basic or Bearer values. ## Step 6: LLM Settings This step consists of customizing the settings related to the language model to adapt it to your specific needs and obtain optimal performance. [[115]] - **Provider:** It determines that the language model provider is OpenAI.\n- **Model:** Select the specific language model you want to use. You can choose from a variety of options, from GPT-3.5 to GPT-4, which have their own unique features and capabilities. The default model is \"gpt-3.5-turbo-16k\".\n- **Temperature:** Adjusts the temperature of the model to control creativity and diversity in the answers generated. Lower values produce more conservative answers, while higher values encourage creativity. The default value is 0.10.\n- **Max output Tokens:** Defines the maximum number of tokens allowed in each answer. This setting determines the maximum length of the answers generated by the language model. The default value is 1024. It is important to note that these settings are optional and can be left with their default values if you want. ## Step 7: SAVE and CONFIRM Once you have completed the configuration, click on the SAVE button. [[116]] This will open a popup window where you can enter the Name and Description of your API. You can also add a file for Icon. After entering the required information, click on CONFIRM to save all changes. [[117]] ## Step 8: Updated API Assistant Once you have created the API Assistant, you return to the Assistants page where you can click on UPDATE. [[919]] This option allows you to view the version identifier with which it was saved. In addition, you can change the name and description, configure it as enabled or disabled, or add an icon if you wish. [[119]] ## Step 9: Edit API Assistant To edit the API Assistant, go to the Assistants home page and click on CONFIG. [[920]] By editing the Assistant, you can manage and maintain different versions of your project efficiently. If you want to overwrite the existing version, click on the button SAVE IN CURRENT REVISION. On the other hand, to create a new version with the changes you have made, choose the SAVE AS NEW REVISION option. In this way, you can control and organize your modifications as needed. Also, to view all versions of the Assistant, you can go to the Assistant home page and click on ALL REVISIONS. ## Step 10: Test your API Assistant Finally, you can test your API Assistant by clicking on Playground on the left side menu of the Backoffice window: [[916]] The result of your query will look as follows: [[122]] It is also possible to use the [[34##API Assistant Sample]]. > **Note**: All LLM requests needed to call the API go through [[34]], allowing you to view the requests, including the payload exchanged between the LLM and Globant Enterprise AI. ## Troubleshooting **Issue:** The assistant is not accepting my API token. **Solution:** Verify that the API token is correctly formatted and included in the Authorization header. Make sure that you are using the correct token type (Basic or Bearer). **Issue:** The AI does not understand my prompts accurately. **Solution:** Refine your prompts to be more specific and clear. Adjust the AI model settings if necessary to better align with your interaction goals. For further assistance or inquiries, please refer to support documentation or contact help desk. {{{12}}} {{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "024aa563-09ab-4888-aaaf-ca12b9e2418d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "be692952-dbfd-4289-a189-a82ce724ef21",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?123",
          "evaluationResultRowContextName": "123",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "``` ## Step 3: Creating the Flow Follow the steps described in [[322]] to create the Flow called Mall Chat Assistant with the following structure: [[364]] To add variables to the Flow, click on the Variable option in the LOGICAL section of the [[371##Flows menu|Flows Menu]]. Next, drag the component below the User input node into the flow. In this case, a variable has been added: today. **Today variable:** It is defined by clicking on the down arrow inside the variables node and selecting the option + Add new variable. Next, enter the name today and press Enter. Deactivate the Clear option. To set the value of this variable, click on the edit icon to the right of the node. In the left menu, click on the Open in new window icon, and in the window that opens (Value) add `$today`. ## Step 4: Implementing the Derivation Logic Every end user query must be evaluated to determine which assistant is the most appropriate to answer. This is implemented with IF conditions in the flow, where the end user's intentions are compared with the available assistants. To add the Conditional component, go to the Interactions section and, below LOGICAL in the Flow menu, drag the component and drop it between the Assistant and Go to nodes. [[365]] Dragging the Conditional node generates a branch with two paths: one labeled If and one labeled Else. This structure allows you to evaluate a condition and direct the flow according to the result. To add an additional condition, click on the Add condition option to the right of the first node of the branch: [[366]] After clicking on Add condition in the first node of the branch, an additional node of Expression type is added, just like the first one. These Expression nodes allow evaluating different conditional expressions. [[367]] To add the variable to be evaluated, click on the word Variable, located at the bottom of each node. In the first two nodes of Expression type, you can see how each condition evaluates the `orchestrationResponse` variable. Depending on the value of this variable, the flow is directed to different assistants: either to the Movie Ticket Assistant or to the Mall Shops Catalog. If neither of these conditions is met, the flow continues through the Else branch, where by default there is a Go to node. A Message node has been added before the Go to node so that the flow can display a specific message to the end user. Then, the corresponding nodes are added to handle the responses according to the evaluated condition. [[368]] A node of API Assistant type that is configured for the Movie Ticket Assistant and handles queries related to the movie theater in the shopping mall is placed below the first node of Expression type (which checks if `orchestrationResponse` is equal to \"Movie Ticket Assistant\"). A node of RAG Assistant type that is configured for the Mall Shop Catalog Assistant and provides information about the stores in the mall is added below the second node of Expression type (which checks if `orchestrationResponse` is equal to \"Mall Shops Catalog\"). Finally, below each node a Go to is added, which redirects back to the User Input node. Once the assistant has processed and answered the query, this allows the flow to return to the entry point to receive new user interactions, ensuring a continuous and fluid conversation. [[369]] ## Step 5: Testing and Adjusting the Mall Chat Assistant Once you have configured the flow, make sure to click on the 'Save' button located at the top left of the screen. This step is essential to save all changes made to the flow. To test the flow you have created, you have three options available from the same design window: 1. Click on the arrow icon (similar to the play button) to run the flow ('Run Flow').\n2. Click on the message icon to open the testing assistant ('Open Testing Assistant').\n3. Click on the open in new window icon (the box with an arrow) to access the demo page ('Go to Demo Page'). The first two options will open a window on the right side of the screen, while the third option will open a new window in the browser, allowing you to see how the flow runs in a demo environment. During testing, you can run various queries to make sure that the flow responds correctly. For example, you can ask \"Where can I purchase books?\" and the flow should correctly identify the query and direct it to the Mall Shop Catalog, where the end user will be informed about the availability of stores that sell books. Then, you can ask \"What movies can I watch?\" and verify that the flow correctly redirects to the Movie Ticket Assistant to get the list of available movies. If the answer is ambiguous, as in \"I would like to watch the last one\", the orchestrator should use the context of the conversation to identify the last movie mentioned and continue the interaction with the Movie Ticket Assistant. In situations where the query does not correspond to any assistant\u2014such as \"What is Linux?\"\u2014the orchestrator should handle the response using a fallback and provide the information directly from the model, so that the flow covers all possible end user interactions. These tests will allow you to adjust the flow as needed, ensuring that the Mall Chat Assistant runs efficiently and delivers a smooth and accurate experience for end users. {{{12}}}{{{17}}}##MARKDOWN##\n\n#How to create a Flow with Conditional component This is a step-by-step guide to create a Mall Chat Assistant using [[321|Flows]] in Globant Enterprise AI. This Flow manages multiple assistants, which are specialized in different areas such as querying store catalogs, booking movie tickets, and orchestrating conversations. The steps required to set up and test this assistant are described below. ## Step 1: Configuring the Work Environment To get started, first make sure you have access to the [[42]] and are familiar with the [[371|interface for creating Flows]]. This environment will allow you to create, manage, and orchestrate the various assistants that make up the Mall Chat Assistant. ## Step 2: Creating Assistants ### Mall Shop Catalog Assistant This [[44|RAG Assistant]] is designed to provide information about the stores in the mall. The objective is to allow end users to check the availability of products or services in the different stores of the mall. The assistant is configured to answer questions related to the location, products, and opening hours of the stores. It uses a simple JSON document containing the store name and a short description. ### Movie Ticket Assistant This [[110]] allows end users to search the movie listings available in the mall and book tickets. It is based on the integration with an API that simulates the availability of movies in a movie theater. This assistant must be able to handle questions about schedules and reservations. ### Orchestrator Mall Assistant This [[566##Chat Assistant creation|Chat Assistant]] manages and redirects conversations to the appropriate assistant. The orchestrator is responsible for understanding the end user's intent and redirecting the conversation accordingly. It uses a system of predefined prompts to redirect queries to the corresponding assistant. The orchestrator keeps the context of the conversation, so the interaction is fluid and coherent. It defines variables such as user name and date. Orchestrator Mall Assistant Prompt: ```\nAssistant is a large language model.\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions.\nAdditionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\nAssistant is a helpful Mall Customer Service agent; you are kind but you always follow the rules. CONVERSATION CONTEXT ---\nThe user name is: {name} Today is: {today} OTHER ASSISTANTS\n---\nAssistant can ask the user to use other Assistants to look up information that may be helpful in answering the user\u2019s original question. The other Assistants the human can ask are:\n> Mall Shops Catalog: This Assistant provides information about shops inside the mall.\n> Movie Ticket Assistant: This assistant provides information about the movie theater inside the mall. RESPONSE FORMAT\n---\nWhen responding to me, if you want the human to ask another Assistant JUST ANSWER WITH THE ASSISTANT NAME AND NOTHING ELSE.\nFor any other response, you can create an HTML fragment with RICH TEXT if it's needed.",
          "evaluationResultRowContextDocId": "57a2c916-1a55-4579-abb9-419226e4e2c1",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "fb9133db-f173-4f0b-a785-f790d919afd0",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?363",
          "evaluationResultRowContextName": "363",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.3,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "142e0294-29e6-4df6-b839-2e075a99eb15",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0042898000",
      "evaluationResultRowEndDate": "2025-05-13T11:12:34",
      "evaluationResultRowOutput": "The supported LLM providers in Globant Enterprise AI include:\n\n1. **OpenAI </B> \n   - Models such as GPT-3.5, GPT-4 (including GPT-4 Turbo, GPT-4o mini, GPT-4o)  \n   - Special models with reasoning capabilities (e.g., o-series models)  \n   - Models with built-in web search tools like `gpt-4o-search-preview`\n\n2. **Anthropic </B> \n   - Claude 3.7 Sonnet and other versions such as Claude 3 Haiku, Claude 3 Opus, Claude 3.5 Sonnet  \n   - Supports \"Extended Thinking\" mode for reasoning\n\n3. **Google Vertex AI </B> \n   - Gemini series models including Gemini 1.0, 1.5, 2.0, 2.5 Flash and Pro versions  \n   - Supports reasoning budget via `thinking` object\n\n4. **NVIDIA </B> \n   - Models such as `nvidia.nemotron-mini-4b-instruct`, `nvidia/nvclip`, `nvidia/baai.bge-m3`, among others\n\n5. **Groq </B> \n   - For example, `groq/llama-3.1-70b-versatile`, `groq/llama-3.2-11b-vision-preview` and others\n\n6. **AWS Bedrock </B> \n   - Includes DeepSeek-R1 and others\n\n7. **SambaNova </B> \n   - Models like QwQ-32B-Preview, DeepSeek-V3-0324\n\n8. **Meta (Llama 3 & 4 Series) </B> \n   - e.g., Meta Llama 3.1 models, Llama 4 Scout and Maverick\n\n9. **Cohere Command**\n\n10. **Additional providers/models </B> \n    - Azure OpenAI models such as `o1`, `o1-mini`, `o3-mini`  \n    - Pinecone embeddings & rerankers support  \n\nThis wide range of providers allows support for various AI models optimized for different use cases including reasoning, extended token contexts, multimodal input (image, video, audio), and more advanced AI capabilities.\n\nFor more detailed info and reasoning features per provider, see [LLMs with Reasoning Capabilities](https://wiki.genexus.com/enterprise-ai/wiki?1168) and Updates Logs in [2023-2025](https://wiki.genexus.com/enterprise-ai/wiki?31) and [General LLMs Info](https://wiki.genexus.com/enterprise-ai/wiki?31).\n\nReferences:\n\n- [LLMs with Reasoning Capabilities](https://wiki.genexus.com/enterprise-ai/wiki?1168)  \n- [Updates Log](https://wiki.genexus.com/enterprise-ai/wiki?31)  \n- [Updates Log](https://wiki.genexus.com/enterprise-ai/wiki?31)",
      "evaluationResultRowStartDate": "2025-05-13T11:12:22",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "9038"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#How to create an API Assistant Here is a step-by-step guide on how to use [[110]] and start chatting with the assistant from the Playground. ## Step 1: Get the OpenAPI 3.0 Specification The first step is to get the OpenAPI 3.0 Specification for the API you want to call. Make sure to give it the right structure, taking the [[124|Weather API]] sample as a reference. ## Step 2: Create API Assistant Enter the [[42]]. On the left side of the screen, you can find the Backoffice menu. In this menu, click on Assistants. Then click on CREATE API ASSISTANT. Next, in the Project Dynamic Combo Box, select the project you want to work with (in this case, the Default one is used). [[918]] ## Step 3: Fill General Information In this step, you must provide the description of your API. Even though this description is optional, it may be helpful for the Assistant to understand the purpose and functionality of your API. You can include any other relevant instructions you deem necessary to guide the Assistant effectively. For example, for the Weather API, it can be something like the following: ```\nThe final response must strictly comply with the following:\nIt must be in the same language as the user's request.\nThe response text must be formatted as a Table, with 2 columns and rows.\nIn case it exists, it must return the LINK to view more details of the Weather.\nDraw an emoji that roughly describes the weather condition, in the Condition column along with the description.\n``` ## Step 4: Fill OpenAPI JSON This step involves completing the OpenAPI JSON file with the OpenAPI 3.0 specification or higher. This is where you specify the technical details of your API, including paths, HTTP methods, query parameters, request and response bodies, among others. This file acts as a detailed specification of your API, allowing for accurate and consistent interaction with the Assistant. [[124|Here]], you can check an example of how to fill OpenAPI JSON. [[112]] Then, you can click on the VALIDATE API button for a basic check of the JSON, which checks if it is properly formed and if it has any services defined. In addition, it also checks if the URL of the service is correctly defined. It should be noted that this validation does not run a test against the API. ## Step 5: Set Authentication In this step, it is possible to define whether the API requires authentication, and if so, what type of authentication will be used. Globant Enterprise AI offers support for authentication based on ApiKeys, which can be Basic or Bearer. [[113]] If the API does not require authentication, select the None option. Otherwise, you must set the API Key value. When setting Authentication Type with the API Key value, make sure to provide the service's API key in the ApiKey field. This unique key gives you exclusive access to the API. In addition, you must specify the Auth Type, where you will have the option to select between the Basic or Bearer values. ## Step 6: LLM Settings This step consists of customizing the settings related to the language model to adapt it to your specific needs and obtain optimal performance. [[115]] - **Provider:** It determines that the language model provider is OpenAI.\n- **Model:** Select the specific language model you want to use. You can choose from a variety of options, from GPT-3.5 to GPT-4, which have their own unique features and capabilities. The default model is \"gpt-3.5-turbo-16k\".\n- **Temperature:** Adjusts the temperature of the model to control creativity and diversity in the answers generated. Lower values produce more conservative answers, while higher values encourage creativity. The default value is 0.10.\n- **Max output Tokens:** Defines the maximum number of tokens allowed in each answer. This setting determines the maximum length of the answers generated by the language model. The default value is 1024. It is important to note that these settings are optional and can be left with their default values if you want. ## Step 7: SAVE and CONFIRM Once you have completed the configuration, click on the SAVE button. [[116]] This will open a popup window where you can enter the Name and Description of your API. You can also add a file for Icon. After entering the required information, click on CONFIRM to save all changes. [[117]] ## Step 8: Updated API Assistant Once you have created the API Assistant, you return to the Assistants page where you can click on UPDATE. [[919]] This option allows you to view the version identifier with which it was saved. In addition, you can change the name and description, configure it as enabled or disabled, or add an icon if you wish. [[119]] ## Step 9: Edit API Assistant To edit the API Assistant, go to the Assistants home page and click on CONFIG. [[920]] By editing the Assistant, you can manage and maintain different versions of your project efficiently. If you want to overwrite the existing version, click on the button SAVE IN CURRENT REVISION. On the other hand, to create a new version with the changes you have made, choose the SAVE AS NEW REVISION option. In this way, you can control and organize your modifications as needed. Also, to view all versions of the Assistant, you can go to the Assistant home page and click on ALL REVISIONS. ## Step 10: Test your API Assistant Finally, you can test your API Assistant by clicking on Playground on the left side menu of the Backoffice window: [[916]] The result of your query will look as follows: [[122]] It is also possible to use the [[34##API Assistant Sample]]. > **Note**: All LLM requests needed to call the API go through [[34]], allowing you to view the requests, including the payload exchanged between the LLM and Globant Enterprise AI. ## Troubleshooting **Issue:** The assistant is not accepting my API token. **Solution:** Verify that the API token is correctly formatted and included in the Authorization header. Make sure that you are using the correct token type (Basic or Bearer). **Issue:** The AI does not understand my prompts accurately. **Solution:** Refine your prompts to be more specific and clear. Adjust the AI model settings if necessary to better align with your interaction goals. For further assistance or inquiries, please refer to support documentation or contact help desk. {{{12}}} {{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "024aa563-09ab-4888-aaaf-ca12b9e2418d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "0c4a5ccf-f927-45e1-b4af-3e47900806e3",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?123",
          "evaluationResultRowContextName": "123",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Public Tools in Globant Enterprise AI Lab [[972]] provides **Public Tools** in every project. These **Public Tools** are preconfigured and available by default for use when defining [[973|Agents]]. Refer to the following dialog: [[1019]] You can access this dialog from the [[1067|AI & Tools Tab of an Agent]] by going to the **Agents & Tools to use** section and clicking on **+ Add**. In this specific case, a filter has been applied to display only **Public Tools**. Therefore, in the left pane, you can see the list of tools starting with com.globant.geai, which correspond to **Public Tools**. Some Public Tools require custom configuration to work properly. These tools are marked with a warning icon, indicating that setup is required before they can be used. Below is a list of Public Tools that require configuration, along with the specific settings needed for each. ## Google Drive Tools Through the following Tools you can create Google Docs and upload images to Google Drive: * `com.globant.geai.gdrive_create_docs_post` * `com.globant.geai.gdrive_upload_image` Before configuring these Tools, you have to go to the Google Cloud Platform to get a JSON file that stores the service account credentials. To achieve this, proceed as follows: 1. Log in to the [https://console.cloud.google.com/ | Google Cloud Console]. 2. Create a new project or select an existing one. 3. Activate the following APIs for that project: * Google Drive API * Google Docs API\n4. From the side menu, go to **IAM & Admin \\> Service Accounts** and create a new service account. 5. Grant the necessary roles (for example, Editor). 6. In the created service account, go to the Keys tab and create a new key in JSON format and download the file. This file contains the credentials needed to authenticate requests. This JSON file will be used later. Once the previous steps have been carried out from the Google Cloud Platform: 1. Go to the [[42]] to [[1020|create a Secret]]. Enter a descriptive name in the **Name** field (for example 'gdrives'). This name is the one that will be later referenced in the **credentials** parameter of the Tool. 2. In the **Value** field of the Secret, paste the full content of the JSON file downloaded from Google Cloud Platform (containing the service account credentials). Confirm to save the Secret.\n3. Inside the Agent you are creating, go to the [[1067|AI & Tools Tab]], select the **Agents & Tools to use** section and click on **+ Add**. In the left pane, select the desired Tool (**com.globant.geai.gdrive_create_docs_post** or **com.globant.geai.gdrive_upload_image**). Also, click on the Tool in the left pane to see the required **Parameters**. Proceed to fill the values in the **Parameters** column. ### Parameters Both Google Drive tools require you to complete the following parameters (in the **Parameters** column): * **credentials**: The name of the Secret that stores the service account credentials in JSON format (in this example, the Secret name is 'gdrives'). * **email**: The email address to which the Google Drive file generated by the Tool will be sent. ## Jira Tool Through the following Tool you can create Jira tickets: * `com.globant.geai.jira_ticket_insert` Before configuring this Tool, you have to get required information from the Atlassian platform. To do so, follow the steps below: * Go to [https://id.atlassian.com/manage-profile/security/api-tokens | Atlassian API Tokens] and log in.\n* Click on **Create a classic API Token**, name it (for example: \u201cJira Tool\u201d), and copy the token generated. Once the previous steps have been carried out from Atlassian: 1. Go to the [[42]] to [[1020|create a Secret]]. 2. Enter a descriptive name in the **Name** field. This name will be used later in the **token** parameter of the Tool.\n3. In the **Value** field, paste the API token value previously obtained.\n4. Save the secret.\n5. Inside the Agent you are creating, go to the [[1067|AI & Tools Tab]], select the **Agents & Tools to use** section and click on **+ Add**. In the left pane, check the desired tool (**com.globant.geai.jira_ticket_insert**). Also, click on the Tool in the left pane to see the required **Parameters**, and proceed to fill in the values in the **Parameters** column. ### Parameters This Tool requires you to complete the following parameters (in the **Parameters** column): * **url:** Your Jira workspace URL (for example: https://genexus-team.atlassian.net/).\n* **username:** The email address you use to access your Jira account.\n* **token:** The name of the Secret that stores the API token you generated in Atlassian. * **projectKey:** The key of the Jira project where you want to create the tickets (to find it, go to https://organization.atlassian.net/jira/projects replacing **organization** with your Jira organization name \u2015in this case: genexus-team\u2015. You will see a list of projects. Look at the Key column to find the projectKey value you need). ## SharePoint Tools Through the following Tools you can create a Word document and upload images in SharePoint: * `com.globant.geai.sharepoint_create_word` * `com.globant.geai.sharepoint_upload_image` Before configuring the Tool, make sure to prepare the required credentials in JSON format. The structure should look as follows: `{` `\"client_id\": \"your-microsoft-email\",` `\"client_secret\": \"your-password\",` `\"sharepoint_url\": \"https://your-domain.sharepoint.com/sites/your-site\"` `}` Note that \"client_id\" and \"client_secret\" values correspond to the SharePoint user credentials. Remember to indicate** your-domain** and** your-site** for the \"sharepoint_url\" value. Once the previous steps have been carried out: 1. Go to the [[42]] to [[1020|create a Secret]]. 2. Enter a descriptive name in the **Name** field. This name will be used later in the **credentials** parameter of the Tool.\n3. In the **Value** field, paste the folder path where the file will be saved in SharePoint.\n4. Save the secret.\n5. Enter [[972]] and inside the Agent you are creating, go to the [[1067|AI & Tools Tab]], select the **Agents & Tools to use** section and click on **+ Add**. In the left pane, select the desired Tool (**com.globant.geai.sharepoint_create_word** or **com.globant.geai.sharepoint_upload_image**). Also, click on the Tool in the left pane to see the required **Parameters**. Proceed to fill the values in the **Parameters** column. ### Parameters Both SharePoint Tools require you to complete the following parameters (in the **Parameters** column): * **credentials**: Name of the Secret that stores the SharePoint access data in JSON format. * **path**: Folder structure where the file will be saved in SharePoint (for example, `/sites/your-site/Shared Documents/your_folder/another_folder`). ## Availability Since April 2025 release. {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "428cf293-6c59-4fda-b1ee-cfabb041475a",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "4acd7542-c4d1-462e-bf66-8340e40db60e",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?1018",
          "evaluationResultRowContextName": "1018",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# API Tokens API Tokens play a crucial role in executing Globant Enterprise AI APIs. These tokens are required to access and use the functionality provided by the APIs. There are two types of API Tokens: Organization API Tokens and Project API Tokens. ### [Organization API Tokens](#Organization+API+Tokens) Certain operations require API Tokens with a higher scope, such as access to Project creation, updating, and deletion. Users with the necessary privileges can manage this type of API Tokens in order to work only with OrganizationAPI endpoints. These API Tokens are not intended to work at the project level and cannot be used to reference assistants or AI models. ### [Project API Tokens](#Project+API+Tokens) For each project, you can define multiple Project API Tokens. This allows for granular control and tracking of usage. It is important to note that API Tokens have project-wide reach. Moreover, the ability to assign API Tokens to specific projects allows for fine-grained access control, ensuring that only authorized individuals or systems can execute requests on specific projects. With this level of granularity, organizations can effectively manage access permissions, track usage patterns, and maintain control over their projects defined within the Globant Enterprise AI API.",
          "evaluationResultRowContextDocId": "146c275d-bb81-4af3-925b-a96fe8e2f48d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "70d02019-c8af-46dc-90c2-6574acc76acd",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?564",
          "evaluationResultRowContextName": "564",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#How to create a Flow with internet search This is a step-by-step guide to create a conversational assistant with internet search using [[321]]. This Flow manages two assistants: a Chat Assistant that answers questions using internet search results, and another Chat Assistant that rephrases follow-up questions (based on the conversation history) as standalone questions. In addition, it includes a Search API component. ## Step 1: Configuring the Work Environment To get started, first, make sure you have access to the [[42]] and are familiar with the [[371|interface for creating Flows]]. ## Step 2: Creating Chat Assistants In this step, two [[566##Chat Assistant creation|Chat Assistants]] are created. They are called 'Response from Search' and 'Follow-up Rephrase'. ### 'Response from Search' Chat Assistant This Chat Assistant generates answers for the end user based solely on the results of an internet search, without using prior knowledge or inventing information. It takes the provided search results and uses them to answer the end user's question in the best possible way. If necessary, the answer can include rich text in HTML format, but Markdown should not be used to format the answer. **LLM Settings:** - AI Provider: Google VertexAI\n- Model: gemini-1.5-pro\n- Temperature: 0.10\n- Max Output Tokens: 8192 **Prompt:** ```\nINSTRUCTIONS\n---\nYour task is to generate a response for the user based on the results of an internet search.\nDO NOT USE PRIOR KNOWLEDGE AND DO NOT MAKE UP INFORMATION USE THE SEARCH RESULTS.\nGiven the following results from an internet search and NOT PRIOR KNOWLEDGE answer the user question the best you can. SEARCH RESULTS\n---\n{searchResults} RESPONSE FORMAT\n---\nFor the response you can generate RICH TEXT USING HTML if it is needed.\nDO NOT USE MARKDOWN TO FORMAT YOUR RESPONSE.\nFOR RICH TEXT DO NOT USE MARKDOWN USE HTML INSTEAD.\n``` ### 'Follow-up Rephrase' Chat Assistant This Chat Assistant rephrases a follow-up question based on the conversation history, ensuring it becomes a standalone question if needed. If the follow-up question already works as a standalone question, it simply repeats the question without any changes. **LLM Settings:** - AI Provider: Google VertexAI\n- Model: gemini-1.5-pro\n- Temperature: 0.10\n- Max Output Tokens: 8192 **Prompt:** ```\nGiven the conversation history and not prior knowledge, and a follow-up question, rephrase the follow-up question to be a standalone question IF IT IS NEEDED. If the question works as a standalone question, just repeat the question. RESPONSE ONLY WITH THE STANDALONE QUESTION AND NOTHING MORE; DO NOT INCLUDE ANY OTHER TEXT.\n``` ## Step 3: Creating the Flow Follow the steps described in [[322]] to create the Flow called 'Search Assistant' with the following structure: [[689]] ## Step 4: Implementing the API component To add the API component, go to the [[494|Interactions section]] and, below INTEGRATIONS in the Flow menu, drag the component between the Assistants nodes. [[690]] ## Step 5: Setting Up the Search API Go to [https://developers.google.com/custom-search/v1/introduction?hl=es-419|Google Search API documentation] and click on Get API key button. [[691]] In the pop-up window that opens, select or create a project from the Select or create project menu. [[692]] Select the Yes option to agree that the use of the services and related APIs is subject to compliance with the applicable Terms of Service. Then click on NEXT to continue with the setup process. In the next window, click on the SHOW KEY button to reveal the API Key. Copy this API key, as it will be required to configure the API node in your flow. [[693]] ## Step 6: Creating a Programmable Search Engine Go to [https://programmablesearchengine.google.com/controlpanel/all|Programmable Search Engine] and click on the Add button. [[694]] In the pop-up window that opens, fill in the necessary fields to create the search engine: [[695]] - **Name of your search engine:** In the \"Search engine name\" field, enter a name for the search engine, such as Search Assistant.\n- **What to search for:** Select the \"Search the entire web\" option to have the engine search the entire web. If you prefer to search only specific sites, you can select the \"Search specific sites or pages\" option and add the necessary URLs.\n- **Search settings:** If you wish, you can adjust additional settings such as image search or SafeSearch filter.\n- **Captcha:** Select the \"I am not a robot\" checkbox.\n- **Create:** At the end of the window, click on the Create button to finalize the creation of the search engine. In the window that appears, you must click on the Customize button. [[696]] This will take you to the Overview section of the search engine you have just created. In this section, you should find and copy the Search engine ID, which is a unique value associated with your search engine. This ID is required to configure the API node in your flow. [[697]] ## Step 7: Configuring the API Node With the Search engine ID and API key, you can now configure the API node. First, you must click on the Edit button that appears when you position the mouse over the API node. [[698]] Once you have clicked on Edit, the State Configuration menu will be displayed on the left side. [[699]] Within this menu, in the URL section, you must configure the URL with the following value: ```\nhttps://www.googleapis.com/customsearch/v1?key=<API key>&cx=<Search engine ID>&q=<query>\n``` Be sure to replace `<API key>` and `<Search engine ID>` with the previously obtained values. In the API node, go to the **Parameters** section and click on **+ Add new** to add a new parameter. Then, configure the parameter with the following details: - **Parameter name:** q.\n- **Value:** lastResponse (the value obtained from the previous response). Next, in the State Configuration section, you must make sure to mark q as **Query params**. [[700]] In the **Responses** section, make sure to map the items property to the **searchResults** variable. If searchResults is not available, you can click on Add new variable to create it. [[701]] ## Step 8: Configuring the Assistant Node After you have configured the API node, go to the Assistant node, which is located just below the API node in your flow. In this node, find the Variable section and change the input variable to searchResults. [[702]] ## Step 9: Testing the Flow Finally, save the changes and test the flow. You can use the test window or the Playground to make sure everything works correctly, verifying that the Assistant responds properly using the search results. | [[704]] | [[705]] | {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "526b3969-e9ac-4188-a986-6b0013b11c4d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "cd9090a8-c223-41e0-be2d-bd630dc5d34a",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?688",
          "evaluationResultRowContextName": "688",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "{ \"requestId\": \"string\", \"error\": { \"code\": 0, \"message\": \"string\" }, \"providerResponse\": \"string\", \"timestamp\": \"string\", \"status\": \"string\"\n}\n``` ### cURL Sample ```bash\ncurl -X POST $BASE_URL/v1/assistant/request/{requestId}/cancel\n``` ## GET/assistant/{id}/status Gets the current status of the data set loading process from the Data Analyst Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `id` | `string` | Data Analyst Assistant ID. | ### Response StatusCode `200` indicates a successful execution; otherwise, StatusCode `400*` with a collection of errors. Possible return errors: - [405](23.html)\n- [2041](23.html)\n- [3000](23.html) ### cURL Sample ```bash\ncurl --location '$BASE_URL/v1/assistant/{{assistantId}}/status' \\\n-H 'Authorization: &SAIA_APITOKEN' \\\n-H 'Content-Type: application/json' \\\n``` ## POST/assistant/{id}/datasets/extend Upload one or more dataset files to the associated {id} Data Analyst Assistant. Note that the file extension must be .csv. ### Response StatusCode `200` indicates a successful execution; otherwise, StatusCode `400*` with a collection of errors. Possible return errors: - [2028](23.html)\n- [2041](23.html)\n- [3004](23.html)\n- [3005](23.html) ### cURL Sample ```bash\ncurl --location '$BASE_URL/v1/assistant/{{assistantId}}/datasets/extend' \\\n--header 'Authorization: &SAIA_APITOKEN' \\\n--form 'file=@\"/C:/temp/Dataset1.csv\"'\n--form 'file=@\"/C:/temp/Dataset2.csv\"'\n```\n\n# Assistant API This API enables the creation of new assistants, the modification of their definitions, and the retrieval of information about them. In addition, it allows the execution of prompts associated with previously defined assistants. > The following endpoints require a Globant Enterprise AI API token related to **project** scope. Check the [generic variables](20.html) needed to use the API. ## Endpoints | Method | Path | Description |\n|---|---|---|\n| `GET` | `/assistant/{id}` | Gets assistant data |\n| `POST` | `/assistant` | Creates a new assistant |\n| `PUT` | `/assistant/{id}` | Updates an assistant |\n| `DELETE` | `/assistant/{id}` | Deletes an assistant |\n| `POST` | `/assistant/text/begin` | Begins a text conversation with the Globant Enterprise AI Assistant |\n| `POST` | `/assistant/text` | Sends a text prompt to the Globant Enterprise AI Assistant |\n| `POST` | `/assistant/chat` | Sends a chat request to the Globant Enterprise AI Assistant |\n| `GET` | `/assistant/request/{id}/status` | Retrieves the status of a request |\n| `POST` | `/assistant/request/{id}/cancel` | Cancels a request |\n| `GET` | `/assistant/{id}/status` | Gets the current status of the Data Analyst Assistant data set loading process. |\n| `POST` | `/assistant/{id}/datasets/extend` | Extends the data sets of an existing Data Analyst Assistant. | **Deprecated**: `/assistant/text/begin` and `/assistant/text`. ## GET/assistant/{id} Gets assistant data. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `id` | `string` | Assistant ID. |\n| `detail` | `string` | Defines the level of detail required. The available options are `summary` (default) or `full`. | If you use the default `summary` option, the active revision details will be shown. The `full` option will display information about all revisions. ### Response ```json\n{ \"assistantId\": \"string\", \"assistantName\": \"string\", \"assistantStatus\": \"integer\", /* 1:Enabled, 2:Disabled */ \"intents\": [ /* One element when summary, everything when using full detail*/ { \"assistantIntentDefaultRevision\": \"string\", \"assistantIntentDescription\": \"string\", \"assistantIntentId\": \"string\", \"assistantIntentName\": \"string\", \"revisions\": [ { \"metadata\": [ { \"key\": \"string\", \"type\": \"string\", \"value\": \"string\" }, { \"key\": \"string\", \"type\": \"string\", \"value\": \"string\" } ], \"modelName\": \"string\", \"prompt\": \"string\", \"providerName\": \"string\", \"revisionDescription\": \"string\", \"revisionId\": \"string\", \"revisionName\": \"string\", \"timestamp\": \"timestamp\", \"variables\": [ /* Optional */ { \"key\": \"string\" }, ... ] } ] } ], \"projectId\": \"string\", \"projectName\": \"string\"\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/assistant/{id}\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\"\n# using the full detail option change the URL to\n$BASE_URL/v1/assistant/{id}?detail=full\n``` ## POST/assistant Creates a new assistant. ### Request Body ```json\n{ \"type\": \"string\", /* text, chat */ \"name\": \"string\", /* Required */ \"description\": \"string\", \"prompt\": \"string\", /* Required */ \"llmSettings\": { \"providerName\": \"string\", \"modelName\": \"string\", \"temperature\": \"decimal\", \"maxTokens\": \"integer\", \"uploadFiles\": \"boolean\", \"llmOutputGuardrail\": \"boolean\", \"inputModerationGuardrail\": \"boolean\", \"promptInjectionGuardrail\": \"boolean\" }, \"welcomeData\": { \"title\": \"string\", \"description\": \"string\", \"features\": [ { \"title\": \"string\", \"description\": \"string\" }, ... ], \"examplesPrompt\": [ { \"title\": \"string\", \"description\": \"string\", \"promptText\": \"string\" }, ... ] }\n}\n``` **Note**: If you do not provide the `WelcomeData` section when creating an assistant, it will be empty. ### Response ```json\n{ \"assistantId\": \"string\", \"assistantName\": \"string\", \"assistantStatus\": \"integer\", /* 1:Enabled, 2:Disabled */ \"intents\": [ { \"assistantIntentDefaultRevision\": \"integer\", \"assistantIntentDescription\": \"string\", \"assistantIntentId\": \"string\", \"assistantIntentName\": \"string\", \"revisions\": [ { \"revisionId\": \"integer\", \"revisionName\": \"string\", \"revisionDescription\": \"string\", \"providerName\": \"string\", \"modelName\": \"string\", \"prompt\": \"string\", \"timestamp\": \"timestamp\", \"metadata\": [ { \"key\": \"string\", \"value\": \"string\", \"type\": \"string\" }, ... ], \"variables\": [ /* Optional */ { \"key\": \"string\" }, ... ] } ] } ], \"projectId\": \"string\", \"projectName\": \"string\", \"welcomeData\": { \"title\": \"string\", \"description\": \"string\", \"features\": [ { \"title\": \"string\", \"description\": \"string\" }, ... ], \"examplesPrompt\": [ { \"title\": \"string\", \"description\": \"string\", \"promptText\": \"string\" }, ... ] }\n}\n``` The returned `assistantId` is needed for other related APIs. ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/v1/assistant\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-type: application/json\" \\ --data '{ \"type\": \"chat\", \"name\": \"TestAssistant\", \"prompt\": \"Translate the following text to Esperanto\" }'\n``` #### Sample with WelcomeData ```bash\ncurl -X POST \"$BASE_URL/v1/assistant\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-type: application/json\" \\ --data '{ \"type\": \"chat\", \"name\": \"Welcome data Assistant\", \"prompt\": \"Translate to English\", \"welcomeData\": { \"title\": \"Assistant with WelcomeData\", \"description\": \"It is to test WelcomeData\", \"features\": [ { \"title\": \"First Feature\", \"description\": \"First Feature Description\" }, { \"title\": \"Second Feature\", \"description\": \"Second Feature Description\" } ], \"examplesPrompt\": [ { \"title\": \"First Prompt Example\", \"description\": \"First Prompt Example Description\", \"promptText\": \"You are an assistant specialized in translating ...\" } ] }\n}'\n``` ## PUT/assistant/{id} Updates an existing assistant. The assistant `type` property cannot be changed. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `id` | `string` | ID of the assistant. | ### Request Body ```json\n{ \"name\": \"string\", /* Optional */ \"description\": \"string\", /* Optional */ \"status\": \"integer\", /* Optional 1:Enabled, 2:Disabled */ \"action\": \"string\", /* save, saveNewRevision (default), savePublishNewRevision */ \"revisionId\": \"integer\", /* Required if user needs to update an existent revision when action = save */ \"prompt\": \"string\", /* Required if revisionId is specified or in case of actions saveNewRevision and savePublishNewRevision*/ \"llmSettings\": { \"providerName\": \"string\", \"modelName\": \"string\", \"temperature\": \"decimal\", \"maxTokens\": \"integer\", \"uploadFiles\": \"boolean\", \"llmOutputGuardrail\": \"boolean\", \"inputModerationGuardrail\": \"boolean\", \"promptInjectionGuardrail\": \"boolean\" }, \"welcomeData\": { \"title\": \"string\", \"description\": \"string\", \"features\": [ { \"title\": \"string\", \"description\": \"string\" }, ... ], \"examplesPrompt\": [ { \"title\": \"string\", \"description\": \"string\", \"promptText\": \"string\" }, ... ] }\n}\n\n``` **Note**: If an assistant had `WelcomeData` and an update does not include this section, the existing information is deleted and becomes empty. Therefore, if you want to add new information to the `WelcomeData`, you must include both the old and the new information. If you only send the new one, it will completely replace the old one. The default value of the `action` parameter, `saveNewRevision`, will create a new revision but will not set it as active. The `save` option will update the active revision. The `savePublishNewRevision` option will create a new revision and set it as active. If only an update of `name`, `description`, or `status` is needed, **at least one of them must be provided** without any changes in the revision; it can be specified as: ```json\n{ \"name\": \"string\", \"description\": \"string\", \"status\": \"integer\", /* 1:Enabled, 2:Disabled */ \"action\": \"string\" /* use \"save\" action */\n}\n``` ### Response ```json\n{ \"assistantId\": \"string\", \"assistantName\": \"string\", \"assistantStatus\": \"integer\", /* 1:Enabled, 2:Disabled */ \"intents\": [ /* One element when summary, everything when using full detail*/ { \"assistantIntentDefaultRevision\": \"string\", \"assistantIntentDescription\": \"string\", \"assistantIntentId\": \"string\", \"assistantIntentName\": \"string\", \"revisions\": [ { \"metadata\": [ { \"key\": \"string\", \"type\": \"string\", \"value\": \"string\" }, { \"key\": \"string\", \"type\": \"string\", \"value\": \"string\" } ], \"modelName\": \"string\", \"prompt\": \"string\", \"providerName\": \"string\", \"revisionDescription\": \"string\", \"revisionId\": \"string\", \"revisionName\": \"string\", \"timestamp\": \"timestamp\", \"variables\": [ /* Optional */ { \"key\": \"string\" }, ... ] } ] } ], \"projectId\": \"string\", \"projectName\": \"string\", \"welcomeData\": { \"title\": \"string\", \"description\": \"string\", \"features\": [ { \"title\": \"string\", \"description\": \"string\" }, ... ], \"examplesPrompt\": [ { \"title\": \"string\", \"description\": \"string\", \"promptText\": \"string\" }, ... ] }\n}\n``` ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v1/assistant/{id}\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-type: application/json\" \\ --data '{ \"action\": \"savePublishNewRevision\", \"prompt\": \"translate the following text to Latin\", \"llmSettings\":{ \"modelName\":\"gpt-4o-mini\", \"temperature\":0.0 } }'\n``` #### Sample with Welcome Data Update ```bash\ncurl -X PUT \"$BASE_URL/v1/assistant/{id}\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-type: application/json\" \\ --data '{ \"welcomeData\": { \"title\": \"Updated WelcomeData assistant\", \"description\": \"It is to update WelcomeData\", \"features\": [ { \"title\": \"First Feature Updated\", \"description\": \"Updated First Feature Description\" }, { \"title\": \"Second Feature Updated\", \"description\": \"UpdatedSecond Feature Description\" } ], \"examplesPrompt\": [ { \"title\": \"Updated First Prompt Example\", \"description\": \"Updated First Prompt Example Description\", \"promptText\": \"You are an assistant specialized in translating ...\" } ] }\n}'\n``` ## DELETE/assistant/{id} Deletes an assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `id` | `string` | Assistant ID (required) | ### Response StatusCode `200` indicates a successful deletion; otherwise, StatusCode `400*` with a collection of errors. ### cURL Sample ```bash\ncurl -X DELETE \"$BASE_URL/v1/assistant/{id}\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-type: application/json\"\n``` ## POST/assistant/text/begin (Deprecated) Begins a text conversation with the Globant Enterprise AI Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `assistant` | `string` | Name of the assistant. |\n| `prompt` | `string` | Text prompt for the assistant. |\n| `revision` | `integer` | Revision number. |\n| `revisionName` | `string` | Name of the revision. | ### Response ```json\n{ \"requestId\": \"string\", \"error\": { \"code\": 0, \"message\": \"string\" }, \"success\": true, \"providerName\": \"string\", \"providerResponse\": \"string\", \"progress\": 0, \"timestamp\": \"string\", \"status\": \"string\", \"text\": \"string\"\n}\n``` ### cURL Sample ```bash\ncurl -X POST $BASE_URL/v1/assistant/text/begin \\ -H \"Content-Type: application/json\" \\ --data '{ \"assistant\": \"string\", \"prompt\": \"string\", \"revision\": 0, \"revisionName\": \"string\" }'\n``` ## POST/assistant/text (Deprecated) Sends a text prompt to the Globant Enterprise AI Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `assistant` | `string` | Name of the assistant. |\n| `prompt` | `string` | Text prompt for the assistant. |\n| `revision` | `integer` | Revision number. |\n| `revisionName` | `string` | Name of the revision. |\n| [variables](85.html) | `collection` | A list of key/value properties (optional). | ### Response ```json\n{ \"requestId\": \"string\", \"error\": { \"code\": 0, \"message\": \"string\" }, \"success\": true, \"providerName\": \"string\", \"providerResponse\": \"string\", \"progress\": 0, \"timestamp\": \"string\", \"status\": \"string\", \"text\": \"string\"\n}\n``` ### cURL Sample ```bash\ncurl -X POST $BASE_URL/v1/assistant/text \\ -H \"Content-Type: application/json\" \\ --data '{ \"assistant\": \"AssistantName\", \"prompt\": \"Your Input here\", \"revision\": 1 }'\n``` ## POST/assistant/chat Sends a chat request to the Globant Enterprise AI Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `assistant` | `string` | Name of the assistant. |\n| `messages` | `array` | Chat request data. |\n| `revision` | `integer` | Revision number. |\n| `revisionName` | `string` | Name of the revision. |\n| [variables](85.html) | `collection` | A list of key/value properties (optional). | ### Response ```json\n{ \"requestId\": \"string\", \"error\": { \"code\": 0, \"message\": \"string\" }, \"success\": true, \"providerName\": \"string\", \"providerResponse\": \"string\", \"progress\": 0, \"timestamp\": \"string\", \"status\": \"string\", \"text\": \"string\"\n}\n``` ### cURL Sample ```bash\ncurl -X POST $BASE_URL/v1/assistant/chat \\ -H \"Content-Type: application/json\" \\ --data '{ \"assistant\": \"string\", \"messages\": [ { \"role\": \"string\", \"content\": \"string\" } ], \"variables\": [ {\"key\": \"string\", \"value\": \"string\"}, {\"key\": \"string\", \"value\": \"string\"} ], \"revision\": 0, \"revisionName\": \"string\" }'\n``` Note that the [variables](85.html) section is optional and depends on your Prompt configuration. ## GET/assistant/request/{id}/status Retrieves the status of a request. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `requestId` | `string (uuid)` | ID of the request. | ### Response ```json\n{ \"requestId\": \"string\", \"error\": { \"code\": 0, \"message\": \"string\" }, \"providerName\": \"string\", \"providerResponse\": \"string\", \"progress\": 0, \"timestamp\": \"string\", \"status\": \"string\"\n}\n``` ### cURL Sample ```bash\ncurl -X GET $BASE_URL/v1/assistant/request/{requestId}/status\n``` ## POST/assistant/request/{id}/cancel Cancels a request. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `requestId` | `string (uuid)` | ID of the request. | ### Response ```json",
          "evaluationResultRowContextDocId": "5abf498f-7704-4080-8cbf-bfa8950a0e7e",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "ef948b93-3ec1-4969-ac3f-f11111a141d4",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?35",
          "evaluationResultRowContextName": "35",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.3,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "218bb92b-629b-42c8-a733-5f94e733a6dd",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0039585600",
      "evaluationResultRowEndDate": "2025-05-13T11:12:39",
      "evaluationResultRowOutput": "No se encuentra informaci\u00f3n espec\u00edfica en la documentaci\u00f3n proporcionada sobre c\u00f3mo crear proyectos usando el API de Globant Enterprise AI. \n\nSin embargo, la secci\u00f3n sobre **API Tokens </B>menciona que para ciertas operaciones de mayor alcance, como la creaci\u00f3n, actualizaci\u00f3n y eliminaci\u00f3n de proyectos, se requieren **Organization API Tokens**, que tienen un scope m\u00e1s amplio que los Project API Tokens. Esto sugiere que la creaci\u00f3n de proyectos es una operaci\u00f3n que requiere estos tokens con privilegios de organizaci\u00f3n, pero no se detallan los endpoints ni el procedimiento para crear proyectos via API.\n\nTe recomiendo preguntar de otra forma o consultar la documentaci\u00f3n espec\u00edfica relacionada con la gesti\u00f3n de proyectos si estuviera disponible.\n\n[API Tokens - Organizaci\u00f3n y Proyectos](https://wiki.genexus.com/enterprise-ai/wiki?564)",
      "evaluationResultRowStartDate": "2025-05-13T11:12:34",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "9565"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# File Support in Assistants File Support in Assistants allows you to work with various file types, including documents, images, videos, and audio, to perform tasks like summarizing documents, extracting information, comparing files, and generating text based on file content. This opens up a world of possibilities for using assistants in new and innovative ways. ## Scenarios The end user should be able to interact with assistants using files in the following ways: - Summarize a document: Provide a concise summary of the document's content.\n- Extract specific information: Extract specific data points from a document, such as the amount, client name, and deadline from a contract.\n- Compare documents: Compare the information in two documents, identifying matching and differing clauses.\n- Generate text based on document content: Create new text based on the content of a document.\n- Ask questions based on uploaded documents: Similar to [RAG Assistants](44.html), but with the ability to upload new documents from the [Playground Interface](349.html) to add to the assistant's context. ## Functionality File Support in Assistants provides a robust set of features for managing and using files: - File Management: You can manage files associated with each project using CRUD operations (create, read, update, delete). A project-level API is available for file management, using the project identifier as the key. You can also configure a storage limit per project to manage storage space effectively.\n- File Referencing: You can reference files within assistant prompts using variables like `{file:filename}` or `{file:*}`. For example, you can use the prompt: \"Compare `{file:id1}` with `{file:id2}`\". The system seamlessly handles file uploads, context window size, and summarization, regardless of whether the model is multimodal or not.\n- File Types: File support extends beyond documents to include images and videos, using a similar syntax: `{file:image1}` or `{file:video1}`. For example, you can use the prompt: \"Extract the text from image `{file:id1}` and summarize its content\".\n- Frontend Upload: End users can easily select existing files or upload new ones when interacting with file-handling assistants. ## How to Use File Support in Assistants 1. Choose your assistant: Select the assistant that best suits your needs.\n2. Upload your file: Click on the \u201cUpload File\u201d button and select the file you want to use. The assistant will recognize the file type and process it accordingly.\n3. Write your prompt: Enter your instructions for the assistant in the prompt box. Be clear and specific about what you want the assistant to do.\n4. Use file references: To reference the uploaded file in your prompt, use the following syntax: - Documents: `{file:filename}` (for example, {file:contract}) - Images: `{file:image1}` - Videos: `{file:video1}` - Audio: `{file:audio1}`\n5. Submit your prompt: Click on the arrow icon (or press Enter on your keyboard) to send your prompt to the assistant.\n6. Review the results: The assistant will process your request and display the results. ## Samples ### Document: - Prompt: \u201cSummarize the key points of the document `{file:contract}`\u201d.\n- Response: \u201cThe contract outlines the terms of service for a software subscription, including pricing, payment terms, and support options\u201d. ### Image: - Prompt: \u201cDescribe the image `{file:product_image}`\u201d.\n- Response: \u201cThe image shows a red sports car with black leather interior\u201d. ### Video: - Prompt: \u201cSummarize the main points of the video `{file:tutorial}`\u201d.\n- Response: \u201cThe video demonstrates how to use the new software feature for creating custom reports\u201d. ### Audio: - Prompt: \u201cTranscribe the audio file `{file:meeting_recording}`\u201d.\n- Response: The assistant will provide a text transcript of the audio recording.",
          "evaluationResultRowContextDocId": "97680bd3-a82d-4d62-87ac-fba252965c5b",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "1511de44-9743-45d4-8b35-1005f504985f",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?346",
          "evaluationResultRowContextName": "346",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#How to upload and manage Files via API It is possible to work with previously uploaded files to perform various operations such as summaries or analysis. To perform these operations, first it is necessary to upload the file using the [[450]], confirm the success of the operation, and then reference it in the requests to the models that require the file. Here is a step-by-step guide on how to upload and manage files using the File API. ## Step 1: Uploading a File Using the File API\nThe first step is to upload the file you want to manage. To do this, use the [[450##POST /files|POST /files]] endpoint of the File API. This endpoint allows you to upload a file and receive a confirmation that the file has been successfully uploaded. The name of the file, which will be used to reference it, can be specified in the `fileName` parameter or, if not provided, it will be the name of the uploaded file. ### Getting your projectId and organizationId You can get your `$PROJECT_ID` and `$ORGANIZATION_ID` using the [[22##GET /accessControl/apitoken/validate|GET /accessControl/apitoken/validate]] endpoint of the [[22]]. Keep in mind that you can replace `$SAIA_ORGANIZATION_APITOKEN` with your `$SAIA_PROJECT_APITOKEN` if needed. ### Sample cURL Command\nTo load a file, use the following sample:\n```bash\ncurl -X POST \"$BASE_URL/v1/files\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: multipart/form-data\" \\ -H \"fileName: $FILE_NAME\" \\ -H \"folder: testFolder\" \\ -H \"organizationId: $ORGANIZATION_ID\" \\ -H \"projectId: $PROJECT_ID\" \\ -F \"file=@/path/to/your_file.mp4\" ``` If you include the `fileName` parameter, that will be the name you will use to reference the file later. If you don't include it, you will need to use the original name of the uploaded file. For example, if you upload `\"file=@/path/to/your_file.mp4\"`, the reference name will be exampleFile (without the extension). ## Step 2: Validating the File Upload\nOnce the file has been successfully uploaded, the response will include information confirming the success of the operation. You don't need to use a dataFileId, but you can check the response to ensure the file was uploaded correctly. ### Sample response\n```json\n{ \"dataFileId\": \"string\", \"dataFileUrl\": \"string\", \"success\": true\n}\n```\n## Step 3: Create a Chat Assistant (if not created already) After uploading the file, you need to create a [[708]] to process the uploaded content, if you haven't already created one. This is done through the [[35##POST/assistant|POST /assistant]] endpoint of the [[35]]. ### Sample cURL Command ```bash\ncurl -X POST \"$BASE_URL/v1/assistant\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"type\": \"chat\", \"name\": \"vertex_ai/gemini-2.0-flash-exp -Test File Upload\", \"prompt\": \"You are a helpful assistant\", \"llmSettings\": { \"providerName\": \"vertex_ai\", \"modelName\": \"gemini-2.0-flash-exp\", \"temperature\": 0.10, \"maxTokens\": 8192, \"uploadFiles\": true } }'\n``` ## Step 4: Referencing the File Once the Chat Assistant is set up, you can reference the file in a request to the Assistant for analysis or summary, using the [[34]] and specifying the file name in the request body. ### Sample cURL Command to Summarize a File: ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"saia:assistant:vertex_ai/gemini-2.0-flash-exp -Test File Upload\", /* Using the assistant created in Step 3 named 'vertex_ai/gemini-2.0-flash-exp -Test File Upload' */ \"messages\": ~~[~~ { \"role\": \"user\", \"content\": \"Provide the content {file:$FILE_NAME}\" } ~~]~~, \"stream\": false }'\n``` ## Alternative: Inquire About Files - Interact Directly with LLMs You can also interact with LLMs directly, asking questions about files like images or videos. [regions]\n[region|cURL]\nSample 1: Ask GPT-4o About an Image\n```bash\ncurl --location '&lt;BASE_URL&gt;/chat' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;SAIA_PROJECT_APITOKEN&gt;' \\\n--data '{ \"model\": \"openai/gpt-4o\", \"max_tokens\": 1024, \"temperature\": 0.1, \"messages\": ~~[~~ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": ~~[~~ { \"text\": \"Tell me what this is about?\", \"type\": \"text\" }, { \"image_url\": { \"url\": \"&lt;IMAGE_FILE_URL&gt;\" }, \"type\": \"image_url\" } ~~]~~ } ~~]~~\n}'\n```\n[/region]\n[region|Python]\nSample 2: Python Code - Sending Gemini a Video File for Analysis\n```python\nimport requests\nimport json\nimport base64 # Define the API endpoint and authentication headers\nurl = \"&lt;BASE_URL&gt;/chat/completions\"\nheaders = { 'Content-Type': 'application/json', 'Authorization': 'Bearer &lt;SAIA_PROJECT_APITOKEN&gt;'\n} # Function to encode a file in Base64 format\ndef encode_file(file_path): \"\"\"Encodes a file in Base64 format for API submission.\"\"\" with open(file_path, 'rb') as file: return base64.b64encode(file.read()).decode('utf-8') # Replace 'path_to_your_file.mp4' with the actual file path\nfile_path = \"path_to_your_file.mp4\"\nencoded_file = encode_file(file_path) # Construct the API request payload\npayload = { \"model\": \"vertex_ai/gemini-1.5-pro-002\", \"messages\": ~~[~~ { \"role\": \"user\", \"content\": ~~[~~ { \"type\": \"text\", \"text\": \"What's this file about?\" }, { \"type\": \"image_url\", \"image_url\": f\"data:video/mp4;base64,{encoded_file}\" } ~~]~~ } ~~]~~\n} # Send the request to the API\nresponse = requests.post(url, headers=headers, json=payload) # Print response details\nprint(\"Status Code:\", response.status_code)\nprint(\"\\nResponse:\")\nprint(json.dumps(response.json(), indent=2))\n```\n[/region]\n[/regions]\n{{{12}}}{{{17}}} ##MARKDOWN##",
          "evaluationResultRowContextDocId": "94fd1b65-0ca6-4b3c-8acf-eed9148e7023",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "4ca88c79-a52e-4f0e-bb43-cc466e7c10f2",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?454",
          "evaluationResultRowContextName": "454",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#File API This API provides endpoints to manage files. It allows you to upload, get, list, delete, and get the content of files. Check the [[20##Generic Variables|Globant Enterprise AI API Reference]] for generic variables needed to use the API. ## Endpoints | Method | Path | Description |\n|---|---|---|\n| `POST` | ` /files` | Uploads a file |\n| `GET` | `/files/{fileId}` | Gets a file by Id |\n| `DELETE` | `/files/{fileId}` | Deletes a file by Id |\n| `GET` | `/files/{fileId}/content` | Gets the content of a file by Id |\n| `GET` | `/files/all` | Gets all files | </br>\n**Note:** The `fileId` parameter refers to the file Id. ## POST /files Uploads a file to the system. ### Parameters **Request Body:** | Name | Type | Description |\n|---|---|---|\n| `file` | `File` | The file you want to upload. | </br>\n**Headers:** | Name | Type | Description |\n|---|---|---|\n| `Authorization` | `string` | `BearerToken <project_api_token>` for authentication. |\n| `fileName` | `string` | File name (optional). If not provided, the name of the uploaded file will be used. |\n| `organizationId` | `string` | Organization ID. |\n| `projectId` | `string` | Project ID. |\n| `folder` | `string` | Destination folder (optional). If not provided, the file will be temporarily saved. | ### Response ```json\n{ \"dataFileId\": \"string\", \"dataFileUrl\": \"string\", \"success\": true\n}\n``` ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/v1/files\" -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" -H \"Accept: application/json\" -H \"fileName: $FILE_NAME\" -H \"organizationId: $ORGANIZATION_ID\" -H \"projectId: $PROJECT_ID\" -H \"folder: $FOLDER\" -F \"file=@/path/to/file.txt\" ``` ## GET /files/{fileId} Gets a file by Id. This endpoint requires a Globant Enterprise AI API token related to organization scope. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `organization` | `string` | Organization Id. |\n| `project` | `string` | Project Id. |\n| `fileId` | `string` | File Id. | ### Response ```json\n{ \"dataFileExtension\": \"string\", \"dataFileId\": \"string\", \"dataFileName\": \"string\", \"dataFilePurpose\": \"string\" \"dataFileSize\": numeric, \"dataFileUrl\": \"string\", \"organizationId\": \"string\", \"projectId\": \"string\", \"success\": true\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/files/{fileId}\" -H \"Authorization: Bearer $SAIA_APITOKEN\" -H \"Accept: application/json\" -G -d \"organization=$ORGANIZATION_ID\" -d \"project=$PROJECT_ID\" ``` ## DELETE /files/{fileId} Deletes a file by Id. This endpoint requires a Globant Enterprise AI API token related to organization scope. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `fileId` | `string` | File Id. | ### Response ```json\n{ \"success\": true\n}\n``` ### cURL Sample ```bash\ncurl -X DELETE \"$BASE_URL/v1/files/{fileId}\" -H \"Authorization: Bearer $SAIA_APITOKEN\" -H \"Accept: application/json\" -G -d \"organization=$ORGANIZATION_ID\" -d \"project=$PROJECT_ID\" ``` ## GET /files/{fileId}/content Gets the content of a file by Id. This endpoint requires a Globant Enterprise AI API token related to organization scope. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `organization` | `string` | Organization Id. |\n| `project` | `string` | Project Id. |\n| `fileId` | `string` | File Id. | ### Response ```json\n{ \"content\": \"string\" }\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/files/{fileId}/content\" -H \"Authorization: Bearer $SAIA_APITOKEN\" -H \"Accept: application/json\" -G -d \"organization=$ORGANIZATION_ID\" -d \"project=$PROJECT_ID\" ``` ## GET /files/all Gets all files. This endpoint requires a Globant Enterprise AI API token related to organization scope. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `organization` | `string` | Organization Id. |\n| `project` | `string` | Project Id. | ### Response ```json\n{ \"dataFiles\": [ { \"DataFileExtension\": \"string\", \"DataFileId\": \"string\", \"DataFileName\": \"string\", \"DataFilePurpose\": \"string\", \"DataFileSize\": \"number\", \"DataFileUrl\": \"string\" }, { \"DataFileExtension\": \"string\", \"DataFileId\": \"string\", \"DataFileName\": \"string\", \"DataFilePurpose\": \"string\", \"DataFileSize\": \"number\", \"DataFileUrl\": \"string\" } ]\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/files/all\" -H \"Authorization: Bearer $SAIA_APITOKEN\" -H \"Accept: application/json\" -G -d \"organization=$ORGANIZATION_ID\" -d \"project=$PROJECT_ID\" ``` ## See Also [[454|Managing Files]] {{{12}}}{{{17}}} ##MARKDOWN##",
          "evaluationResultRowContextDocId": "976e4c73-c22d-4320-a1d8-ff7dd96d0200",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "53bf13db-cacd-4a76-a8a2-2bf6a758755f",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?450",
          "evaluationResultRowContextName": "450",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#scanned documents processing When handling non-machine-readable documents, such as scanned PDFs, extracting all content accurately can be challenging. Globant Enterprise AI leverages the power of [[406|Vision Large Models (VLMs)]] to extract complete information from scanned documents, even when their quality is suboptimal. This methodology represents the state-of-the-art (SOTA) in scanned document processing, as traditional OCR techniques tend to be highly inaccurate in low-quality scenarios. [[813]] As we can see in this example, the response provided is identical to the content in the original document. [[814]] {{{12}}} {{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "f8007b90-6fd5-4884-bf63-d81549b3f75d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "54f05410-8ba7-4694-96d5-0302460fbe0c",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?815",
          "evaluationResultRowContextName": "815",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# File support for multimodal models Multimodal models allow interactions that go beyond traditional text-based prompts. These models can understand and process information from a variety of sources, including text, file documents, images, audio, and video. This means they can analyze, process, and generate content based on a richer and more complete set of inputs. Globant Enterprise AI implements multimodal models, allowing you to interact with models such as Gemini and OpenAI in a completely new way. By combining text with files, a world of possibilities opens up for analyzing, processing, and generating content. These models can directly receive files alongside your prompts, eliminating the need to manually extract and inject file content. This direct file integration offers several advantages. - Simplifies file handling. You no longer need to extract and inject file content into the prompt.\n- Files are sent directly to the model, streamlining the process and enabling direct model interaction.\n- Empowers you to interact directly with files within your prompts, making it easier to analyze, process, and generate content based on their contents. This eliminates the need for manual extraction and injection, simplifying your workflow.\n- Leverages the information contained in files to provide more comprehensive and insightful responses, enhancing their capabilities. ## File References in Prompts You can reference files in your prompts using a specific format: `{file:filename}`. For example, to reference a file named \"report.pdf\" you would write: `{file:report}`. > **Note**: This reference format only works for files that have already been uploaded to your project. If you're uploading a file for the first time, you'll need to use the standard file upload mechanism within the platform. ## How Files are Managed in Your Project 1. **File Storage**: All files uploaded to your project are stored securely and can be accessed by your prompts.\n2. **File Access**: When you reference a file in your prompt, the system automatically retrieves it and sends it to the multimodal model.\n3. **File Types**: The system supports a wide range of file types, including text documents, images, audio files, and more. ## Sample Suppose you have a document called \"product_specs.txt\" loaded in your project. You can use it in your prompt as follows: ```plaintext\n\"Please summarize the key features of the product described in {file:product_specs}\".\n``` The multimodal model will then access the \"product_specs.txt\" file and use its contents to generate a summary of the product features. ## See Also [Supported models and file formats](406.html)",
          "evaluationResultRowContextDocId": "381b4a5d-cb99-40b8-a58e-e67f82082418",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "9a6e4235-997d-44a4-bd13-4018a715aae3",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?345",
          "evaluationResultRowContextName": "345",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "--form 'dialogue=\"true\"'\n``` * Video without audio, use a vLLM every 10 seconds for interpretation, merging 20 results at a time:\n```bash curl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--form 'file=@\"/C:/temp/SampleFile.mp4\"' \\\n--form 'provider=\"geai\"' \\\n--form 'model=\"openai/gpt-4.1-nano\"' \\\n--form 'mediaPrompt=\"Describe this video frame as succint as possible\"' \\\n--form 'dialogue=\"false\"' \\\n--form 'extractedFramesPerSecond=\"10\"' \\\n--form 'merge=\"20\"'\n``` * Audio file:\n```bash\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--form 'file=@\"/C:/temp/SampleFile.mp3\"' \\\n--form 'provider=\"geai\"'\n``` {{{12}}}{{{17}}}##MARKDOWN##\n\n#geai Ingestion Provider The `geai` ingestion option supports a wide range of file types for intelligent content extraction and transformation, including PDFs, images, and multimedia formats. Depending on the type of document, different strategies are applied to maximize the retrieval of useful information, including text, tables, images, and audio transcripts. For `PDF files` they can be processes differently based on their content: * Scanned PDF where each page is treated as an image and will use by default the `hi_res` strategy and the `scannedPrompt` parameter. Therefore, for each page an LLM call will be executed to interpret the image and get as much text as possible.\n* Standard PDF containing text, images and tables will take into account the selected strategy parameter with a combination of `imagePrompt`/`tablePrompt` parameters. When treating media files (including audio and video from the supported [[137|file formats]]) it applies automated transcription or image frame processing based on configuration. ## Parameters You can customize the `geai` processing with these parameters: |**Parameter**|**Description**|\n|---|---|\n|`strategy`|Determines the processing approach. Options:<br>-`auto` (default): Globant Enterprise AI selects the best option based on the document.<br>- `hi_res`: High-resolution processing. Requires the model parameter. More expensive but potentially yields better results for complex documents, especially those PDF documents that have images and tables. check an example [[809|here]].|.|\n|`model`|Specifies the AI model for image processing when OCR is not feasible. Default: `openai/gpt-4o`. Use the format `provider/modelname`. You must use models with [[200|visual support]]. Some examples are: `openai/gpt-4o`, `openai/gpt-4o-mini`, `anthropic/claude-3-5-sonnet-20240620`, `vertex_ai/gemini-1.5-flash` etc.|\n|`imagePrompt`|Custom prompt for image interpretation and text generation. If not provided, a default prompt is used.|\n|`scannedPrompt`|Custom prompt for scanned documents where the whole page is an image.|\n|`tablePrompt`|Custom prompt for table interpretation and text generation. If not provided, a default prompt is used.|\n|`logoProcess`|Determines whether the visual model will process the logos within the document or not. Options: <br>-`False` (default): Does not process the logos. <br>-`True`: To process the logos means to extract the explanation of each logo.|\n|`dpi`| Defines the DPI (Dots Per Inch) used when processing images. Default: 200 DPI.|\n|`structure`|Specifies whether the document is assumed to have a table structure. Valid values are: <br>- `(empty)`: Default value. Assumes no table structure. <br>- `table` : Assumes the document is in a table or tabular format, applicable only to `csv` and `xls*` formats; check an example [[799|here]].|\n|`startPage`| First page number to begin processing from. Default is 1.|\n|`endPage`| Last page number to process. If set to 0, processing continues until the end of the document.|\n|`dialogue`| Boolean flag indicating if the video contains spoken dialogue for transcription.|\n|`mediaPrompt`| Prompt used to describe or summarize visual frames in videos.|\n|`frameSamplingRate`| Time interval in seconds between each extracted frame from the video. For example, a value of 2 captures one frame every 2 seconds, defaults to 5.|\n|`merge`| Number of consecutive transcript lines to merge. Set to 0 to merge all lines.|\n|`whisperModel`| Specifies the Whisper model variant to use for transcription (small, medium, large). Defaults to small.| ## Default imagePrompt The `imagePrompt` default is as follows: ```\nYou are an assistant tasked with extracting all text from images in the image text language.\nThese images are pages from a PDF document. Extract and transcribe all visible text in the image,\nmaintaining the structure and layout as much as possible. Include any headers, footers, and page numbers.\nBe thorough and don't miss any text, no matter how small or where it's positioned in the image.\n``` ## Default tablePrompt The `tablePrompt` default is as follows: ```\nYou are an assistant tasked with extracting all text from images in the image text language. These images are pages from a PDF document. Extract and transcribe all visible text in the image, maintaining the structure and layout as much as possible. Include any headers, footers, and page numbers.\nBe thorough and don't miss any text, no matter how small or where it's positioned in the image.\n``` ## Default scannedPrompt The `scannedPrompt` default is as follows: ```\nYou are an assistant tasked with extracting all text from images in the image text language. These images are pages from a PDF document. Extract and transcribe all visible text in the image, maintaining the structure and layout as much as possible. Include any headers, footers, and page numbers.\nBe thorough and don't miss any text, no matter how small or where it's positioned in the image.\n``` ## Default mediaPrompt The `mediaPrompt` default is as follows: ```\nGenerate a descriptive caption of this video frame optimized for search and retrieval.\nUse keywords and phrases that capture the visual content, scene type, objects, people, and potential actions or events depicted.\n``` ## Samples ### PDF The following shows how to use [[34]] for different ingestion provider options: * Minimal options:\n```bash\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: multipart/form-data\" \\ --form 'file=@\"/C:/temp/SampleFile.pdf\"' \\ --form 'provider=\"geai\"'\n``` * High-resolution strategy:\n```bash curl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: multipart/form-data\" \\ --form 'file=@\"/C:/temp/SampleFile.pdf\"' \\ --form 'provider=\"geai\"' \\ --form 'strategy=\"hi_res\"' ``` * High-resolution with custom model and prompt:\n```bash\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: multipart/form-data\" \\ --form 'file=@\"/C:/temp/SampleFile.pdf\"' \\ --form 'provider=\"geai\"' \\ --form 'model=\"openai/gpt-4o\"' \\ --form 'imagePrompt=\"Resume the image as succinctly as possible in markdown format\"'\n``` * Using `startPage` and `endPage` to only process a range of pages from a document\n```bash\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"Content-Type: multipart/form-data\" \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"' \\\n--form 'provider=\"geai\"' \\\n--form 'model=\"openai/gpt-4o-mini\"' \\\n--form 'startPage=\"2\"' \\\n--form 'endPage=\"2\"'\n``` ### Media * Video file with available spoken audio:\n```bash\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--form 'file=@\"/C:/temp/SampleFile.mp4\"' \\\n--form 'provider=\"geai\"' \\\n--form 'model=\"openai/gpt-4.1-nano\"' \\",
          "evaluationResultRowContextDocId": "9950e1d0-633e-4034-9869-77f2d050dfda",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "fc8673c2-1857-4c3b-b2b4-ec304c8d9cbd",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?1142",
          "evaluationResultRowContextName": "1142",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataSetRowId": "2668f6d0-5172-451e-b7e8-ffab9540d2e2",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0030588400",
      "evaluationResultRowEndDate": "2025-05-13T11:12:44",
      "evaluationResultRowOutput": "You can retrieve the content of a file by using the File API endpoint:\n\n```\nGET /files/{fileId}/content\n```\n\n### How it works:\n- You need to provide the `fileId` of the file whose content you want to retrieve.\n- The request requires authentication with a Globant Enterprise AI API token scoped for the related organization.\n- Additional parameters to specify are the `organization` ID and the `project` ID.\n\n### Sample response:\n```json\n{\n  \"content\": \"string\"\n}\n```\n\n### Example cURL command:\n```bash\ncurl -X GET \"$BASE_URL/v1/files/{fileId}/content\" \\\n-H \"Authorization: Bearer $SAIA_APITOKEN\" \\\n-H \"Accept: application/json\" \\\n-G -d \"organization=$ORGANIZATION_ID\" \\\n-d \"project=$PROJECT_ID\"\n```\n\nThis will return the content of the file as a string.\n\nFor complete details, see the [File API documentation](https://wiki.genexus.com/enterprise-ai/wiki?450) [4].",
      "evaluationResultRowStartDate": "2025-05-13T11:12:39",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "7131"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "- New Average Request Time metric added in the Project Dashboard. ### January 8th - The option formerly known as 'Search Documents' has been improved and renamed to RAG Assistant (Retrieval Augmented Generation) to provide an optimized experience when searching and generating information. #### Frontend - Feedback is provided during conversations with RAG Assistants, indicating where you are in the process.\n- 'Response streaming' support for RAG Assistants.\n- Settings are hidden when selecting an assistant, except when 'Chat with LLMs' is selected. ## 2023 ### December 19th - Fixed: Too Many Redirects when accessing Playground using a browser in Spanish language. ### December 6th - New backoffice design.\n- Access to the Playground from the backoffice to chat with the assistants defined in the project.\n- Upload images for analysis with GPT-4 Vision.\n- Google Analytics support at the frontend.\n- Keep a conversation thread when chatting with documents.\n- An email notification is sent automatically when a new member is invited to join the organization or project. ### November 28th - First version officially released!! ### November 6th - The following OpenAI models are supported: GTP-4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), and GPT-4 Vision (gpt-4-vision-preview). ### October 18th - [Manage quotas per project](71.html): It is possible to set quota limits to control project expenses or usage. ### October 11th - [AI-Driven Load Balancing](32.html): The platform automatically manages the Load Balancing process when you work with generative AI providers, efficiently addressing the limits imposed by LLM platforms.\n\n- In the [Organization API](22.html), the ability to set and manage usage limits on projects through the [POST /project](22.html) and [GET /project/{id}](22.html) endpoints has been added.\n- [Quota Limit](71.html) now includes improvements such as highlighting the active quota in green, offering options to cancel active quotas, among others. ### October 17th - Rebranding to Globant Enteprise AI\n- Improvements in RAG - New option not to [compact the conversation history](297.html) - Support for Redis, MongoDB Atlas and PG Vector [vectorstores](267.html) - New endpoint to get an embedding. Check details here: [Embeddings API](422.html)\n- Playground improvements\n- File management improvements\n- New [LLMs supported](200.html) - NVIDIA provider with new models supported - nvidia.nemotron-mini-4b-instruct - meta.llama-3.1-8b-instruct - meta.llama-3.1-70b-instruct - meta.llama-3.1-405b-instruct - meta.llama-3.2-3b-instruct - Groq provider supported - groq/llama-3.1-70b-versatile - groq/llama-3.2-11b-vision-preview - groq/llama-3.2-3b-previewgroq/llama-3.2-1b-preview\n- New [embeddings models](447.html) added - Vertex AI: - vertex_ai/textembedding-gecko - vertex_ai/text-embedding-004 - vertex_ai/textembedding-gecko-multilingual - Nvidia: - nvidia/nvclip - nvidia/nv-embed-v1 - nvidia/baai.bge-m3 - nvidia/snowflake.arctic-embed-l - nvidia/nv-embedqa-mistral-7b-v2 - nvidia/embed-qa-4 - nvidia/nv-embedqa-e5-v5 ### September 25th - [Support for file](346.html) processing with prompt-based assistants. This will enable many scenarios, such as uploading documents and summarizing, extracting, and checking information, etc. Also, depending on the model used by the assistant, it will be able to process audio, video, or images.\n- [Support for multi-modal](345.html) LLMs allow processing docs, audio, video, and images in models like GPT-4o or Gemini Pro.\n- Chat with data assistants - The model used to build the queries was updated with GPT-4o, which improves the quality of the generated query. - Configure the query builder server by organization and project. This means you can connect with different DBMS from each project when building Chat with data assistants. - Show an explanation of how the query was built.\n- New [Playground Interface](349.html) design - New design - Upload documents from the front end to chat with them.\n- [Flows builder](321.html) - There will be two types of Flows, one more oriented to build a conversational UI and the other to build assistant flows. Access to these flows will only be available through [Chat API](34.html) or through the [channels](384.html) offered by Flows.\n- New models hosted in AWS Bedrock added: - Amazon Titan Express v1 - Amazon Titan Lite v1 - Anthropic Claude 3 Haiku - Anthropic Claude 3 Sonnet - Anthropic Claude 3.5 Sonnet - Cohere Command - Meta Llama 3 8B - Meta LLama 3 70B\n- It is now possible to provide clear guidance on the assistant's capabilities, allowing you to add information such as descriptions, features, and example prompts. This configuration can be done from the Backoffice, [Start Page](408.html), or WelcomeData section of the [Assistant API](35.html) and [RAG Assistants API](29.html) endpoints.\n- RAG Assistants - New endpoint in [RAG Assistants API](29.html): using [DELETE/profile/{name}/documents](29.html), all documents associated with the specified RAG Assistant are deleted. ### August 9th - Support of new models - GPT-4o mini\n- RAG Assistants - New option called [CLEANUP](60.html) allows to delete the documents associated to a RAG Assistant. - When creating a new assistant, the following defaults are updated: - [Chunk Count](69.html) updated from 2 to 5. - [Model Name](68.html) from gpt-3.5-turbo to gpt-4o-mini.\n- Data Analyst Assistant - New [Conversational Module](291.html) for Data Analyst Assistant.\n- Considerations - **Important**: [max_tokens integer below minimum value](290.html) error may occur if you are using the _max_tokens_ value as -1; those Assistants must be updated to use a correct positive value. ### July 4th - Permissions by assistants in role-based access control. A new [role and permissions management system](242.html) has been implemented to provide greater flexibility and control. Please read [Migration to the new Roles and Permissions Management System](264.html).\n- Dashboard performance optimizations. Please read [Using the Request Summary Table for Improved Dashboard Performance](273.html).\n- RAG Improvements - New option to customize the [context](205.html). - Support for [Max Marginal Relevance](206.html) - Support for _similarity_hybrid_ and _semantic_hybrid_ for [Azure AISearch](207.html) - Advanced options for SelfQuery retriever prompt case. See [Self Query Use Case](150.html) - Support for Parent Document retrieval option. See [Parent Document Retrieval Strategy](204.html) ### June 10th - [Enterprise AI Proxy](19.html) is deprecated. Use [Chat API](34.html) instead.\n- Support for new LLMs - OpenAI new model GPT-4o - Models in Google Vertex - Gemini 1.0 Pro - Gemini 1.5 Flash preview-0514 - Gemini 1.5 Pro preview-0514 - Claude 3 Haiku - Claude 3 Opus - Claude 3 Sonnet\n- RAG Improvements - New option to initialize RAG Assistant based on another when creating a new RAG Assistant. - New option to export document list in View Documents over a RAG Assistant. - Added filter options when browsing Documents. - SelfQuery RAG retriever partial support for a [customized Prompt](150.html). - Support for text-embedding-004 in Google models to generate the embeddings.\n- Deprecated [Assistant API](35.html) endpoints. - /assistant/text/begin - /assistant/text\n- Support to deploy in Google Cloud Platform. ### May 8th - New [Chat with Data Assistant](159.html).\n- New [Ingestion SDK](172.html) to automate document ingestion in RAG assistants.\n- New models hosted in NVIDIA platform supported. See [Supported Chat Models](200.html) for more details.\n- New option to export information about projects and members available for the organization administrator.\n- New API to extend dataset for [Data Analyst Assistant 1.0](89.html).\n- New filter by user email in Requests.\n- Update default to use _text-embedding-3-small_ OpenAI Embeddings for new RAG assistants.\n- Support for gemini-1.5-pro-preview-0409 model added. ### April 3rd - New [API Assistant](110.html).\n- New [Data Analyst Assistant 1.0](89.html).\n- The user Dashboard displays the email to identify the most active user or the one who generates the most costs. ### March 11th - GeneXus Identity Provider is implemented, expanding the login options in the [Backoffice](42.html) of the production environment. This allows for login not only with Google but also with Apple or GeneXus Account.\n- It is possible to customize the icon for each assistant. ### February 29th - [Frontend](82.html) improvements in UI/UX.\n- Option to get feedback from end users when interacting with [RAG Assistant](44.html).\n- Gemini Pro LLM support.\n- New [Dashboard](562.html) with user metrics.\n\n# Updates Log This log shows the most important fixes or features added to the platform. ## Log ## 2025 ### April 10th - New [Globant Enterprise AI Lab](972.html) - The Lab is designed for defining, managing, and orchestrating autonomous AI agents. It provides a standardized model for representing agents, their capabilities, and their interactions within complex workflows. The core components of the Lab include: - Agents & Tools: This module allows for the definition and management of individual agents and their resources, such as skills and tools. It serves as a central hub for cataloging and managing the agent workforce. - Agentic Processes: This component enables the definition of processes based on tasks executed by Agents. These workflows facilitate collaboration among agents to achieve larger objectives. More details at [How to create an Agentic Process](985.html). - Agent RunTime: This module provides the execution environment for agentic workflows, where agents perform tasks based on their skills and interact with artifacts, driven by events and the flow of knowledge. - The Lab aims to meet the growing demand for intelligent, self-sufficient AI agents capable of collaborating and solving complex problems with minimal human intervention. It offers a flexible and adaptable model, allowing for the creation and management of a diverse range of agents, from co-pilots working alongside humans to fully automated agents executing complex tasks. Implemented as a module of Globant Enterprise AI, the Lab supports the development of intelligent agents that can work autonomously or in collaboration with humans and other agents.\n- New features in Flows - Agent Integration Component: You can now directly integrate agents created with the AI Lab into a Flow. These agents can be exposed through platforms like WhatsApp, Teams, or Slack. - [File Upload Support](1053.html) from WhatsApp: Flows now support receiving file attachments such as documents, images, audio, and video directly from WhatsApp interactions. - Audio and Video Attachment in Web Chat: The Web Chat component now allows users to attach audio and video files, enhancing the interaction experience.\n- New Features for the Data Analyst Agent - Reduced Configuration Requirements: The setup needed to enable the assistant to respond to a wide range of questions has been minimized. - Enhanced Analysis Module: An additional analysis module has been incorporated to complement the responses with relevant business conclusions and interpretations of the obtained data.\n- New metrics to track processed tokens. - A new view has been added to each project dashboard to monitor token-based consumption. More details at [Tokens tab in Dashboard](965.html). - To enable this feature, you must run the initialization process described in [Reprocessing Requests and Loading the Summary Table](1015.html).\n- LLMs: - New Gemini 2.5 Pro (via providers Vertex AI and Gemini): Gemini 2.5 is Google\u2019s latest reasoning model, engineered to tackle increasingly complex challenges. This model is designed for tasks that demand advanced analytical thinking and robust problem-solving capabilities. More details at [LLM API](663.html). - Migration to Gemini 2.0 series (Vertex AI): Based on recommendations from Vertex AI, we have migrated from the legacy Gemini 1.0 and 1.5 models to the more advanced Gemini 2.0 series, offering improved performance, scalability, and integration capabilities. For comprehensive information, please refer to the [Deprecated Models](Deprecated Models). - New Azure OpenAI models (o1, o1-mini and o3-mini): We have expanded our model availability by introducing these models via Azure, providing the same high-quality capabilities as those offered through the OpenAI provider. - DeepSeek-R1 via AWS Bedrock: Recently added through a Serverless API, DeepSeek-R1 offers reliable inference with a substantial 128K token context window and up to 32K maximum output tokens. - OpenAI's new models with built-in web search tool: These specialized models integrate web search capabilities directly into the Chat Completions API, enabling them to both interpret and execute search queries in real time. - openai/gpt-4o-search-preview - openai/gpt-4o-mini-search-preview - New models - Beta only: - gemini/gemma-3-27b-it: Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 has a large, 128K context window, and multilingual support in over 140 languages. - SambaNova: - DeepSeek-R1: This Provider offers the fastest performance for running DeepSeek, processing up to 198 tokens per second per user, with a 16K token context window. The model is hosted in US data centers; privately and securely. - DeepSeek-V3-0324: This model significantly outperforms its predecessor with enhanced reasoning benchmarks, improved code executability and refined web aesthetics, and superior Chinese writing aligned with the R1 style. It also offers better multi-turn interactive rewriting, translation quality, detailed report analysis, and more accurate function calling. - QwQ-32B: SambaNova provides access to QwQ-32B-Preview, the best open source test-time compute model released by Alibaba. - Llama 4 collection by Meta: - Llama 4 Scout: A 17B-parameter multimodal MoE model with 16 experts that excels in text and image understanding. The model is currently in beta and is available via providers Nvidia, Groq and SambaNova. Via Groq, it supports a 128k tokens context window with fast inference at 460 tokens/sec, while SambaNova Cloud runs at 697 tokens/second/user. - Llama 4 Maverick: Available via Nvidia, this 17-billion-parameter model featuring 128 experts supports a 32k-token context window. ### March 10th - New LLMs: - GPT-4.5 - Claude 3.7 Sonnet (Providers Anthropic, Vertex AI and AWS Bedrock) - Updates in Gemini 2.0 series: - vertex_ai/gemini-2.0-flash-lite-preview-02-05 - vertex_ai/gemini-2.0-flash-thinking-exp-01-21\n- RAG Revision #6 - Support for o3-mini, gpt-4.5-preview, claude-3-7-sonnet-20250219, new DeepSeek, Gemini2* and sambanova LLM providers. - New pinecone provider available for [embeddings](447.html) and [rerankers](638.html). - The [CleanUp](60.html) action message has been corrected to clearly specify that it will permanently delete the RAG Assistant files and update the information in the RDS. - Added [usage](712.html) element on every response. - Improvements when changing the [LLM](69.html)/[Embeddings](68.html) settings; all models and providers are normalized to be selected from standard combo-box items; use the [override mechanism](209.html) if you need other options. - Support for [guardrails](840.html). - New [documentAggregation](298.html) property to decide how sources are grouped and returned.\n- It is possible to [provide feedback](170.html) on the response of the Chat with Data Assistant in the Frontend.",
          "evaluationResultRowContextDocId": "68ace6a6-8b1b-4736-a466-9ed0a67b97a9",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "0497f2f4-e818-4de1-95bf-a9fc288fe50c",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?31",
          "evaluationResultRowContextName": "31",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Migration to the new Roles and Permissions Management System This document describes how to migrate to the new [Roles and Permissions Management System](242.html). This is a one-time process that transfers data from existing users and projects to the new role and permission structures. ### [Prerequisites](#Prerequisites) Before you begin the manual migration process, make sure that the latest version of the \"Security Configuration\" module is installed. This module is typically installed/updated automatically during the initial deployment of the environment (the first access to the console login). To verify it, navigate to SUPERADMIN OPTIONS > System > System Modules and make sure that the \"Security Configuration\" module does not show any available updates. ## [Manually Migrating to the New System](#Manually+Migrating+to+the+New+System) If the migration process is not automated, you can manually migrate to the new system by following these steps: 1. Navigate to SUPERADMIN OPTIONS > System > System Modules. ![Image](./images/265.png)\n2. Find the \"System\" module and click on **Install/Update**.\n3. The module will begin the migration process in the background. While the module will indicate that it has been updated, you need to check System > Operation Logs for a summary of the migration. This summary will indicate whether the migration was successful. ![Image](./images/266.png)\n4. Review the Migration Log: ![Image](./images/268.png) - **Start of Process:** When the migration process begins, a log entry will appear indicating this. For example, in the \"Data\" column of the log, you will see a message similar to \"The migration process is about to start...\". - **End of Process:** When the process is complete, another log entry will appear. Verify that the beginning of the \"Data\" description indicates that the execution was successful. For example, the message could be \"OVERALL EXECUTION: SUCCESS = true\". - **Viewing Details:** You can view the details of the log entry to see all the steps that were executed during the migration. The log will detail the steps and results, such as \"Populating OrganizationRole table...\".\n5. If the migration process fails, the \"Log level\" will be set to \"ERROR\". The details of the log will indicate where the process failed. In the event of a failure, no changes will be committed to the database, and the system will continue to function normally without the migrated schema. You will need to analyze the log to determine the cause of the failure.\n6. If the process is completed successfully, no further action is required. ### [Using the Old Role-Permissions Structures](#Using+the+Old+Role-Permissions+Structures) If you need to revert to the old role-permission structures after successfully migrating, you can set the following system parameter: 1. Navigate to SUPERADMIN OPTIONS > System > Parameters. ![Image](./images/269.png)\n2. Configure the following parameter: **Name:** USE_OLD_ROLES_SCHEMA **Description:** Enables the active use of the old Role-Permissions structures **Type:** String **Value S:** 1 ## [Migrated Projects](#Migrated+Projects) When migrating a GeneXus Enterprise AI environment to use the new Roles and Permissions Management System, the behavior for all existing projects is preserved. However, this is not optional for new projects. ### [Activating the New Scheme](#Activating+the+New+Scheme) To activate the new scheme in a previously defined project, follow these steps: 1. **Organization Member Role:** Make sure you have the \"Organization Member\" role.\n2. **Project Options:** Navigate to the \"Project\" section under \"ORGANIZATION OPTIONS\". ![Image](./images/243.png) As seen in the image, the projects MyNew and Default have a red exclamation mark (!). This red warning symbol indicates that these projects have not been migrated to the new roles and permissions scheme.\n3. **Update:** Click on \"UPDATE\".\n4. **Check User Permissions:** Select the checkbox labeled \"Check user permissions by API?\". ![Image](./images/244.png) **Notes:** - Existing projects do not have a \"Project Administrator\" role. An \"Organization Member\" must assign this role to Backend users who need to manage project members and roles.\n- Once checked and confirmed, this cannot be reversed for that project.",
          "evaluationResultRowContextDocId": "107ebd9d-01d0-47c5-af7e-1873ab45709c",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "122d5389-bffb-4dbc-9034-393da4cfe8d7",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?264",
          "evaluationResultRowContextName": "264",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Managing Roles and Permissions in Globant Enterprise AI The management of roles and permissions is a fundamental part of security management. It allows defining which actions and functionalities users can perform according to their role. Instead of assigning permissions directly to users, permissions are managed through roles. These roles, assigned to users, control their access to actions in the [Backoffice](42.html) and also determine access to specific assistants from the [Frontend](82.html). When creating a project, three default [roles](245.html) are generated that allow different levels of access and functions: - #### [Project Member:](#Project+Member%3A) This role grants access to the Backoffice for working with the project, including creating assistants, viewing requests, viewing and creating API tokens, and more. - #### [Project Administrator:](#Project+Administrator%3A) This role includes the \"Project Member\" role; in addition, it allows working with the list of members and defining roles within the project. - #### [Access all Project Assistants:](#Access+all+Project+Assistants%3A) This role grants Frontend access to all active project assistants. Each project user is assigned one or more roles, and the management of users and permissions is carried out in the [Members section](253.html). Here you can view the list of users with access to the project, modify the assigned roles, or revoke access completely. In addition, new members can be added by entering their email address and selecting the corresponding roles. Invitations are processed automatically if the user is logged in to Globant Enterprise AI. Keep in mind that the invitation is sent by email and is valid for 72 hours if the user has never logged in; after that, it expires and a new one must be generated. The Invitation History records all actions related to the assignment and revoked roles, including the person responsible for each action. > **Note**: The management of Roles and Members is only available for users with 'Organization member' and 'Project Administrator' roles. ### [See Also](#See+Also) [Migration to the new Roles and Permissions Management System](264.html)",
          "evaluationResultRowContextDocId": "452af474-b7bc-419b-b0ce-da8ddf6fcbea",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "247cc8e0-4572-464e-be57-954e552b1fd5",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?242",
          "evaluationResultRowContextName": "242",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Managing Roles and Permissions in Globant Enterprise AI: Roles Roles are fundamental to managing security and permissions within Globant Enterprise AI. They define which actions and functionalities users can perform based on their responsibilities within a project. This section focuses on managing these roles, which control frontend access to assistants. ## Project Roles Overview **Location:** Roles are created and edited within each Project. To achieve this, you must enter the [Globant Enterprise AI Backoffice](42.html). On the left side of the screen, you can find the Backoffice menu. In this menu, click on Roles below \"PROJECT OPTIONS\". ![Image](./images/246.png) **Role Description:** Each project has four default roles that allow different levels of access and functions: [Project Member](242.html), [Project Administrator](242.html), [Access all Project Assistants](242.html), and [Access for Project Assistants without Membership](242.html). **Role Origin:** It takes the value System for the roles that are defined by default when creating the project. If you create a new role, the value will be user-defined. **Role Type:** Roles are categorized as \"Backend\" or \"Frontend\". Backend roles implicitly grant Frontend access. **Permissions:** Allows defining which assistants are available to each role with the 'Frontend' value in the Role Type column, ensuring that members who are assigned these roles can access only the configured assistants, from the Frontend interface. Available only for the \"Access for Project Assistants without Membership\" role and for user-defined roles. **User Reference:** Each role displays a quick reference to the users assigned to it. Clicking on the \"USERS\" column reveals the list of assigned users. ![Image](./images/247.png) **System-Defined Roles:** Backend roles are system-defined. All roles you create will be \"Frontend\" roles. ## Creating Project Roles **Default Roles:** Every new project starts with four system-defined roles. **Creating New Roles:** To create a new role, follow these steps: 1. **Select Project:** In the Project Dynamic Combo Box, select the project you want to work with.\n2. **Navigate to Roles:** Navigate to the \"Roles\" section below \"PROJECT OPTIONS\".\n3. **Create New:** Click on the \"CREATE NEW\" button. ![Image](./images/248.png) **Role Details:** When creating a new role, provide a description (which also acts as the name) and optionally an \"External Id\". The \"External Id\" defaults to the internal identifier of the role but can be used to map the role to a role in another IDP (like Microsoft Entra ID). You can update the \"External Id\" later by editing the role. ![Image](./images/249.png) **Editing and Deleting Roles:** User-defined roles can be edited (name/description, external ID) and deleted, but only if the role is not assigned to any user. ![Image](./images/250.png) ## Assigning Assistant Permissions to Roles **List of assistants:** By clicking on \"PERMISSIONS\", a window opens where you can edit the permissions of a role, selecting from the list the assistants you wish to include. ![Image](./images/251.png) **Confirmation:** To confirm, click on the ADD button. Once you have confirmed the permissions, the list of assistants included in the role is displayed. ![Image](./images/252.png) **Deleting assistants:** To remove an assistant from the list, use the \"DELETE\" action at the end of each item. ## Frontend Access with Project Roles **Assistant Visibility:** Frontend users with a specific role assigned will only see the assistants included in that role when accessing the Frontend. **Multiple Roles:** If a user has multiple Frontend roles assigned, they will see the combined list of assistants included in all their assigned roles, even if assistants are repeated across roles. **Backend role:** Users who have a Backend role can see all the project's assistants in the Frontend. This is because Backend roles imply full access to the Frontend.",
          "evaluationResultRowContextDocId": "4b5cb192-a002-4bbf-8ea7-bf0d2c52dc67",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "5b4e7c3e-c938-4cf5-8216-aa5d651a0591",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?245",
          "evaluationResultRowContextName": "245",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# API Tokens API Tokens play a crucial role in executing Globant Enterprise AI APIs. These tokens are required to access and use the functionality provided by the APIs. There are two types of API Tokens: Organization API Tokens and Project API Tokens. ### [Organization API Tokens](#Organization+API+Tokens) Certain operations require API Tokens with a higher scope, such as access to Project creation, updating, and deletion. Users with the necessary privileges can manage this type of API Tokens in order to work only with OrganizationAPI endpoints. These API Tokens are not intended to work at the project level and cannot be used to reference assistants or AI models. ### [Project API Tokens](#Project+API+Tokens) For each project, you can define multiple Project API Tokens. This allows for granular control and tracking of usage. It is important to note that API Tokens have project-wide reach. Moreover, the ability to assign API Tokens to specific projects allows for fine-grained access control, ensuring that only authorized individuals or systems can execute requests on specific projects. With this level of granularity, organizations can effectively manage access permissions, track usage patterns, and maintain control over their projects defined within the Globant Enterprise AI API.",
          "evaluationResultRowContextDocId": "146c275d-bb81-4af3-925b-a96fe8e2f48d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "a4b2c0d1-f0df-4a8e-b5b5-b1a5813fdf71",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?564",
          "evaluationResultRowContextName": "564",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.8,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "2f9d7722-9b18-49bf-9d11-4614fdcbc20b",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0033100600",
      "evaluationResultRowEndDate": "2025-05-13T11:12:54",
      "evaluationResultRowOutput": "Permissions in Globant Enterprise AI are not assigned directly to users. Instead, permissions are managed through **roles**, which are then assigned to users. Each role defines a set of permissions or access rights that control what actions and functionalities a user can perform within a project or organization.\n\n### How it works:\n- Users receive one or more **roles**.\n- Each role encapsulates specific permissions, such as access to project resources, ability to create or manage assistants, and frontend access to assistants.\n- There are default roles like:\n  - **Project Member: </B>Access to backoffice functionalities like creating assistants and managing API tokens.\n  - **Project Administrator: </B>Includes the Project Member role plus managing project members and roles.\n  - **Access all Project Assistants: </B>Grants frontend access to all active project assistants.\n- Custom roles can be created to grant specific permissions, especially for frontend access to assistants.\n\n### User management:\n- Users can be added by entering their email and assigning them the appropriate roles.\n- Invitations for new users are sent automatically and expire after 72 hours if not accepted.\n- The roles assigned to users can be modified or revoked from the Members section.\n- Only users with the \"Organization member\" or \"Project Administrator\" roles can manage roles and members.\n\n### Summary:\n- Permissions \u2192 assigned to Roles \u2192 Roles \u2192 assigned to Users.\n- Roles control both backend and frontend access.\n- Assigning or revoking a role changes the permissions of a user accordingly.\n\nThis system offers granular control and clear management of permissions through role-based access control.\n\nFor more details, you can see the full description of managing roles and permissions here: [Managing Roles and Permissions in Globant Enterprise AI](https://wiki.genexus.com/enterprise-ai/wiki?242) and the section about roles specifically here: [Managing Roles and Permissions in Globant Enterprise AI: Roles](https://wiki.genexus.com/enterprise-ai/wiki?245) [3](https://wiki.genexus.com/enterprise-ai/wiki?242) [4](https://wiki.genexus.com/enterprise-ai/wiki?245)",
      "evaluationResultRowStartDate": "2025-05-13T11:12:45",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "7150"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#LLMs with Reasoning Capabilities This article explains how to enable and control the reasoning features available in three leading Large Language Model (LLM) providers\u2014OpenAI, Anthropic, and Google Vertex AI\u2014when calling the `chat/completions` endpoint. For each provider, you will find an overview of the relevant parameters and a cURL example that follows the standard request format used in GEAI projects. ## Providers\n### 1. OpenAI\nOpenAI\u2019s o-series reasoning models expose an optional [https://platform.openai.com/docs/api-reference/chat/create#chat-create-reasoning_effort|`reasoning_effort`] parameter that lets you choose how many tokens the model may devote to internal reasoning. This gives you explicit control over latency and cost. | Allowed values | Effect |\n| --- | --- |\n| `low` | Minimal extra reasoning (fastest, cheapest) |\n| `medium` | Balanced reasoning vs. speed/cost |\n| `high` | Maximum reasoning effort (slowest, highest cost) | #### **Sample cURL** ```\ncurl -X POST \"$BASE_URL/chat/completions\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"openai/o4-mini\", \"messages\": [ { \"role\": \"user\", \"content\": \"Could you explain and compare in detail the relationships between category theory and homotopy theory in modern algebraic topology, describe concrete applications to the classification of fibrations, then outline how these ideas extend to theoretical physics\u2014especially quantum field theory and quantum gravity\u2014considering AdS/CFT correspondence and renormalisation methods in non\u2011commutative geometry, and finally discuss the philosophical consequences of these advances on the unity of physical laws?\" } ], \"stream\": false, \"temperature\": 1, \"max_completion_tokens\": 100000, \"reasoning_effort\": \"high\"\n}' ``` ### 2. Anthropic Claude 3.7 Sonnet offers an [https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking|Extended Thinking] mode. Enable it by adding a top\u2011level `thinking` object with a `budget_tokens` field that sets the token budget for reasoning. * Range: 0 to \u2248\u202f63000 (Claude\u2019s max output tokens = 64\u202f000).\n* Note: Above 32000 tokens, the model may not consume the entire budget. #### **Sample cURL**\n```\ncurl -X POST \"$BASE_URL/chat/completions\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"anthropic/claude-3-7-sonnet-latest\", \"max_tokens\": 20000, \"thinking\": { \"type\": \"enabled\", \"budget_tokens\": 16000 }, \"messages\": [ { \"role\": \"user\", \"content\": \"Are there infinitely many prime numbers that leave a remainder of 2 when divided by 3?\" } ]\n}' ``` ### 3. Google Vertex AI Google\u2019s latest model, [https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash|Gemini 2.5 Flash Preview], also supports a reasoning budget via a `thinking` object. * Range: 1 to 24\u202f000 tokens. #### **Sample cURL** ```\ncurl -X POST \"$BASE_URL/chat/completions\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"vertex_ai/gemini-2.5-flash-preview-04-17\", \"messages\": [ { \"role\": \"user\", \"content\": \"A farmer buys 30 animals consisting only of chickens and cows, and when he counts their legs he gets 74\u2014how many of the animals are cows?\" } ], \"thinking\": { \"type\": \"enabled\", \"budget_tokens\": 10000 }, \"stream\": false, \"temperature\": 0.1\n}'\n``` {{{216| Although [https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro|Gemini 2.5 Pro Preview] is also a reasoning\u2011capable model, its reasoning feature is currently fixed and cannot be enabled, disabled, or tuned|||}}} ##Availability Since April 2025 release. {{{12}}}{{{17}}} ##MARKDOWN##",
          "evaluationResultRowContextDocId": "2d0746ec-0c30-4a2f-882c-733bfb8b8b46",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "2868de1a-249c-49a5-b6a2-62497545fef6",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?1168",
          "evaluationResultRowContextName": "1168",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "- New Average Request Time metric added in the Project Dashboard. ### January 8th - The option formerly known as 'Search Documents' has been improved and renamed to RAG Assistant (Retrieval Augmented Generation) to provide an optimized experience when searching and generating information. #### Frontend - Feedback is provided during conversations with RAG Assistants, indicating where you are in the process.\n- 'Response streaming' support for RAG Assistants.\n- Settings are hidden when selecting an assistant, except when 'Chat with LLMs' is selected. ## 2023 ### December 19th - Fixed: Too Many Redirects when accessing Playground using a browser in Spanish language. ### December 6th - New backoffice design.\n- Access to the Playground from the backoffice to chat with the assistants defined in the project.\n- Upload images for analysis with GPT-4 Vision.\n- Google Analytics support at the frontend.\n- Keep a conversation thread when chatting with documents.\n- An email notification is sent automatically when a new member is invited to join the organization or project. ### November 28th - First version officially released!! ### November 6th - The following OpenAI models are supported: GTP-4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), and GPT-4 Vision (gpt-4-vision-preview). ### October 18th - [Manage quotas per project](71.html): It is possible to set quota limits to control project expenses or usage. ### October 11th - [AI-Driven Load Balancing](32.html): The platform automatically manages the Load Balancing process when you work with generative AI providers, efficiently addressing the limits imposed by LLM platforms.\n\n- In the [Organization API](22.html), the ability to set and manage usage limits on projects through the [POST /project](22.html) and [GET /project/{id}](22.html) endpoints has been added.\n- [Quota Limit](71.html) now includes improvements such as highlighting the active quota in green, offering options to cancel active quotas, among others. ### October 17th - Rebranding to Globant Enteprise AI\n- Improvements in RAG - New option not to [compact the conversation history](297.html) - Support for Redis, MongoDB Atlas and PG Vector [vectorstores](267.html) - New endpoint to get an embedding. Check details here: [Embeddings API](422.html)\n- Playground improvements\n- File management improvements\n- New [LLMs supported](200.html) - NVIDIA provider with new models supported - nvidia.nemotron-mini-4b-instruct - meta.llama-3.1-8b-instruct - meta.llama-3.1-70b-instruct - meta.llama-3.1-405b-instruct - meta.llama-3.2-3b-instruct - Groq provider supported - groq/llama-3.1-70b-versatile - groq/llama-3.2-11b-vision-preview - groq/llama-3.2-3b-previewgroq/llama-3.2-1b-preview\n- New [embeddings models](447.html) added - Vertex AI: - vertex_ai/textembedding-gecko - vertex_ai/text-embedding-004 - vertex_ai/textembedding-gecko-multilingual - Nvidia: - nvidia/nvclip - nvidia/nv-embed-v1 - nvidia/baai.bge-m3 - nvidia/snowflake.arctic-embed-l - nvidia/nv-embedqa-mistral-7b-v2 - nvidia/embed-qa-4 - nvidia/nv-embedqa-e5-v5 ### September 25th - [Support for file](346.html) processing with prompt-based assistants. This will enable many scenarios, such as uploading documents and summarizing, extracting, and checking information, etc. Also, depending on the model used by the assistant, it will be able to process audio, video, or images.\n- [Support for multi-modal](345.html) LLMs allow processing docs, audio, video, and images in models like GPT-4o or Gemini Pro.\n- Chat with data assistants - The model used to build the queries was updated with GPT-4o, which improves the quality of the generated query. - Configure the query builder server by organization and project. This means you can connect with different DBMS from each project when building Chat with data assistants. - Show an explanation of how the query was built.\n- New [Playground Interface](349.html) design - New design - Upload documents from the front end to chat with them.\n- [Flows builder](321.html) - There will be two types of Flows, one more oriented to build a conversational UI and the other to build assistant flows. Access to these flows will only be available through [Chat API](34.html) or through the [channels](384.html) offered by Flows.\n- New models hosted in AWS Bedrock added: - Amazon Titan Express v1 - Amazon Titan Lite v1 - Anthropic Claude 3 Haiku - Anthropic Claude 3 Sonnet - Anthropic Claude 3.5 Sonnet - Cohere Command - Meta Llama 3 8B - Meta LLama 3 70B\n- It is now possible to provide clear guidance on the assistant's capabilities, allowing you to add information such as descriptions, features, and example prompts. This configuration can be done from the Backoffice, [Start Page](408.html), or WelcomeData section of the [Assistant API](35.html) and [RAG Assistants API](29.html) endpoints.\n- RAG Assistants - New endpoint in [RAG Assistants API](29.html): using [DELETE/profile/{name}/documents](29.html), all documents associated with the specified RAG Assistant are deleted. ### August 9th - Support of new models - GPT-4o mini\n- RAG Assistants - New option called [CLEANUP](60.html) allows to delete the documents associated to a RAG Assistant. - When creating a new assistant, the following defaults are updated: - [Chunk Count](69.html) updated from 2 to 5. - [Model Name](68.html) from gpt-3.5-turbo to gpt-4o-mini.\n- Data Analyst Assistant - New [Conversational Module](291.html) for Data Analyst Assistant.\n- Considerations - **Important**: [max_tokens integer below minimum value](290.html) error may occur if you are using the _max_tokens_ value as -1; those Assistants must be updated to use a correct positive value. ### July 4th - Permissions by assistants in role-based access control. A new [role and permissions management system](242.html) has been implemented to provide greater flexibility and control. Please read [Migration to the new Roles and Permissions Management System](264.html).\n- Dashboard performance optimizations. Please read [Using the Request Summary Table for Improved Dashboard Performance](273.html).\n- RAG Improvements - New option to customize the [context](205.html). - Support for [Max Marginal Relevance](206.html) - Support for _similarity_hybrid_ and _semantic_hybrid_ for [Azure AISearch](207.html) - Advanced options for SelfQuery retriever prompt case. See [Self Query Use Case](150.html) - Support for Parent Document retrieval option. See [Parent Document Retrieval Strategy](204.html) ### June 10th - [Enterprise AI Proxy](19.html) is deprecated. Use [Chat API](34.html) instead.\n- Support for new LLMs - OpenAI new model GPT-4o - Models in Google Vertex - Gemini 1.0 Pro - Gemini 1.5 Flash preview-0514 - Gemini 1.5 Pro preview-0514 - Claude 3 Haiku - Claude 3 Opus - Claude 3 Sonnet\n- RAG Improvements - New option to initialize RAG Assistant based on another when creating a new RAG Assistant. - New option to export document list in View Documents over a RAG Assistant. - Added filter options when browsing Documents. - SelfQuery RAG retriever partial support for a [customized Prompt](150.html). - Support for text-embedding-004 in Google models to generate the embeddings.\n- Deprecated [Assistant API](35.html) endpoints. - /assistant/text/begin - /assistant/text\n- Support to deploy in Google Cloud Platform. ### May 8th - New [Chat with Data Assistant](159.html).\n- New [Ingestion SDK](172.html) to automate document ingestion in RAG assistants.\n- New models hosted in NVIDIA platform supported. See [Supported Chat Models](200.html) for more details.\n- New option to export information about projects and members available for the organization administrator.\n- New API to extend dataset for [Data Analyst Assistant 1.0](89.html).\n- New filter by user email in Requests.\n- Update default to use _text-embedding-3-small_ OpenAI Embeddings for new RAG assistants.\n- Support for gemini-1.5-pro-preview-0409 model added. ### April 3rd - New [API Assistant](110.html).\n- New [Data Analyst Assistant 1.0](89.html).\n- The user Dashboard displays the email to identify the most active user or the one who generates the most costs. ### March 11th - GeneXus Identity Provider is implemented, expanding the login options in the [Backoffice](42.html) of the production environment. This allows for login not only with Google but also with Apple or GeneXus Account.\n- It is possible to customize the icon for each assistant. ### February 29th - [Frontend](82.html) improvements in UI/UX.\n- Option to get feedback from end users when interacting with [RAG Assistant](44.html).\n- Gemini Pro LLM support.\n- New [Dashboard](562.html) with user metrics.\n\n# Updates Log This log shows the most important fixes or features added to the platform. ## Log ## 2025 ### April 10th - New [Globant Enterprise AI Lab](972.html) - The Lab is designed for defining, managing, and orchestrating autonomous AI agents. It provides a standardized model for representing agents, their capabilities, and their interactions within complex workflows. The core components of the Lab include: - Agents & Tools: This module allows for the definition and management of individual agents and their resources, such as skills and tools. It serves as a central hub for cataloging and managing the agent workforce. - Agentic Processes: This component enables the definition of processes based on tasks executed by Agents. These workflows facilitate collaboration among agents to achieve larger objectives. More details at [How to create an Agentic Process](985.html). - Agent RunTime: This module provides the execution environment for agentic workflows, where agents perform tasks based on their skills and interact with artifacts, driven by events and the flow of knowledge. - The Lab aims to meet the growing demand for intelligent, self-sufficient AI agents capable of collaborating and solving complex problems with minimal human intervention. It offers a flexible and adaptable model, allowing for the creation and management of a diverse range of agents, from co-pilots working alongside humans to fully automated agents executing complex tasks. Implemented as a module of Globant Enterprise AI, the Lab supports the development of intelligent agents that can work autonomously or in collaboration with humans and other agents.\n- New features in Flows - Agent Integration Component: You can now directly integrate agents created with the AI Lab into a Flow. These agents can be exposed through platforms like WhatsApp, Teams, or Slack. - [File Upload Support](1053.html) from WhatsApp: Flows now support receiving file attachments such as documents, images, audio, and video directly from WhatsApp interactions. - Audio and Video Attachment in Web Chat: The Web Chat component now allows users to attach audio and video files, enhancing the interaction experience.\n- New Features for the Data Analyst Agent - Reduced Configuration Requirements: The setup needed to enable the assistant to respond to a wide range of questions has been minimized. - Enhanced Analysis Module: An additional analysis module has been incorporated to complement the responses with relevant business conclusions and interpretations of the obtained data.\n- New metrics to track processed tokens. - A new view has been added to each project dashboard to monitor token-based consumption. More details at [Tokens tab in Dashboard](965.html). - To enable this feature, you must run the initialization process described in [Reprocessing Requests and Loading the Summary Table](1015.html).\n- LLMs: - New Gemini 2.5 Pro (via providers Vertex AI and Gemini): Gemini 2.5 is Google\u2019s latest reasoning model, engineered to tackle increasingly complex challenges. This model is designed for tasks that demand advanced analytical thinking and robust problem-solving capabilities. More details at [LLM API](663.html). - Migration to Gemini 2.0 series (Vertex AI): Based on recommendations from Vertex AI, we have migrated from the legacy Gemini 1.0 and 1.5 models to the more advanced Gemini 2.0 series, offering improved performance, scalability, and integration capabilities. For comprehensive information, please refer to the [Deprecated Models](Deprecated Models). - New Azure OpenAI models (o1, o1-mini and o3-mini): We have expanded our model availability by introducing these models via Azure, providing the same high-quality capabilities as those offered through the OpenAI provider. - DeepSeek-R1 via AWS Bedrock: Recently added through a Serverless API, DeepSeek-R1 offers reliable inference with a substantial 128K token context window and up to 32K maximum output tokens. - OpenAI's new models with built-in web search tool: These specialized models integrate web search capabilities directly into the Chat Completions API, enabling them to both interpret and execute search queries in real time. - openai/gpt-4o-search-preview - openai/gpt-4o-mini-search-preview - New models - Beta only: - gemini/gemma-3-27b-it: Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 has a large, 128K context window, and multilingual support in over 140 languages. - SambaNova: - DeepSeek-R1: This Provider offers the fastest performance for running DeepSeek, processing up to 198 tokens per second per user, with a 16K token context window. The model is hosted in US data centers; privately and securely. - DeepSeek-V3-0324: This model significantly outperforms its predecessor with enhanced reasoning benchmarks, improved code executability and refined web aesthetics, and superior Chinese writing aligned with the R1 style. It also offers better multi-turn interactive rewriting, translation quality, detailed report analysis, and more accurate function calling. - QwQ-32B: SambaNova provides access to QwQ-32B-Preview, the best open source test-time compute model released by Alibaba. - Llama 4 collection by Meta: - Llama 4 Scout: A 17B-parameter multimodal MoE model with 16 experts that excels in text and image understanding. The model is currently in beta and is available via providers Nvidia, Groq and SambaNova. Via Groq, it supports a 128k tokens context window with fast inference at 460 tokens/sec, while SambaNova Cloud runs at 697 tokens/second/user. - Llama 4 Maverick: Available via Nvidia, this 17-billion-parameter model featuring 128 experts supports a 32k-token context window. ### March 10th - New LLMs: - GPT-4.5 - Claude 3.7 Sonnet (Providers Anthropic, Vertex AI and AWS Bedrock) - Updates in Gemini 2.0 series: - vertex_ai/gemini-2.0-flash-lite-preview-02-05 - vertex_ai/gemini-2.0-flash-thinking-exp-01-21\n- RAG Revision #6 - Support for o3-mini, gpt-4.5-preview, claude-3-7-sonnet-20250219, new DeepSeek, Gemini2* and sambanova LLM providers. - New pinecone provider available for [embeddings](447.html) and [rerankers](638.html). - The [CleanUp](60.html) action message has been corrected to clearly specify that it will permanently delete the RAG Assistant files and update the information in the RDS. - Added [usage](712.html) element on every response. - Improvements when changing the [LLM](69.html)/[Embeddings](68.html) settings; all models and providers are normalized to be selected from standard combo-box items; use the [override mechanism](209.html) if you need other options. - Support for [guardrails](840.html). - New [documentAggregation](298.html) property to decide how sources are grouped and returned.\n- It is possible to [provide feedback](170.html) on the response of the Chat with Data Assistant in the Frontend.\n\n- The new [Evaluation APIs](893.html) introduce key functionalities through three interconnected APIs: [DataSet API](839.html), [Evaluation Plan API](849.html), and [Evaluation Result API](856.html). This version is primarily designed for users with a data science profile and is mainly accessed via APIs, complemented by a series of Jupyter notebooks that demonstrate their use. For a comprehensive guide on how to use these APIs, you can refer to [How to evaluate an AI Assistant](881.html) and the [EvaluationAPITutorial.ipynb](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?887,,) notebook, which provide practical examples and code for working through the evaluation process.\n- File attachment support in Flows (version 0.9).\n- Support for Full Story integration in the Workspace/Playground to generate user access statistics in Full Story.\n- In the [LLM API](663.html), for models that have descriptions in the specified languages, the `descriptions` property is included in the Response, which contains the descriptions in the available languages, such as `Spanish`, `English` and `Japanese`.\n- [Data Analyst Assistant 2.0](886.html) version presents important improvements, simplifying the interaction with the data by reducing the main components to just two: **Dispatcher** and **Thinker**. In addition, the metadata structure is automatically generated when loading the datasets, streamlining the setup process. For more information, see [How to create a Data Analyst Assistant](105.html).\n- The option to consult version-specific documentation is now available. Articles with versions show the option \u201cOther document versions\u201d in the header. Clicking on \u201cOther document versions\u201d brings up a menu that allows you to choose between the most recent version (\u201cLatest\u201d) or earlier versions (e.g. \u201c2025-02 or prior\u201d). If you select a version other than \u201cLatest\u201d, a message appears: \u201cThis is not the latest version of this document; to access the latest version, click here\u201d. This message provides a direct link to the most up-to-date documentation. [Components Version Update](822.html) ### February 10th - New documentation with details about [Supported Chart Types](731.html).\n- New [Usage Limits API](802.html).\n- Flows - Logs associated with conversations are accessible from the [conversation history](378.html).\n- RAG Revision #5 - Ingestion supports new [file formats](137.html). For on-premises installations, update the _FILE_TYPES_ parameter. - New optional _cleanProcessing_ parameter when using [Parent Document Retrieval Strategy](204.html). - [Embeddings API](422.html) cache support; disabled by default from a [RAG assistant](68.html). - New [mode](68.html) property for using multimodal embeddings and creating a [Multimodal RAG](783.html).\n- New endpoint [GET /accessControl/apitoken/validate](22.html) returns information about the organization and project associated with the provided apitoken.\n- New LLMs: - Already in production - **OpenAI**: o3-mini - Already in Beta - **DeepSeek**: - deepseek/deepseek-reasoner - deepseek/deepseek-chat - azure/deepseek-r1 - nvidia/deepseek-ai-deepseek-r1 - groq/deepseek-r1-distill-llama-70b - sambanova/DeepSeek-R1-Distill-Llama-70B - Updates in Gemini 2.0 series: - gemini-2.0-flash-thinking-exp-01-21 (Via Providers Gemini and Vertex AI) - gemini/gemini-2.0-flash-lite-preview - gemini/gemini-2.0-pro-exp - vertex_ai/gemini-2.0-flash-001 - sambanova/Llama-3.1-Tulu-3-405B [Components Version Update](822.html) ### January 13th - Internationalization, [Backoffice](42.html), and frontend support for Japanese.\n- Invitations now include information about the organization and project in the subject.\n- New LLMs - Already in Production - **OpenAI**: o1 (2024-12-17 version) - Already in Beta - New Providers: **Cohere** - **Cohere**: Cohere-r\n- [Guardrails](682.html) configured by assistant.\n- [Rerank API](625.html) to semantically order a list of document chunks given a query.\n- New optional RAG [Retrieve and Rerank](620.html) adds an extra layer of precision to ensure that only the most relevant information reaches the model used in the generation step. ## 2024 ### December 12th - [Automatic Creation of Default Assistant](640.html)\n- [Organization Usage Limits](666.html): It is possible to set quota limits to control organization expenses or usage.\n- Chat with Data Assistant - [Show details](170.html) about the generated query in the Playground. - Support in [Chat API](34.html) to interact with Chat with Data Assistant.\n- Flows - Support for markdown when showing the response on the different channels supported by Flows (web, Slack, WhatsApp, and Teams). - New component for connecting flows to the agent overflow console (Human-in-the-loop) via B2Chat. Please read [How to connect a Flow to B2Chat](674.html).\n- RAG - [Chroma](537.html), [Elasticsearch](579.html), [OpenSearch](580.html) vectorstore support. - [Improvements](581.html) when processing PDF documents (update to RAG module revision #4 or higher). - Integration with [AWS Knowledge Bases](577.html) at query time.\n- Data Analyst Assistant - Option to update metadata options. - New version by default in new Data Analyst assistants.\n- New LLMs - OpenAI: gpt-4o-2024-11-20 - AWS Bedrock: Anthropic Claude 3.5 Haiku - Amazon Nova models (Micro, Lite, and Pro) - Llama 3.1 405B on Vertex AI - Beta: - Support for providers Cerebras, SambaNova and xAI (Grok models). - All new Gemini Experimental models.\n- Security - It is possible to define a user 'all-members@domain.com' to give access to the Frontend to all users of the domain '@domain.com'. Permissions are enforced based on the roles assigned to this user. Please read [Allowing Frontend Access for Users from Specific Domains](657.html) ### November 12th - Flows execution integrated into the Playground\n- New [LLMs support](200.html) - **OpenAI**: o1-preview and o1-mini - **Claude Sonnet 3.5 v2 - Providers**: Anthropic, Vertex AI, and AWS Bedrock - **Llama 3.2 models - Providers**: Vertex AI and AWS Bedrock\n- Chat with data assistants - Possibility to edit metadata, [entities](170.html), and attribute descriptions. - The Properties tab has been renamed to [Settings](170.html) along with the options that can be configured in it.\n- RAG - New [returnSourceDocuments](298.html) option to disable returning the documents section used to answer the question. - New [step](298.html) option to use the assistant as a retrieval tool. - Support for [custom history](85.html) in conversations using the _chat_history_ variable.\n- Stand-alone Frontend based on the new Playground UI - Options to customize the Frontend to use the client logo, color palette, welcome message, and descriptions. - Feature to collect feedback (thumbs up/down) in each response. - Google Analytics support.\n- Data Analyst Assistant - Support to upload large CSV files.",
          "evaluationResultRowContextDocId": "68ace6a6-8b1b-4736-a466-9ed0a67b97a9",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "595f8e30-a98e-4f0f-a7c0-2ed1a62c0313",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?31",
          "evaluationResultRowContextName": "31",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Supported models and file formats This table provides a detailed overview of the multimodal models supported by Globant Enterprise AI and their compatibility with different file formats such as `pdf`<sup>(1)</sup>, `txt`, `png`, `jpg`, `jpeg`, `gif`, `mp3` and `mp4`. The size of the supported files will depend on the context window of each model. <div style=\"background-color: rgb(242,242,242);border-left: 6.0px solid rgb(128,128,128);margin-bottom: 15.0px;padding: 4.0px 12.0px;\"><b>Note</b>:All non-multimodal models (not included in the table) accept text (`txt`) and PDF (`pdf`) files.</div> | **Provider** | **Model** | **Supported formats** | **Context window (tokens)**|\n|---|---|---|---|\n| anthropic | claude-3-5-sonnet-20241022 | txt, pdf, jpeg, jpg, png, gif | 200000 |\n| anthropic | claude-3-haiku-20240307 | txt, pdf, png, gif | 200000 |\n| anthropic | claude-3-opus-20240229 | txt, pdf, png, gif | 200000 |\n| anthropic | claude-3-5-sonnet-20240620 | txt, pdf, png, gif | 200000 |\n| anthropic | claude-3-7-sonnet-latest | txt, pdf, png, gif, jpeg | 200000 |\n| awsbedrock | anthropic.claude-3-opus | txt, pdf | 200000 |\n| awsbedrock | meta.llama3-2-11b | txt, pdf | 128000 |\n| awsbedrock | meta.llama3-2-90b | txt, pdf | 128000 |\n| awsbedrock | anthropic.claude-3-7-sonnet | txt, pdf, png, gif, jpeg | 200000 |\n| awsbedrock | anthropic.claude-3.5-sonnet-v2 | txt, pdf, jpeg, jpg, png, gif | 200000 |\n| awsbedrock | anthropic.claude-3-haiku | txt, pdf, png, gif | 200000 |\n| awsbedrock | anthropic.claude-3.5-sonnet | txt, pdf, png, gif | 200000 |\n| awsbedrock | amazon.nova-lite-v1:0 | txt, pdf, png, gif | 300000 |\n| awsbedrock | amazon.nova-pro-v1:0 | txt, pdf, png, gif | 300000 |\n| azure | gpt-4o | txt, pdf, gif, jpg, png | 128000 |\n| azure | gpt-4o-mini | txt, pdf, gif, jpg, png | 128000 |\n| azure | o1 | txt, pdf, jpg, png, gif | 200000 |\n| gemini | gemini-2.0-flash-thinking-exp-01-21 | txt, pdf, png, mp3 | 1048576 |\n| gemini | gemini-1.5-pro-latest | txt, pdf, jpg | 2097152 |\n| gemini | gemini-exp-1206 | txt, pdf, jpg | 2097152 |\n| gemini | gemini-2.0-pro-exp | txt, pdf, jpg | 2097152 |\n| gemini | gemini-1.5-flash-8b-exp-0827 | txt, pdf, jpg, png, gif, mp3 | 1048576 |\n| gemini | gemini-1.5-flash-latest | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| gemini | gemini-2.0-flash-exp | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| gemini | gemini-2.0-flash-lite-preview | txt, pdf, jpg, png, mp3 | 1048576 |\n| gemini | gemma-3-27b-it | txt, pdf | 131072 |\n| gemini | gemini-2.5-pro-exp-03-25 | txt, pdf, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| groq | meta-llama-4-scout-17b-16e-instruct | txt, pdf, jpg, jpeg, png | 131072 |\n| groq | meta-llama-4-maverick-17b-128e-instruct | txt, pdf, jpg, jpeg, png | 131072 |\n| groq | llama-3.2-11b-vision-preview | txt, pdf | 128000 |\n| groq | llama-3.2-90b-vision-preview | txt, pdf | 128000 |\n| nvidia | meta-llama-4-scout-17b-16e-instruct | txt, pdf, png, jpeg, jpg, gif | 131072 |\n| nvidia | meta-llama-4-maverick-17b-128e-instruct | txt, pdf, png, jpeg, jpg, gif | 32768 |\n| openai | chatgpt-4o-latest | txt, pdf, jpg, png, gif | 128000 |\n| openai | o1 | txt, pdf, jpg, png, gif | 200000 |\n| openai | o1-pro | pdf, jpg, jpeg, png, gif, webp | 200000 |\n| openai | o3 | txt, pdf, jpg, jpeg, png, gif | 200000 |\n| openai | o4-mini | txt, pdf, jpg, jpeg, png, gif | 200000 |\n| openai | gpt-4.1 | txt, pdf, gif, jpg, png, jpeg | 1047576 |\n| openai | gpt-4.1-mini | txt, pdf, gif, jpg, png, jpeg | 1047576 |\n| openai | gpt-4.1-nano | txt, pdf, gif, jpg, png, jpeg | 1047576 |\n| openai | gpt-4.5-preview | txt, pdf, gif, jpg, png, jpeg | 128000 |\n| openai | gpt-4o | txt, pdf, gif, jpg, png | 128000 |\n| openai | gpt-4o-mini | txt, pdf, gif, jpg, png | 128000 |\n| openai | gpt-4o-2024-11-20 | txt, pdf, gif, jpg, png | 128000 |\n| openai | gpt-4o-search-preview | txt, pdf | 128000 |\n| openai | gpt-4o-mini-search-preview | txt, pdf | 128000 |\n| openrouter | [https://openrouter.ai/openrouter/auto|auto] | txt, pdf, gif, jpg, jpeg, png | depends on the selected model |\n| sambanova | Llama-3.2-11B-Vision-Instruct | txt, pdf, png, jpg | 4096 |\n| sambanova | Llama-3.2-90B-Vision-Instruct | txt, pdf | 4096 |\n| vertex_ai | claude-3-7-sonnet-20250219 | txt, pdf, png, gif, jpeg | 200000 |\n| vertex_ai | claude-3-5-sonnet-20240620 | txt, pdf, pdf, png, gif | 200000 |\n| vertex_ai | claude-3-haiku-20240307 | txt, pdf, png, gif | 200000 |\n| vertex_ai | claude-3-opus-20240229 | txt, pdf, png, gif | 200000 |\n| vertex_ai | meta.llama-3.2-90b-vision-instruct-maas | txt, pdf | 128000 |\n| vertex_ai | meta-llama-4-scout-17b-16e-instruct-maas | txt, pdf, jpeg, jpg, png, gif | 10000000 |\n| vertex_ai | meta-llama-4-maverick-17b-128e-instruct-maas | txt, pdf, jpeg, jpg, png, gif | 1000000 |\n| vertex_ai | claude-3-5-sonnet-v2-20241022 | txt, pdf, jpeg, jpg, png, gif | 200000 |\n| vertex_ai | gemini-2.0-flash-thinking-exp-01-21 | txt, pdf, jpg, png, mp3, mp4 | 32000 |\n| vertex_ai | gemini-1.5-pro | txt, pdf, jpg, png, gif, mp3, mp4, ogg | 2097152 |\n| vertex_ai | gemini-1.5-pro-002 | txt, pdf, jpg, png, gif, mp3, mp4, ogg | 2097152 |\n| vertex_ai | gemini-2.0-flash-exp | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| vertex_ai | gemini-2.0-flash-001 | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| vertex_ai | gemini-2.0-flash-lite | txt, pdf, jpg, jpeg, png, mp3, mp4, ogg | 1048576 |\n| vertex_ai | gemini-2.0-flash | txt, pdf, jpg, jpeg, png, mp3, mp4, ogg | 1048576 |\n| vertex_ai | gemini-2.5-pro-exp-03-25 | txt, pdf, jpg, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| vertex_ai | gemini-2.5-flash-preview-04-17 | txt, pdf, jpg, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| vertex_ai | gemini-2.5-pro-preview-05-06 | txt, pdf, jpg, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| xai | grok-vision-beta | txt, pdf, png | 8192 |\n| xai | grok-2-vision-1212 | txt, pdf, png | 32768 | <sup>(1)</sup> - Encrypted files are not supported. ## See Also [[200]] [[843]] {{{12}}} {{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "2021f0ec-69d2-4e24-a755-4628261a3c84",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "7c493993-841c-4290-a2e6-71b544ec4e55",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?406",
          "evaluationResultRowContextName": "406",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Supported Embeddings Models **List of supported embeddings models** | **Provider** | **Model** | **Multimodal** | **Description** | **Environment Support** |\n|---|---|---|---|---|\n| **openai** | text-embedding-ada-002<br>text-embedding-3-small<br>text-embedding-3-large|No<br>No<br>No<br>No|Must set dimensions to 1536, other values are not enabled by default<sup>(1)</sup> | Beta, QA, Production|\n| **azureopenai** | * | | Same models as openai, configure the associated URL endpoints | Beta, QA, Production |\n| **google/googlevertexai/vertex_ai**<sup>(3)</sup>|multimodalembedding@001<sup>(2)</sup><br>text-embedding-004<br>text-embedding-005<br>text-multilingual-embedding-002<br>|Yes<br>No<br>No<br>No|The dimension parameter must be configured to 768<sup>(1)</sup> or 1408<sup>(2)</sup> | Beta, QA, Production |\n| **awsbedrock** | amazon.titan-embed-image-v1<br>cohere.embed-english-v3<br>amazon.titan-embed-v2<br>cohere.embed-english<br>cohere.embed-multilingual|Yes<br>Yes<br>No<br>Yes<br>No|Check the model dimensions configuration<sup>(1)</sup>| Beta, QA, Production |\n| **nvidia** | baai.bge-m3<br>nv-embed-v1<br>nvclip<br>embed-qa-4<br>nv-embedqa-e5-v5<br>snowflake.arctic-embed-l<br>nv-embedqa-mistral-7b-v2|No<br>No<br>Yes<br>No<br>No<br>No<br>No|Check the model dimensions configuration<sup>(1)</sup> | Beta, QA, Production |\n| **cohere** | embed-english-v3.0<br>embed-multilingual-v3.0<br>embed-english-light-v3.0<br>embed-multilingual-light-v3.0|Yes<br>No<br>No<br>No| 1024 dimensions except for the \"*light*\" models using 384. | Beta, QA, Production |\n| **pinecone**<sup>(2)</sup> | multilingual-e5-large<br>llama-text-embed-v2|No<br>No| 1024 dimensions. | Beta, QA | <sup>(1)</sup> - For [[441]], the associated module must be installed and configured. <sup>(2)</sup> - The configuration must be done in the [[70|Retrieval tab]] by configuring the [[298|Profile Metadata]] parameter. For more information, refer to [[298##Not registered Providers and Models|Not registered Providers and Models]]. Please note that the associated `Embeddings costs` are not recorded, and no related `Requests` are logged as they are processed by the provider. <sup>(3)</sup> - Check the [https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#supported-models|Provider recommendations]. {{{12}}}{{{17}}}\n##MARKDOWN##",
          "evaluationResultRowContextDocId": "9d067949-05dd-404f-8c5a-55e97750d270",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "f1ca1c6b-6055-46d0-a30d-0775092229cf",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?447",
          "evaluationResultRowContextName": "447",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.2,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "362bdf9f-3969-4892-8da8-1fd8ff5088c7",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0048212800",
      "evaluationResultRowEndDate": "2025-05-13T11:13:04",
      "evaluationResultRowOutput": "Los modelos de OpenAI soportados en Globant Enterprise AI son los siguientes, junto con los formatos que aceptan y su ventana de contexto en tokens:\n\n| Modelo               | Formatos soportados                 | Context window (tokens) |\n|----------------------|-----------------------------------|------------------------|\n| chatgpt-4o-latest    | txt, pdf, jpg, png, gif            | 128,000                |\n| o1                   | txt, pdf, jpg, png, gif            | 200,000                |\n| o1-pro                | pdf, jpg, jpeg, png, gif, webp    | 200,000                |\n| o3                   | txt, pdf, jpg, jpeg, png, gif     | 200,000                |\n| o4-mini               | txt, pdf, jpg, jpeg, png, gif     | 200,000                |\n| gpt-4.1               | txt, pdf, gif, jpg, png, jpeg     | 1,047,576              |\n| gpt-4.1-mini          | txt, pdf, gif, jpg, png, jpeg     | 1,047,576              |\n| gpt-4.1-nano          | txt, pdf, gif, jpg, png, jpeg     | 1,047,576              |\n| gpt-4.5-preview       | txt, pdf, gif, jpg, png, jpeg     | 128,000                |\n| gpt-4o                | txt, pdf, gif, jpg, png            | 128,000                |\n| gpt-4o-mini           | txt, pdf, gif, jpg, png            | 128,000                |\n| gpt-4o-2024-11-20     | txt, pdf, gif, jpg, png            | 128,000                |\n| gpt-4o-search-preview | txt, pdf                          | 128,000                |\n| gpt-4o-mini-search-preview | txt, pdf            | 128,000                |\n\nAdem\u00e1s, algunos modelos con sufijos \"-mini\" o \"-search-preview\" permiten ajustar el par\u00e1metro `reasoning_effort` para controlar el esfuerzo de razonamiento interno.\n\nEsta informaci\u00f3n est\u00e1 recogida en el contexto de modelos multimodales en la tabla de modelos y formatos soportados y m\u00e1s detalles sobre las capacidades de razonamiento en los modelos OpenAI en los documentos relacionados a LLMs con capacidades de razonamiento. \n\nPuedes revisar esta informaci\u00f3n en el apartado de modelos soportados: [Contexto 1](https://wiki.genexus.com/enterprise-ai/wiki?406), y sobre razonamiento con OpenAI en [Contexto 2](https://wiki.genexus.com/enterprise-ai/wiki?1168).",
      "evaluationResultRowStartDate": "2025-05-13T11:12:54",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "10468"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#Amazon Knowledge Base Retriever [https://aws.amazon.com/bedrock/knowledge-bases/|Amazon Bedrock Knowledge Bases] is a managed service for end-to-end RAG workflow provided by Amazon Web Services (AWS). It provides an ingestion workflow for converting your documents into embeddings (vector) and storing the embeddings in a specialized vector database. To query existing Knowledge Bases using Globant Enterprise AI for [[441|AWS on-premises]] installations, you can add the following configuration to your RAG:\n```\nretriever: awsKB\nknowledgeBaseId: your-knowledge-base-id\n``` You must set the `knowledgeBaseId` parameter in the [[70##Profile Metadata|Profile Metadata]] as follows: ```json\n{ \"chat\": { \"retriever\": { \"knowledgeBaseId\": \"sampleKBId\" } }\n}\n```\nBy setting the RAG assistant as an `awsKB` retriever, a direct connection is established to the associated Knowledge Base using the complete configuration defined in the Bedrock environment. ## Considerations * You must perform the ingestion process directly through the AWS KB APIs or AWS Console. * To query AWS Knowledge Bases, you need: * A Globant Enterprise AI deployment on AWS (other cloud platforms are not supported). * An account with access to the indexes and documents. * You can query each Knowledge Base with one RAG by mapping the Knowledge Base Id parameter. * All data source synchronization and model selection tasks are managed through the AWS Bedrock console. {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "bf7ba40a-d828-42aa-9d36-88bfd11e978c",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "71bc9296-ab15-496c-9c68-7b6f58fe3ed0",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?577",
          "evaluationResultRowContextName": "577",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "- New Average Request Time metric added in the Project Dashboard. ### January 8th - The option formerly known as 'Search Documents' has been improved and renamed to RAG Assistant (Retrieval Augmented Generation) to provide an optimized experience when searching and generating information. #### Frontend - Feedback is provided during conversations with RAG Assistants, indicating where you are in the process.\n- 'Response streaming' support for RAG Assistants.\n- Settings are hidden when selecting an assistant, except when 'Chat with LLMs' is selected. ## 2023 ### December 19th - Fixed: Too Many Redirects when accessing Playground using a browser in Spanish language. ### December 6th - New backoffice design.\n- Access to the Playground from the backoffice to chat with the assistants defined in the project.\n- Upload images for analysis with GPT-4 Vision.\n- Google Analytics support at the frontend.\n- Keep a conversation thread when chatting with documents.\n- An email notification is sent automatically when a new member is invited to join the organization or project. ### November 28th - First version officially released!! ### November 6th - The following OpenAI models are supported: GTP-4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), and GPT-4 Vision (gpt-4-vision-preview). ### October 18th - [Manage quotas per project](71.html): It is possible to set quota limits to control project expenses or usage. ### October 11th - [AI-Driven Load Balancing](32.html): The platform automatically manages the Load Balancing process when you work with generative AI providers, efficiently addressing the limits imposed by LLM platforms.\n\n- In the [Organization API](22.html), the ability to set and manage usage limits on projects through the [POST /project](22.html) and [GET /project/{id}](22.html) endpoints has been added.\n- [Quota Limit](71.html) now includes improvements such as highlighting the active quota in green, offering options to cancel active quotas, among others. ### October 17th - Rebranding to Globant Enteprise AI\n- Improvements in RAG - New option not to [compact the conversation history](297.html) - Support for Redis, MongoDB Atlas and PG Vector [vectorstores](267.html) - New endpoint to get an embedding. Check details here: [Embeddings API](422.html)\n- Playground improvements\n- File management improvements\n- New [LLMs supported](200.html) - NVIDIA provider with new models supported - nvidia.nemotron-mini-4b-instruct - meta.llama-3.1-8b-instruct - meta.llama-3.1-70b-instruct - meta.llama-3.1-405b-instruct - meta.llama-3.2-3b-instruct - Groq provider supported - groq/llama-3.1-70b-versatile - groq/llama-3.2-11b-vision-preview - groq/llama-3.2-3b-previewgroq/llama-3.2-1b-preview\n- New [embeddings models](447.html) added - Vertex AI: - vertex_ai/textembedding-gecko - vertex_ai/text-embedding-004 - vertex_ai/textembedding-gecko-multilingual - Nvidia: - nvidia/nvclip - nvidia/nv-embed-v1 - nvidia/baai.bge-m3 - nvidia/snowflake.arctic-embed-l - nvidia/nv-embedqa-mistral-7b-v2 - nvidia/embed-qa-4 - nvidia/nv-embedqa-e5-v5 ### September 25th - [Support for file](346.html) processing with prompt-based assistants. This will enable many scenarios, such as uploading documents and summarizing, extracting, and checking information, etc. Also, depending on the model used by the assistant, it will be able to process audio, video, or images.\n- [Support for multi-modal](345.html) LLMs allow processing docs, audio, video, and images in models like GPT-4o or Gemini Pro.\n- Chat with data assistants - The model used to build the queries was updated with GPT-4o, which improves the quality of the generated query. - Configure the query builder server by organization and project. This means you can connect with different DBMS from each project when building Chat with data assistants. - Show an explanation of how the query was built.\n- New [Playground Interface](349.html) design - New design - Upload documents from the front end to chat with them.\n- [Flows builder](321.html) - There will be two types of Flows, one more oriented to build a conversational UI and the other to build assistant flows. Access to these flows will only be available through [Chat API](34.html) or through the [channels](384.html) offered by Flows.\n- New models hosted in AWS Bedrock added: - Amazon Titan Express v1 - Amazon Titan Lite v1 - Anthropic Claude 3 Haiku - Anthropic Claude 3 Sonnet - Anthropic Claude 3.5 Sonnet - Cohere Command - Meta Llama 3 8B - Meta LLama 3 70B\n- It is now possible to provide clear guidance on the assistant's capabilities, allowing you to add information such as descriptions, features, and example prompts. This configuration can be done from the Backoffice, [Start Page](408.html), or WelcomeData section of the [Assistant API](35.html) and [RAG Assistants API](29.html) endpoints.\n- RAG Assistants - New endpoint in [RAG Assistants API](29.html): using [DELETE/profile/{name}/documents](29.html), all documents associated with the specified RAG Assistant are deleted. ### August 9th - Support of new models - GPT-4o mini\n- RAG Assistants - New option called [CLEANUP](60.html) allows to delete the documents associated to a RAG Assistant. - When creating a new assistant, the following defaults are updated: - [Chunk Count](69.html) updated from 2 to 5. - [Model Name](68.html) from gpt-3.5-turbo to gpt-4o-mini.\n- Data Analyst Assistant - New [Conversational Module](291.html) for Data Analyst Assistant.\n- Considerations - **Important**: [max_tokens integer below minimum value](290.html) error may occur if you are using the _max_tokens_ value as -1; those Assistants must be updated to use a correct positive value. ### July 4th - Permissions by assistants in role-based access control. A new [role and permissions management system](242.html) has been implemented to provide greater flexibility and control. Please read [Migration to the new Roles and Permissions Management System](264.html).\n- Dashboard performance optimizations. Please read [Using the Request Summary Table for Improved Dashboard Performance](273.html).\n- RAG Improvements - New option to customize the [context](205.html). - Support for [Max Marginal Relevance](206.html) - Support for _similarity_hybrid_ and _semantic_hybrid_ for [Azure AISearch](207.html) - Advanced options for SelfQuery retriever prompt case. See [Self Query Use Case](150.html) - Support for Parent Document retrieval option. See [Parent Document Retrieval Strategy](204.html) ### June 10th - [Enterprise AI Proxy](19.html) is deprecated. Use [Chat API](34.html) instead.\n- Support for new LLMs - OpenAI new model GPT-4o - Models in Google Vertex - Gemini 1.0 Pro - Gemini 1.5 Flash preview-0514 - Gemini 1.5 Pro preview-0514 - Claude 3 Haiku - Claude 3 Opus - Claude 3 Sonnet\n- RAG Improvements - New option to initialize RAG Assistant based on another when creating a new RAG Assistant. - New option to export document list in View Documents over a RAG Assistant. - Added filter options when browsing Documents. - SelfQuery RAG retriever partial support for a [customized Prompt](150.html). - Support for text-embedding-004 in Google models to generate the embeddings.\n- Deprecated [Assistant API](35.html) endpoints. - /assistant/text/begin - /assistant/text\n- Support to deploy in Google Cloud Platform. ### May 8th - New [Chat with Data Assistant](159.html).\n- New [Ingestion SDK](172.html) to automate document ingestion in RAG assistants.\n- New models hosted in NVIDIA platform supported. See [Supported Chat Models](200.html) for more details.\n- New option to export information about projects and members available for the organization administrator.\n- New API to extend dataset for [Data Analyst Assistant 1.0](89.html).\n- New filter by user email in Requests.\n- Update default to use _text-embedding-3-small_ OpenAI Embeddings for new RAG assistants.\n- Support for gemini-1.5-pro-preview-0409 model added. ### April 3rd - New [API Assistant](110.html).\n- New [Data Analyst Assistant 1.0](89.html).\n- The user Dashboard displays the email to identify the most active user or the one who generates the most costs. ### March 11th - GeneXus Identity Provider is implemented, expanding the login options in the [Backoffice](42.html) of the production environment. This allows for login not only with Google but also with Apple or GeneXus Account.\n- It is possible to customize the icon for each assistant. ### February 29th - [Frontend](82.html) improvements in UI/UX.\n- Option to get feedback from end users when interacting with [RAG Assistant](44.html).\n- Gemini Pro LLM support.\n- New [Dashboard](562.html) with user metrics.\n\n# Updates Log This log shows the most important fixes or features added to the platform. ## Log ## 2025 ### April 10th - New [Globant Enterprise AI Lab](972.html) - The Lab is designed for defining, managing, and orchestrating autonomous AI agents. It provides a standardized model for representing agents, their capabilities, and their interactions within complex workflows. The core components of the Lab include: - Agents & Tools: This module allows for the definition and management of individual agents and their resources, such as skills and tools. It serves as a central hub for cataloging and managing the agent workforce. - Agentic Processes: This component enables the definition of processes based on tasks executed by Agents. These workflows facilitate collaboration among agents to achieve larger objectives. More details at [How to create an Agentic Process](985.html). - Agent RunTime: This module provides the execution environment for agentic workflows, where agents perform tasks based on their skills and interact with artifacts, driven by events and the flow of knowledge. - The Lab aims to meet the growing demand for intelligent, self-sufficient AI agents capable of collaborating and solving complex problems with minimal human intervention. It offers a flexible and adaptable model, allowing for the creation and management of a diverse range of agents, from co-pilots working alongside humans to fully automated agents executing complex tasks. Implemented as a module of Globant Enterprise AI, the Lab supports the development of intelligent agents that can work autonomously or in collaboration with humans and other agents.\n- New features in Flows - Agent Integration Component: You can now directly integrate agents created with the AI Lab into a Flow. These agents can be exposed through platforms like WhatsApp, Teams, or Slack. - [File Upload Support](1053.html) from WhatsApp: Flows now support receiving file attachments such as documents, images, audio, and video directly from WhatsApp interactions. - Audio and Video Attachment in Web Chat: The Web Chat component now allows users to attach audio and video files, enhancing the interaction experience.\n- New Features for the Data Analyst Agent - Reduced Configuration Requirements: The setup needed to enable the assistant to respond to a wide range of questions has been minimized. - Enhanced Analysis Module: An additional analysis module has been incorporated to complement the responses with relevant business conclusions and interpretations of the obtained data.\n- New metrics to track processed tokens. - A new view has been added to each project dashboard to monitor token-based consumption. More details at [Tokens tab in Dashboard](965.html). - To enable this feature, you must run the initialization process described in [Reprocessing Requests and Loading the Summary Table](1015.html).\n- LLMs: - New Gemini 2.5 Pro (via providers Vertex AI and Gemini): Gemini 2.5 is Google\u2019s latest reasoning model, engineered to tackle increasingly complex challenges. This model is designed for tasks that demand advanced analytical thinking and robust problem-solving capabilities. More details at [LLM API](663.html). - Migration to Gemini 2.0 series (Vertex AI): Based on recommendations from Vertex AI, we have migrated from the legacy Gemini 1.0 and 1.5 models to the more advanced Gemini 2.0 series, offering improved performance, scalability, and integration capabilities. For comprehensive information, please refer to the [Deprecated Models](Deprecated Models). - New Azure OpenAI models (o1, o1-mini and o3-mini): We have expanded our model availability by introducing these models via Azure, providing the same high-quality capabilities as those offered through the OpenAI provider. - DeepSeek-R1 via AWS Bedrock: Recently added through a Serverless API, DeepSeek-R1 offers reliable inference with a substantial 128K token context window and up to 32K maximum output tokens. - OpenAI's new models with built-in web search tool: These specialized models integrate web search capabilities directly into the Chat Completions API, enabling them to both interpret and execute search queries in real time. - openai/gpt-4o-search-preview - openai/gpt-4o-mini-search-preview - New models - Beta only: - gemini/gemma-3-27b-it: Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 has a large, 128K context window, and multilingual support in over 140 languages. - SambaNova: - DeepSeek-R1: This Provider offers the fastest performance for running DeepSeek, processing up to 198 tokens per second per user, with a 16K token context window. The model is hosted in US data centers; privately and securely. - DeepSeek-V3-0324: This model significantly outperforms its predecessor with enhanced reasoning benchmarks, improved code executability and refined web aesthetics, and superior Chinese writing aligned with the R1 style. It also offers better multi-turn interactive rewriting, translation quality, detailed report analysis, and more accurate function calling. - QwQ-32B: SambaNova provides access to QwQ-32B-Preview, the best open source test-time compute model released by Alibaba. - Llama 4 collection by Meta: - Llama 4 Scout: A 17B-parameter multimodal MoE model with 16 experts that excels in text and image understanding. The model is currently in beta and is available via providers Nvidia, Groq and SambaNova. Via Groq, it supports a 128k tokens context window with fast inference at 460 tokens/sec, while SambaNova Cloud runs at 697 tokens/second/user. - Llama 4 Maverick: Available via Nvidia, this 17-billion-parameter model featuring 128 experts supports a 32k-token context window. ### March 10th - New LLMs: - GPT-4.5 - Claude 3.7 Sonnet (Providers Anthropic, Vertex AI and AWS Bedrock) - Updates in Gemini 2.0 series: - vertex_ai/gemini-2.0-flash-lite-preview-02-05 - vertex_ai/gemini-2.0-flash-thinking-exp-01-21\n- RAG Revision #6 - Support for o3-mini, gpt-4.5-preview, claude-3-7-sonnet-20250219, new DeepSeek, Gemini2* and sambanova LLM providers. - New pinecone provider available for [embeddings](447.html) and [rerankers](638.html). - The [CleanUp](60.html) action message has been corrected to clearly specify that it will permanently delete the RAG Assistant files and update the information in the RDS. - Added [usage](712.html) element on every response. - Improvements when changing the [LLM](69.html)/[Embeddings](68.html) settings; all models and providers are normalized to be selected from standard combo-box items; use the [override mechanism](209.html) if you need other options. - Support for [guardrails](840.html). - New [documentAggregation](298.html) property to decide how sources are grouped and returned.\n- It is possible to [provide feedback](170.html) on the response of the Chat with Data Assistant in the Frontend.\n\n- The new [Evaluation APIs](893.html) introduce key functionalities through three interconnected APIs: [DataSet API](839.html), [Evaluation Plan API](849.html), and [Evaluation Result API](856.html). This version is primarily designed for users with a data science profile and is mainly accessed via APIs, complemented by a series of Jupyter notebooks that demonstrate their use. For a comprehensive guide on how to use these APIs, you can refer to [How to evaluate an AI Assistant](881.html) and the [EvaluationAPITutorial.ipynb](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?887,,) notebook, which provide practical examples and code for working through the evaluation process.\n- File attachment support in Flows (version 0.9).\n- Support for Full Story integration in the Workspace/Playground to generate user access statistics in Full Story.\n- In the [LLM API](663.html), for models that have descriptions in the specified languages, the `descriptions` property is included in the Response, which contains the descriptions in the available languages, such as `Spanish`, `English` and `Japanese`.\n- [Data Analyst Assistant 2.0](886.html) version presents important improvements, simplifying the interaction with the data by reducing the main components to just two: **Dispatcher** and **Thinker**. In addition, the metadata structure is automatically generated when loading the datasets, streamlining the setup process. For more information, see [How to create a Data Analyst Assistant](105.html).\n- The option to consult version-specific documentation is now available. Articles with versions show the option \u201cOther document versions\u201d in the header. Clicking on \u201cOther document versions\u201d brings up a menu that allows you to choose between the most recent version (\u201cLatest\u201d) or earlier versions (e.g. \u201c2025-02 or prior\u201d). If you select a version other than \u201cLatest\u201d, a message appears: \u201cThis is not the latest version of this document; to access the latest version, click here\u201d. This message provides a direct link to the most up-to-date documentation. [Components Version Update](822.html) ### February 10th - New documentation with details about [Supported Chart Types](731.html).\n- New [Usage Limits API](802.html).\n- Flows - Logs associated with conversations are accessible from the [conversation history](378.html).\n- RAG Revision #5 - Ingestion supports new [file formats](137.html). For on-premises installations, update the _FILE_TYPES_ parameter. - New optional _cleanProcessing_ parameter when using [Parent Document Retrieval Strategy](204.html). - [Embeddings API](422.html) cache support; disabled by default from a [RAG assistant](68.html). - New [mode](68.html) property for using multimodal embeddings and creating a [Multimodal RAG](783.html).\n- New endpoint [GET /accessControl/apitoken/validate](22.html) returns information about the organization and project associated with the provided apitoken.\n- New LLMs: - Already in production - **OpenAI**: o3-mini - Already in Beta - **DeepSeek**: - deepseek/deepseek-reasoner - deepseek/deepseek-chat - azure/deepseek-r1 - nvidia/deepseek-ai-deepseek-r1 - groq/deepseek-r1-distill-llama-70b - sambanova/DeepSeek-R1-Distill-Llama-70B - Updates in Gemini 2.0 series: - gemini-2.0-flash-thinking-exp-01-21 (Via Providers Gemini and Vertex AI) - gemini/gemini-2.0-flash-lite-preview - gemini/gemini-2.0-pro-exp - vertex_ai/gemini-2.0-flash-001 - sambanova/Llama-3.1-Tulu-3-405B [Components Version Update](822.html) ### January 13th - Internationalization, [Backoffice](42.html), and frontend support for Japanese.\n- Invitations now include information about the organization and project in the subject.\n- New LLMs - Already in Production - **OpenAI**: o1 (2024-12-17 version) - Already in Beta - New Providers: **Cohere** - **Cohere**: Cohere-r\n- [Guardrails](682.html) configured by assistant.\n- [Rerank API](625.html) to semantically order a list of document chunks given a query.\n- New optional RAG [Retrieve and Rerank](620.html) adds an extra layer of precision to ensure that only the most relevant information reaches the model used in the generation step. ## 2024 ### December 12th - [Automatic Creation of Default Assistant](640.html)\n- [Organization Usage Limits](666.html): It is possible to set quota limits to control organization expenses or usage.\n- Chat with Data Assistant - [Show details](170.html) about the generated query in the Playground. - Support in [Chat API](34.html) to interact with Chat with Data Assistant.\n- Flows - Support for markdown when showing the response on the different channels supported by Flows (web, Slack, WhatsApp, and Teams). - New component for connecting flows to the agent overflow console (Human-in-the-loop) via B2Chat. Please read [How to connect a Flow to B2Chat](674.html).\n- RAG - [Chroma](537.html), [Elasticsearch](579.html), [OpenSearch](580.html) vectorstore support. - [Improvements](581.html) when processing PDF documents (update to RAG module revision #4 or higher). - Integration with [AWS Knowledge Bases](577.html) at query time.\n- Data Analyst Assistant - Option to update metadata options. - New version by default in new Data Analyst assistants.\n- New LLMs - OpenAI: gpt-4o-2024-11-20 - AWS Bedrock: Anthropic Claude 3.5 Haiku - Amazon Nova models (Micro, Lite, and Pro) - Llama 3.1 405B on Vertex AI - Beta: - Support for providers Cerebras, SambaNova and xAI (Grok models). - All new Gemini Experimental models.\n- Security - It is possible to define a user 'all-members@domain.com' to give access to the Frontend to all users of the domain '@domain.com'. Permissions are enforced based on the roles assigned to this user. Please read [Allowing Frontend Access for Users from Specific Domains](657.html) ### November 12th - Flows execution integrated into the Playground\n- New [LLMs support](200.html) - **OpenAI**: o1-preview and o1-mini - **Claude Sonnet 3.5 v2 - Providers**: Anthropic, Vertex AI, and AWS Bedrock - **Llama 3.2 models - Providers**: Vertex AI and AWS Bedrock\n- Chat with data assistants - Possibility to edit metadata, [entities](170.html), and attribute descriptions. - The Properties tab has been renamed to [Settings](170.html) along with the options that can be configured in it.\n- RAG - New [returnSourceDocuments](298.html) option to disable returning the documents section used to answer the question. - New [step](298.html) option to use the assistant as a retrieval tool. - Support for [custom history](85.html) in conversations using the _chat_history_ variable.\n- Stand-alone Frontend based on the new Playground UI - Options to customize the Frontend to use the client logo, color palette, welcome message, and descriptions. - Feature to collect feedback (thumbs up/down) in each response. - Google Analytics support.\n- Data Analyst Assistant - Support to upload large CSV files.",
          "evaluationResultRowContextDocId": "68ace6a6-8b1b-4736-a466-9ed0a67b97a9",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "93fdf2e2-0f7f-4226-bc35-8cc852377d24",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?31",
          "evaluationResultRowContextName": "31",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Supported Embeddings Models **List of supported embeddings models** | **Provider** | **Model** | **Multimodal** | **Description** | **Environment Support** |\n|---|---|---|---|---|\n| **openai** | text-embedding-ada-002<br>text-embedding-3-small<br>text-embedding-3-large|No<br>No<br>No<br>No|Must set dimensions to 1536, other values are not enabled by default<sup>(1)</sup> | Beta, QA, Production|\n| **azureopenai** | * | | Same models as openai, configure the associated URL endpoints | Beta, QA, Production |\n| **google/googlevertexai/vertex_ai**<sup>(3)</sup>|multimodalembedding@001<sup>(2)</sup><br>text-embedding-004<br>text-embedding-005<br>text-multilingual-embedding-002<br>|Yes<br>No<br>No<br>No|The dimension parameter must be configured to 768<sup>(1)</sup> or 1408<sup>(2)</sup> | Beta, QA, Production |\n| **awsbedrock** | amazon.titan-embed-image-v1<br>cohere.embed-english-v3<br>amazon.titan-embed-v2<br>cohere.embed-english<br>cohere.embed-multilingual|Yes<br>Yes<br>No<br>Yes<br>No|Check the model dimensions configuration<sup>(1)</sup>| Beta, QA, Production |\n| **nvidia** | baai.bge-m3<br>nv-embed-v1<br>nvclip<br>embed-qa-4<br>nv-embedqa-e5-v5<br>snowflake.arctic-embed-l<br>nv-embedqa-mistral-7b-v2|No<br>No<br>Yes<br>No<br>No<br>No<br>No|Check the model dimensions configuration<sup>(1)</sup> | Beta, QA, Production |\n| **cohere** | embed-english-v3.0<br>embed-multilingual-v3.0<br>embed-english-light-v3.0<br>embed-multilingual-light-v3.0|Yes<br>No<br>No<br>No| 1024 dimensions except for the \"*light*\" models using 384. | Beta, QA, Production |\n| **pinecone**<sup>(2)</sup> | multilingual-e5-large<br>llama-text-embed-v2|No<br>No| 1024 dimensions. | Beta, QA | <sup>(1)</sup> - For [[441]], the associated module must be installed and configured. <sup>(2)</sup> - The configuration must be done in the [[70|Retrieval tab]] by configuring the [[298|Profile Metadata]] parameter. For more information, refer to [[298##Not registered Providers and Models|Not registered Providers and Models]]. Please note that the associated `Embeddings costs` are not recorded, and no related `Requests` are logged as they are processed by the provider. <sup>(3)</sup> - Check the [https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#supported-models|Provider recommendations]. {{{12}}}{{{17}}}\n##MARKDOWN##",
          "evaluationResultRowContextDocId": "9d067949-05dd-404f-8c5a-55e97750d270",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "e5c65eea-df62-444e-87dc-3eaddc4038e6",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?447",
          "evaluationResultRowContextName": "447",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Configuration - General Information [[65]] ## Name\nIdentifying name of the assistant that can be customized according to your preferences. ## Description\nDetailed description of the purpose and capabilities of your assistant. This is a place to provide information about how the assistant enhances the end user experience. ## Status\nIndicates whether the assistant is enabled or disabled for use. ## Embeddings Settings\nAn embedding model is a type of machine learning model designed to convert data\u2014like words, phrases, or sentences\u2014into dense numerical vectors, known as embeddings. These embeddings capture the semantic meaning of the input. Specific parameters related to embeddings and related model characteristics: ### Provider Name\nDetermines the embeddings model service provider used by your RAG assistant. This is a mandatory parameter that can take any of the following values: - **azure**: To use Azure OpenAI. - **cohere**: To use cohere models. - **vertex_ai**: To use Google Vertex AI, check valid models [https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding|here], you can set for example *embedding-001*, *text-embedding-004*, *text-embedding-005* embeddings models<sup>(1)</sup> of 768 Dimensions. - **awsbedrock**: To use embeddings models from AWS Bedrock. - **openai**: To use OpenAI. ### Model Name\nSpecific name of the model being used; [[447|supported values]] depend on the Provider Name selected. For example, if the Provider Name parameter has the \"openai\" value, Model Name takes the value 'text-embedding-ada-002' for assistants created up to 4/11/2024 and 'text-embedding-3-small' onwards. {{{216|If you need to use a model not detailed on the list, use the [[298##Not registered Providers and Models|overide mechanism]]|||}}} ### apiKey\nAPI authentication key provided to access the language model service. For example, if the Provider parameter is \"azureopenai\", you must specify \"apiKey\" with the authentication key. ### Type\nIt allows configuring the information processing approach for the embeddings model. It only applies for the following providers: - CoHere (using awsprovider): `search_document` (default), `search_query`, `classification`, `clustering`. - Google: `RETRIEVAL_DOCUMENT` (default for embeddings), `RETRIEVAL_QUERY` (default for querying), `SEMANTIC_SIMILARITY`, `CLASSIFICATION`, `CLUSTERING`, `TASK_TYPE_UNSPECIFIED` - nvidia: `passage`, `query`; check [https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/task-types|here] for more detail. ### useProxy\nUse Globant Enterprise AI proxy to get embeddings, all requests will be logged (enabled by default). ### cache\nCache the embeddings for the current Organization/Project (disabled by default), needs to enable the `useProxy` property too. ### BatchSize\nDetermines how many items are grouped in each batch when calling the embeddings model during the ingestion process. By default, `batchSize` is set to 100 items. However, you may need to adjust this parameter based on the specific embeddings model you are using. ### Mode\nDefined the mode options are: - `empty` (default) will use text embeddings. - `multimodal` support for image embeddings (notice the model must support it). ### endpoint\nThe URL pointing to the language model service (advanced). ### Dimensions\nIt allows configuring the index dimensionality in the vectorStore. Each model has a specific dimension, and the default value is 1536, associated with the OpenAI provider. <sup>(1)</sup>: Depending on the **Provider Name** and **Model Name** selected for your embeddings, you will need to check the **Dimensions** value. By default, it assumes 1536, which is valid for OpenAI *text-embedding-ada-002*, *text-embedding-3-small* and *text-embedding-3-large* (3072) shrunk to that value. Check the [[140|troubleshooting]] section for more detail. **Notes:** - Please [[274|contact the help desk ]] if you want to use a **Model Name** with a **Dimensions** value other than 1536, as it must match the **Vector Store Provider** configuration in the Index section.\n- If you change the Provider Name, Model Name, Dimensions, it is recommended to reingest all your data. {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "b496e5f2-2bce-4300-aad8-66eb65337440",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "e97658d9-71e0-4c19-b74f-e6bb262a574c",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?68",
          "evaluationResultRowContextName": "68",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.2,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "368bfe92-c540-4847-a5b0-948acf170a63",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0034077200",
      "evaluationResultRowEndDate": "2025-05-13T11:13:09",
      "evaluationResultRowOutput": "AWS Bedrock supports several LLM models in Globant Enterprise AI, including:\n\n- Anthropic Claude 3.5 Haiku (and other Claude 3.x variants)\n- Amazon Titan Express v1\n- Amazon Titan Lite v1\n- DeepSeek-R1 via AWS Bedrock\n- New Amazon Nova models (Micro, Lite, and Pro)\n\nAdditional models related to DeepSeek and Claude 3.7 Sonnet are also supported via AWS Bedrock.\n\nThis information is part of the updates and available models on the platform, showing integration with AWS Bedrock as a provider of LLMs and embeddings.\n\nFor more detailed information and the latest supported models by AWS Bedrock, see the updates log in the 2025 section of the documentation.\n\nReferences:\n- [3](https://wiki.genexus.com/enterprise-ai/wiki?31)\n- [4](https://wiki.genexus.com/enterprise-ai/wiki?31)",
      "evaluationResultRowStartDate": "2025-05-13T11:13:04",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "8087"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#Organization Usage Limits The organization usage limit defines the maximum amount of money allocated for LLM consumption across the entire organization. This feature allows you to set both a soft limit for alerts and a hard limit as the maximum cap, with the option for daily, weekly, monthly renewals or Freemium. For each organization, you can configure quota limits to control project expenses or usage. To access and configure quota limits, you must have SuperAdmin access. ## Access and Limits Configuration To manage quota limits, follow these steps: 1. Enter the [[42]]. You need administrator privileges.\n2. On the left side of the screen, find the Backoffice menu.\n3. Under the SuperAdmin Options group, select the Organizations section.\n4. Use the search field to filter organizations by name. There are two options to configure the filter:<br> - **Contains:** Searches for organizations whose name contains the entered term, such as \u201cDocumTeam\u201d. - **Starts with:** Filters organizations whose names begin with the entered text. 5. Select the desired organization from the filtered list. 6. Click on USAGE LIMITS for the selected organization. [[667]] ## Quota Status Display When you enter the Quota Limit section, you will see a screen with the following information for each quota: [[668]] - **Valid from and Valid until:** Start and end dates of the quota.\n- **Status:** Organizations can operate with or without defined quotas. Possible statuses include:<br> - **No Limits:** No restrictions on consumption (default when no quotas are defined). - **Active:** Quota is in use with an available balance (highlighted in green). - **Expired:** Quota period has reached its end date. - **Empty:** Hard limit of an active quota has been reached. - **Cancelled:** Quota has been manually cancelled (orange dot). - **Type:** Can be daily, weekly, monthly, or Freemium. By choosing Freemium, a one-time or trial usage fee is set, with a specific cost limit and no expiration date. This means that the fee is not renewable and only allows limited access to the service.\n- **Renewable?:** Indicates if the quota automatically renews at the end of its term.\n- **Unit:** Measure unit (USD for organization limits).\n- **Soft limit alert:** Usage limit that triggers an email notification when reached.\n- **Hard limit:** Maximum allowed usage limit.\n- **Used and Remaining:** Current usage and available balance. Once a quota is created using the CREATE NEW button, two additional options become available: [[669]] - **UPDATE:** Allows you to modify the parameters of the active quota.\n- **CANCEL:** This option allows you to cancel the active quota, enabling the creation of a new limit. {{{216|Keep in mind that to update an existing quota, the new Hard limit value must be greater than the previous value. For example, if the previously set Hard limit was 300, when updating it, the new value must be greater than 300, such as 350 or 400. In addition, the Soft limit and Hard limit must be equal to or greater than the sum of all Hard limit and Soft limit quotas per project}}} ## Important Considerations 1. The organization limit acts as a maximum cap for all projects.\n2. When setting an organization limit, ensure it is not lower than any existing project limit, as project limits cannot exceed the organization limit.\n3. If an organization limit is defined for projects without quotas, those projects will start using the organization's account balance.\n4. Projects with specific quotas will respect their limits while adhering to the organization's limits. {{{17}}} {{{12}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "c6e6c48a-93dd-4db5-96e9-d4f9b065a282",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "5527a8ce-c984-4b04-9a2d-235610356809",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?666",
          "evaluationResultRowContextName": "666",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Managing quotas per project For each project, it is possible to configure quota limits to control the project expenses or usage. To access and configure quota limits, you need to be a member of the organization. This means you must be listed under \"Members\" in the [Organization Options](42.html) section. ![Image](./images/573.png) ## [Access and Limits Configuration](#Access+and+Limits+Configuration) To manage quota limits, follow the steps below: 1. Enter the [Globant Enterprise AI Backoffice](42.html). On the left side of the screen, you can find the Backoffice menu. In this menu, under the group of options called ORGANIZATION OPTIONS you will find the Projects section, where you can manage quota limits for each project. 2. Select the desired project and click on USAGE LIMITS. ![Image](./images/78.png) ## [Quota Status Display](#Quota+Status+Display) When you enter the installments section, you will see a screen with the following information for each quota: ![Image](./images/536.png) - **Valid from** and **Valid until**: Start and end dates of the quota.\n- **Status**: Projects can operate with or without defined quotas. If no quotas are defined, the status is **No Limits**, meaning there are no restrictions on consumption. However, once a quota is defined, this status cannot be reverted. When quotas are defined, the project can transition into one of the following statuses: - **Active**: Indicates that the quota is in use and has an available balance. This quota is highlighted in green. Only one quota can be active at a time in a project. - **Expired**: This status is displayed when a quota period definition has reached its end date. If it was marked as renewable, a new registry will be available as Active on the next renewal date. However, if it is not marked as renewable, the project will need a new limit definition to continue operating. - **Empty**: This status occurs when the **Hard limit** of an active quota has been reached, exhausting the available balance. The quota remains in green and is highlighted as active but with a full usage balance. If the quota is marked as renewable, the balance will be automatically restored at renewal time. - **Cancelled**: If a quota has been manually cancelled, it appears with an orange dot. This indicates that you have voluntarily removed the usage restriction to create a new one, or simply that you no longer want it to be active in the project. - **Type**: Type of quota, which can be daily, weekly, monthly, or Freemium. By choosing **Freemium**, a one-time or trial usage quota is established, with a specific limit of requests or costs and no expiration date. This means that the quota is non-renewable and only allows limited access to the service.\n- **Renewable?**: Indicates whether the quota is renewable at the end of its term. If this option is enabled (marked with a check), the quota is automatically renewed when it reaches its expiration date or when the available balance is exhausted (**Empty** status). If the quota is not renewable (no check), the project will require a new quota to be defined manually once the current quota has expired or its balance has been exhausted.\n- **Unit**: Unit of measure, such as requests (Requests) or costs (USD).\n- **Soft limit alert**: Sets a usage limit that, when reached, automatically sends an email notification to the project administrator.\n- **Hard limit**: Defines the maximum allowed usage limit. Once reached, the platform blocks any additional requests and logs an error.\n- **Used** and **Remaining**: Current usage and available balance. ## [Creation of a New Quota Limit](#Creation+of+a+New+Quota+Limit) To create a new quota limit, follow these steps: 1. Click the **CREATE NEW** button on the quota management screen. This will open a configuration window.\n2. Complete the configuration fields according to the required values, such as subscription type, unit of measure, alert limits, and renewal option. ![Image](./images/539.png) If you select **Freemium** as the subscription type, the **Renewable?** option will not be available. This is because Freemium represents a one-time usage or trial limit with no possibility of automatic renewal.\n3. Click on **CONFIRM** to save the quota limit, or **CANCEL** to discard the changes. ## [Actions available for active quotas](#Actions+available+for+active+quotas) In the quotas screen, you can perform the following actions on active quotas: ![Image](./images/541.png) - **UPDATE**: Allows you to modify parameters of the active quota, such as soft limit, enable or disable renewal, and increase the hard limit. If an active quota runs out of balance but is renewable, you can increase the hard limit to restore the balance without creating a new quota.\n- **CANCEL**: This option allows you to cancel the active quota, making it possible to define a new limit instead. This is useful when you want to completely modify the usage restrictions for the project. ### [Example of Updating the Hard Limit](#Example+of+Updating+the+Hard+Limit) If the **Status** of an active and renewable quota changes to **Empty** due to depletion of its balance, you can restore its availability by increasing the **Hard limit** using the **UPDATE** action. ![Image](./images/542.png) ![Image](./images/543.png)",
          "evaluationResultRowContextDocId": "3d3270f0-4ca5-455e-8b7a-58326655cf64",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "8edf5de3-9e5d-46cf-baf8-5afd30379c92",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?71",
          "evaluationResultRowContextName": "71",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Projects Projects are the core entities where several configurations and settings are defined. Within each project, you can define assistants, generate API tokens for API access, and carry out document management functionalities. You can also add, update, or delete projects within the organization. For each project, it is possible to define quotas that limit the consumption of AI models. Additional information is available at [Managing quotas per project](71.html). The following sections will guide you to perform project-related operations within the Globant Enterprise AI backoffice, enabling you to efficiently manage projects and their associated settings. ### [Adding a project](#Adding+a+project) To define a new project, follow the steps below: 1. Access the Globant Enterprise AI backoffice interface and log in with your organization administrator credentials. 2. Once logged in, select \"Projects\" in the left menu, below the ORGANIZATION OPTIONS section. ![Projects Menu](./images/572.png) 3. Click on the \"CREATE NEW\" button. 4. In the project creation form, provide the following information: - **Name**: Enter a meaningful and descriptive name for the project. - **Description**: Briefly summarize the purpose or objective of the project. - **Project administrator (User Email)**: Optionally, you can specify a Project Administrator by entering their user email. 5. After entering the necessary information, click on the \"Confirm\" button to save the project details. Congratulations! You have successfully created a new project using the Globant Enterprise AI backoffice. Now, you can proceed to configure assistants, define API tokens, and manage document management settings for this project. > **Note**: It is recommended to choose project names and descriptions that accurately reflect the intended purpose and scope of the project. This facilitates better organization and improves clarity for all users involved in project management. ### [Set as Active](#Set+as+Active) You have the option to set a certain project as the active project. The \"Set as Active\" function provides the same action as selecting a certain project in the header combo box. To set a project as active, go to \"Projects\" in the left menu. Click on \"UPDATE\" and, in the box below \"Active\", select the option to mark it with a check and activate the desired project. Upon setting a project as active, all subsequent actions and filters within the Globant Enterprise AI backoffice will be applied specifically to that project.",
          "evaluationResultRowContextDocId": "d53b0ada-6af0-42ee-8ba6-e80c603dea20",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "c88ef256-3098-4159-a2b1-04ba5ab9aa16",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?565",
          "evaluationResultRowContextName": "565",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.9,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "3b287605-d74c-494d-95e6-d47f7c3255b9",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0014253600",
      "evaluationResultRowEndDate": "2025-05-13T11:13:14",
      "evaluationResultRowOutput": "You can manage quota limits for each project in the Globant Enterprise AI Backoffice. Specifically, you need to:\n\n1. Access the Globant Enterprise AI Backoffice.\n2. In the Backoffice menu on the left side, under the **ORGANIZATION OPTIONS </B>section, select the **Projects </B>section.\n3. Choose the desired project.\n4. Click on **USAGE LIMITS </B>to configure and manage the quota limits for that project.\n\nThis process requires you to be a member of the organization to have the necessary permissions to access these settings.\n\nThis information is detailed in the \"Managing quotas per project\" section of the documentation.  \n[Source: Managing quotas per project](https://wiki.genexus.com/enterprise-ai/wiki?71) [1](https://wiki.genexus.com/enterprise-ai/wiki?71)",
      "evaluationResultRowStartDate": "2025-05-13T11:13:09",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "3203"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "is defined; for example, \"lastUserInput\". This variable contains the user's most recent input and will be used as the basis for the query on the selected documents or tags. Similarly, in the Output section, there is also a Variable field that specifies the variable in which the output generated by that node will be stored. In this case, we are interested in saving the RAG Assistant\u2019s response in a new variable called lastResponse. Therefore, the Use as response option is disabled. Clicking on the pencil icon to the right of the node opens the \"State Configuration\" menu where you can modify the following items: - **Acceptable confidence level**: The default value is 0.2, which indicates the level of confidence in the match of the chunks that the RAG obtains after searching the vector database. Only responses that meet or exceed this confidence level will be considered valid.\n\n- **Include conversation history**: Enabled by default. It ensures that the conversation history is included in the context of the interaction, allowing the assistant to take previous messages into account when generating responses.\n- **Include context variables**: Enabled by default. It allows context variables stored during the conversation to be sent to the assistants so that they can be used within the prompts.\n- **Show sources**: Enabled by default. It allows the assistant to show the sources of the information used to generate the response, which helps to provide transparency and to validate the accuracy of the response.\n- **Handle empty response**: Disabled by default. It handles situations where the assistant is unable to generate a valid response. When enabled, it opens a new flow branch (as shown in the image), allowing you to configure specific actions to follow in case no response is available. In this new branch, you can add any of the components available in the Interactions menu on the left to continue the interaction as needed. ![Handle Empty Response](./images/337.png) In this case, if the RAG Assistant finds the answer in the documents, you want the flow to call an assistant to translate the response to Japanese and store it in the same input variable (lastResponse). If no answer is found, the flow goes to a second RAG Assistant configured with documents used in Globant Enterprise AI courses. The variable that goes into the RAG Assistant is lastUserInput, which contains the user's last input. The response generated by this RAG Assistant is stored in the lastResponse variable. ![RAG Assistant Configuration](./images/338.png) ### Go To Node Finally, the Go To node is used to connect different flows within the interaction. This node allows you to select the flow to which you want to redirect the end user based on their current interaction. ![Go To Node](./images/339.png) Clicking on the blank bar (corresponding to the drop-down menu) displays a list of available flows: ![Available Flows](./images/340.png) Here you can select and define the flow to which you want to redirect the user. To review or modify any of these flows, you can access the Flows menu on the left side of the screen. In addition, \"Pick from canvas\" allows you to select a node directly from the canvas, facilitating navigation and connection between different parts of the Flow. The 'Go' icon next to the node allows you to navigate directly to the node to which it is connected. In this case, there are two 'Go To' nodes: - The first 'Go To': Returns to the 'User Input' node. Since the previous node (which translates to Japanese) does not store the response in a new variable and simply redirects to the 'User Input', the response received by the end user is the answer to their question translated to Japanese.\n- The second 'Go To': Sends the response to the 'Assistant' node, which is responsible for translating the generated response to Japanese using the lastResponse variable. This variable contains the response obtained from either of the two RAG Assistants previously executed in the flow. In this way, the flow ensures that the end user receives the response in Japanese, regardless of whether the response was generated by the first RAG Assistant or the second. ![Flow Diagram](./images/341.png) ## Step 3: Test flow Once you have configured the flow, make sure to click on the 'Save' button located at the top left of the screen. To test the flow you have created, you have three options available from the same design window: ![Test Flow Options](./images/342.png) 1. Click on the arrow icon (similar to the play button) to run the flow ('Run Flow').\n2. Click on the message icon to open the testing assistant (\u201cOpen Testing Assistant\u201d). ![Testing Assistant](./images/343.png)\n3. Click on the icon to open in a new window (the box with an arrow) to access the demo page (\u201cGo to Demo Page\u201d). ![Demo Page](./images/344.png) The first two options will open a window on the right side of the screen, while the third option will open a new window in the browser.\n\n# How to create a Flow This is a step-by-step guide to create and test an Assistants [Flow](321.html). ## Step 1: Create New First, log in to the [Globant Enterprise AI Backoffice](42.html). In the Project Dynamic combo box, select the project you want to work with (in this case, Default is used). Next, on the left side of the screen, you will find the backoffice menu. In this menu, click on Flows. ![Flow Menu](./images/324.png) By clicking on Flows in the Globant Enterprise AI Backoffice, a new window opens in the browser with the Flow Builder, where the Flows associated with the selected Globant Enterprise AI project are created and managed. When accessing Globant Enterprise AI\u2019s Flow Builder for the first time, a welcome screen like the one shown in the image below is displayed. From this screen, you can start creating a new flow by clicking on the \"Create\" button. ![Welcome Screen](./images/325.png) When you click on \"Create\", a pop-up window opens in which you must fill in the following information: 1. **Name**: Descriptive name for the flow, allowing it to be easily identified.\n2. **Description**: This field is optional, but it is recommended to add a brief description of the flow to clarify its purpose or content.\n3. **Type**: Allows you to select the type of flow to create, with the options \"Chat Flow\" or \"Assistant Flow\". Selecting one of these types will determine specific configurations and characteristics of the flow. In this case, Assistant Flow is selected.\n\n4. **Select languages**: Sets the language in which the flow will be configured, and allows you to define the language of the hard-coded messages. Multiple languages can be selected, so the same message can be available in different languages. Once you have completed these fields, you can click on the \"Create\" button. ![Create Flow](./images/326.png) ## Step 2: Manage the Start flow Once you have clicked on \"Create,\" you will return to the welcome screen. There, click on the \"Manage\" button of the newly created flow to add the assistants you want and customize their behavior. ![Manage Flow](./images/327.png) Clicking on the \"Manage\" button displays the first flow, which is created automatically. This initial flow, called \"start,\" establishes a basic configuration that you can customize, adding or modifying interactions, messages and assistants according to your needs. ![Start Flow](./images/328.png) ### Start flow The initial flow, called \"start,\" is automatically generated with the ID input.welcome. ![Start Flow ID](./images/329.png) ### Start (Intent: input.welcome) Node The Start (Intent: input.welcome) node marks the beginning of the conversation flow. This node is activated every time the flow is triggered, and its main purpose is to start the flow and, from there, the other nodes that manage the interaction with the user are connected. To rename the start node, go to the left menu, find Flows > start, click on the three dots next to it, select \"Rename Flow\" and, after typing the new name, press Enter to save the change. ![Rename Flow](./images/330.png) ### Message Node The next node, Message, is in charge of sending the welcome message or any other initial message configured. In this case, the message is: \"Hello {name}, how can I help you?\". Here, {name} represents a dynamic variable that is filled with the user's name. The message displayed in this node is customizable, and the text can be formatted by selecting different styles, such as Normal, Heading 1, among others, from the drop-down menu that appears where it says \"Normal\". This allows you to adjust the text style according to your needs. ![Message Node](./images/331.png) On the top bar of the node, you can see several formatting buttons such as bold, italic, underline, among others, to customize the text. In addition, there are icons that allow you to add lists, links and emojis. The last button, which has the shape of a small drawer, allows you to insert dynamic variables in the message. Clicking on this button displays a list of the variables available in the system, and it is also possible to define new custom variables. This Message node can be deleted by clicking on the trash can icon to the right of the node. It is also possible to add a variant by clicking on the \"plus\" (+) button at the top right. ### User Input Node The flow then proceeds to capture the user input through the User Input node, storing it in the lastUserInput variable for later use. ### Assistant Node In the Assistant node, you must configure the assistant to perform specific actions. This node uses the information stored in the lastUserInput variable, which contains the text entered by the end user in the User Input node. ![Assistant Node](./images/332.png) To configure the assistant, click on the \"Select an assistant\" field. By default, \"No assistant selected\" will be displayed. From the drop-down menu, choose the assistant you wish to use. **Note**: The assistants available are those that have been defined in the associated project, which was configured in the first step. The assistants can be any of those defined in Globant Enterprise AI, including [Data Analyst Assistant](886.html), [API Assistant](110.html), [Chat with Data Assistant](159.html). Once selected, the Prompt field will display the message that defines the task of the assistant. This message is read-only and can\u2019t be edited. The assistant's response can be stored in the lastUserInput variable or in a new variable, depending on your needs. By default, the Use as response option is enabled, which causes the assistant's response to be used directly in the lastUserInput variable. If you prefer to store the response in a new variable, disable Use as response. This will enable the Output field, where you can select or create a new variable. ![Assistant Output](./images/333.png) In addition, you can enable the option to read the assistant's response in JSON format. By enabling this option, you will have the ability to map the fields of that JSON to variables previously defined in your configuration. This allows for more precise integration of the assistant's responses in more complex workflows or in systems that require a specific format for data handling. The pencil icon to the right of the node corresponds to the Edit button. By clicking on it, you can enable or disable the Include conversation history and Include context variables options. By default, Include conversation history is enabled. It allows the assistant to access the conversation history to generate more coherent and contextual responses. The Include context variables option is also enabled by default. This option sends to the assistant variables defined in the flow, such as previously captured data or configured values, so that they can be used within prompts. This node can be deleted by clicking on the trash can icon to the right of the node. In this case, the Assistant node is deleted: ![Delete Assistant Node](./images/334.png) ### Insert RAG Assistant or Assistant Node On the left menu, below the Interactions category, you will find the different components that can be added to the flow. In particular, in the GENERATIVE section, you can choose between adding an Assistant or a RAG Assistant. ![Insert RAG Assistant](./images/335.png) In this case, you need to add a RAG Assistant. To do so, click on the RAG Assistant option in the GENERATIVE section. Next, drag the component to the desired location in the flow. In this case, you can insert it between the User input and Go To nodes. When you add a new RAG node, you must click on the \"Select an assistant\" field. By default, the option \"No assistant selected\" will appear. From the drop-down menu, choose the RAG assistant you wish to use. Once you select the RAG assistant, the next field, \"Select a document\", allows you to choose a specific document that will act as the source for responding to the end user's request. If you select a document, the assistant will only use that document as a reference. ![Select Document](./images/336.png) Alternatively, you can use the \"Indicates the documents' tags\" field. Here you can add the tags defined in [Step 2: Upload RAG documents](60.html), which are used to filter the documents. This allows the assistant to search only in the documents that match the indicated tags to respond to the end user's request. In the Input section, a field called Variable is displayed. There, the variable to be used as input for that node",
          "evaluationResultRowContextDocId": "09ebe65a-fb9d-4f45-8146-084558fef786",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "6dd5bfef-430c-4901-bcb6-5657fda1f98c",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?322",
          "evaluationResultRowContextName": "322",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "an appropriate name could be: ConvertArticleToMarkdown * **Task description:** Provide a brief description of what the task does. For example: Convert a documentation article to Markdown format, applying correct structure, headings, and style conventions. * **Task context:** Include relevant information that gives context to the task, such as details of the overall process or considerations that the Agent must take into account. For instance: This task is part of a documentation workflow in which generated articles must be transformed into Markdown to ensure consistency across publications. The Agent should format the content according to standard Markdown rules, maintaining clarity and proper use of headings, lists, and emphasis. * **Task instructions:** Specify clear instructions that guide the Agent in the execution of the task. These instructions are part of the prompt that will be sent to the Agent. For example: Take the provided article and convert it into a well-structured Markdown document. Use appropriate Markdown syntax for headings, bold, italics, bullet points, and code blocks where needed. Make sure the content remains clear, professional, and aligned with documentation standards. If you click on the expand icon, a larger editing window will open to make typing easier. ### Examples Examples allow you to define input/output pairs that help the Agent understand what kind of result is expected. This is especially useful in tasks involving content transformation, such as converting text to Markdown. [[997]] To add an example, click on the **\\+ Add** option under the Examples section. Two fields will appear: * **Enter the example input**: where you provide a sample of the original content. * **Enter the example output**: where you show how that content should look after processing. Although this section is optional, it is recommended to include at least one example when the output must follow a specific structure or formatting style. In this case, since the task consists of converting an article to Markdown, a useful example would be: * **Example input**: `This is the introduction. It explains how the system works and what the key components are. Features include: easy setup, automatic updates, and compatibility with all major platforms.` * **Example output**: `# Introduction` `This is the introduction. It explains how the system works and what the key components are.` `## Features` `- Easy setup` `- Automatic updates` `- Compatibility with all major platforms` ### Inputs Although it is not mandatory in this case, you can indicate an artifact that works as input if this task must work with a previously generated content. Insert **Text** as the input artifact. This artifact represents the content provided by the user when triggering the Agentic Process. Since this is the initial material to be processed, it must be configured as the input of the task. ### Outputs This field is mandatory to ensure that the task generates an output that can be used by the following tasks in the process. In this case, since the task involves validating a documentation article, a suitable output could be an artifact of type **MarkdownArticle**\u2014or similar\u2014containing the corrected and validated content. Once you have configured all the necessary fields, the task node will be displayed as follows: [[998]] ## Step 7: Add a Second Task To continue building the process, add a second task node following the same procedure described earlier. This task uses the **Expert Translator** agent to translate the article\u2014 already converted to Markdown\u2014into the target language specified by the user. #### **Configure the task:** * **Agent**: Click on the **Select** link in the Properties panel and choose the **ExpertTranslator** agent from the list. * **Task name**: `TranslateMarkdownArticle`\n\n* **Task description**: Translate a Markdown article into the target language provided by the user. * **Task context**: This task is executed after the article has been reformatted to Markdown. Its purpose is to make the content accessible in the language selected by the end user, while maintaining structure and readability. * **Task instructions**: Translate the provided Markdown content into the target language `{{language}}`. Make sure to preserve the original formatting, structure, and headings used in the Markdown document. The output should remain clear, professional, and easy to read. **Note**: The `{{language}}` syntax refers to a process variable. You must define this variable by clicking on the process name (in this case, Documentation Assistant) at the top of the screen, selecting \"Edit process variables\", and adding a new variable named `language` of String type. [[999]] [[1001]] * **Input**: Select `MarkdownArticle`, which is the output of the previous task. * **Output**: Define an artifact named `TranslatedMarkdownArticle`. ## Step 8: Add an event Once you have configured the tasks, you can continue building the flow by adding a new task or event. To add a new task, follow the same procedure explained in the previous step: go to one of the connection points of the current node (for example, the one at the bottom), click on the \\+ icon and select Task. If you want to add an event instead, click on the same \\+ icon and select the Event option from the context menu. This action will insert an event type node that can be used to handle intermediate situations within the process. [[1002]] After selecting Event, a new node is inserted and automatically connected to the task. The right panel (Properties) opens automatically, allowing you to define the event that will trigger the next action. In this panel, complete the following fields: [[1003]] * **Signal name**: Enter a descriptive name for the event signal (e.g., `MarkdownReady`). * **Type**: In this field, a dropdown menu will appear with two options: `User` and `Artifact.` In this case, you must select `User` because the event will be triggered when a specific artifact is generated, acting as a continuation signal within the process. * **Inputs**: When you select `Artifact`, the *Inputs* section is enabled automatically. Here, you must define which artifact should be available for the event to be triggered. This behavior is similar to how the *Start* node works when configured with the *Artifact* type. The event node acts as an intermediate pause in the process and can be triggered either by the generation of a specific artifact or by a user action. In this case, the **Type** is set to `User`, which means the process will wait until the end user confirms that they want to proceed with the next step. ## Step 9: Add the Finish Node To complete the flow, add a **Finish** node after the event by clicking on the **\\+** icon and selecting **Finish**, just as you did for the previous elements. [[1004]] ## Step 10: Test the Agentic Process Once the flow is complete, you can test how the process behaves from an end-user perspective. Click on the **Run Test** button at the bottom right of the screen. This opens the testing panel, where you can simulate the interaction with the Agentic Process. [[1005]] In the testing panel: * Fill in the **Subject** field with a brief title or description.\n* Upload a document in the **Artifact (Text)** field. This document will be used as the input for applying the Globant writing style and converting it to Markdown.\n* Enter the **Language** you want the final output translated into (e.g., English, Portuguese, etc.). [[1006]] Once all fields are completed, click on **Run** to execute the process. After clicking on **Run**, the Agentic Process begins execution. It processes the input using the first task and pauses when it reaches the **event node**. At this point, the process waits for user confirmation to proceed. As shown in the image, a **MarkdownReady** section appears with a **Send** button next to it. [[1007]] To continue and complete the process, the end user must **click on the Send button**. This confirms that the Markdown version is ready and triggers the next step in the workflow, enabling the translation task to run and allowing the process to reach the **Finish** node. ## Step 11: Publish the Process Once the entire Agentic Process has been configured and tested, the final step is to publish it. To do this, click on the **Publish** button located in the bottom-right corner of the screen. This action makes the process available in the Playground, where it can be accessed and used like any other assistant. ### Delete a node To delete a node you must click on it. You will know that the node is selected when you see the circles around it. Then press the Delete key on your keyboard to remove it from the flow. ## Availability Since April 2025 release. {{{12}}}{{{17}}}##MARKDOWN##\n\n#How to create an Agentic Process An Agentic Processes represents a sequence of tasks that must be executed to achieve a specific result. These tasks may involve interactions with users, AI Agents and external tools. Agentic Processes are organized as a logical flow that can be initiated by an end user action or by the availability of a previously generated artifact. This is a step-by-step guide to create, test, and publish an Agentic Process that connects two AI Agents: the first one applies the Globant writing style to a user-provided text and converts it into Markdown format; the second one takes that Markdown content and translates it into the target language selected by the user. ## Step 1: Access The Lab First, log in to the [[42]]. In the Project Dynamic combo box, select the project you want to work with; in this case, Default(Docum Team) is used. Next, on the left side of the screen, you will find the Backoffice menu. In this menu, click on **The Lab.** [[975]] By clicking on The Lab in the Globant Enterprise AI Backoffice, a new window opens in the browser with [[972]]. ## Step 2: Go to the Agentic Processes Dashboard Once inside Globant Enterprise AI Lab, you will see the Side Navigation Menu on the left edge of the screen. If the menu is collapsed, click on the arrow icon to expand it. [[986]] In this menu, select the Agentic Processes option to access the Agentic Processes Dashboard. If no Agentic Processes have been created yet, a message saying \"No Agentic Processes Created\" will appear. Below that message, click on the New Processes button to start creating a new process. ## Step 3: Fill in the Process Details After clicking on the New Process button, a pop-up window titled Process Details will appear on the screen. Here you must complete the following fields: [[987]] * **Process Name:** Enter a unique and descriptive name for the process. * **Process Description:** Optionally, include a brief explanation of what the process is intended to do. Once the information is complete, click on **Save** to continue. ## Step 4: Set the Initial Signal After saving, a new tab with the name of your process opens automatically in the Agentic Processes dashboard. This tab shows a grid where you can design the flow of your process. By default, it includes a **Start** node at the center of the screen. This node represents how the interaction begins. [[988]] Click on **Start** to open the **Properties** panel on the right side of the screen. [[989]] In the **Signal name** field, you can define a label for the trigger, and in the Type dropdown, you must select one of the following options: * **User:** The process starts based on a user-initiated action. * **Artifact:** The process is triggered when a specific artifact is created. If you select **Artifact**, an additional **Inputs** section appears, where you must define a variable representing the artifact that should trigger the process. [[990]] An artifact is the (intermediate or final) result produced by a task within the process. It can be, for example, a generated document, a response produced by an Agent, or any other type of structured content that represents a deliverable within the process flow. Click on **Add+** and select the appropriate artifact from the dropdown menu. This instructs the process to begin only after that specific artifact is available. In this case, the **Signal name** is set to `UserInput`, and the **Type** selected is **Artifact**. In the **Inputs** section, `Text` is added. This configuration is used because the *Subject* field that end users can fill when interacting with the Agentic Process is very limited in length. Therefore, if the user wants to apply the Globant writing style to a longer piece of content, that content must be uploaded as a file artifact. ## Step 5: Add a Task Node To add a task node, move your cursor over the **Start** block. When you do this, you can see four circular anchor points around it (top, right, bottom, left). Hovering near any of these points displays the \\+ icon. [[991]] Click on the \\+ icon that appears when hovering over the anchor point of your choice. When you do this, a context menu appears with three options: **Task**, **Event**, and **Finish**. Click on **Task** to add a new task node to the process. This task represents the first action to be executed once the process is triggered. Once the task is added, the node appears and automatically opens the **Properties** panel on the right side of the screen, which allows you to configure the details of the task. [[992]] In addition to creating new tasks by clicking on the **\\+** icon next to a node, you can also add a task by dragging it directly from the **Toolbox**. To do this, go to the **Toolbox** (located in the bottom left corner) and click on **Task**. Then, drag the new task node to the desired position on the canvas. Once placed, connect the task to the rest of the flow by clicking on the **\\+** icon from the node you want it to connect to. A dotted line will appear, indicating the connection. Once confirmed, the flow will integrate this task into the process. ## Step 6: Configure the Task A task in an Agentic Process represents a unit of action that is executed as part of the overall flow. It may involve different activities, such as invoking an AI Agent, requesting information from a user, executing an external tool, processing data, or generating an artifact as an output. Each task can have inputs (for example, artifacts generated by previous tasks or user data) and outputs (new artifacts or actions that trigger subsequent tasks). To configure the task, fill in the available fields in the Properties panel: [[993]] * Agent * Task name * Task description * Task context * Task instructions * Examples * Inputs * Outputs ### Select an Agent At the top of the Properties panel, within the Agent block, click on the Select link. This action opens a pop-up window listing all the Agents previously defined in AI Lab for the current project. [[994]] In this window, you can use the search field to filter Agents by name or description. When you select one of them, its detailed information is displayed on the right side, including: [[995]] * Agent name and description * AI model used * Reasoning strategy configured * Tools Once you've confirmed that this is the Agent you want to use, click on the Select button. The Agent is now assigned to the task, and its name will appear in the Properties panel instead of \u201cSelect\u201d. This task uses the **Documentation Assistant** agent to apply Globant\u2019s writing rules and convert the text into Markdown format. ### Complete the task fields In the Task section, you must configure the following fields. They allow you to define precisely what the task should do and are combined with the Agent's capabilities to execute the corresponding action. [[996]] * **Task name:** Specify a clear name that identifies the task. In this case, since the goal is to review the article content,",
          "evaluationResultRowContextDocId": "05b50d0d-54f6-40f4-869d-d16f7160191e",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "ecb4b0f8-edbe-4ff1-b624-e3c9a3dba651",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?985",
          "evaluationResultRowContextName": "985",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "* **Body**: This section allows you to write the content of the email. You can format the text using basic editing options such as bold, italics, underline, lists and links. You can also insert emojis or variables. ### Spreadsheet Allows you to write data into a spreadsheet directly from a flow. You can use this node to store user responses or any other information in real time, organizing the data in specific columns of the spreadsheet. [[508]] To configure the Spreadsheet node, click on the Edit icon located on the right side of the node. Clicking on Edit will open a side menu where you can enter the Spreadsheet ID and the Sheet name. The Spreadsheet ID is obtained from the URL of the Google Spreadsheet, and the Sheet name corresponds to the specific tab within the file where you want to save the data, such as \"Sheet 1\". In addition, make sure to upload the JSON file with the service account credentials and that the service account has permissions on the spreadsheet. The node displays a drop-down field where you can select the variables to be written to the spreadsheet. You must arrange the variables in the order in which they appear in the spreadsheet. You can also add other context variables if necessary. ## RESPONSES The Responses section contains the nodes responsible for generating responses to the end user. [[509]] ### Message Sends custom messages within a flow, such as an initial greeting. For example, you can use: \"Hello {name}, how can I help you?\", where {name} is a dynamic variable that is filled with the user's name. [[510]] This message is fully customizable. You can format the text using different styles, such as Normal or Heading 1, and formatting tools such as bold, italics, lists, links and emojis. In addition, you can insert dynamic variables, and add lists, links and emojis. You can also add message variants. ### Image Adds images to interactions within a flow, displaying a space where you can upload or select an image. [[511]] In addition, you can make the content dynamic by clicking on the nut-shaped icon, allowing you to use Velocity Template Language (VTL) to create dynamic responses, adapting the image content to different contexts. Clicking on Add Image brings up a menu on the left side that allows you to upload an image by dragging it or by browsing for the file on your device. You can also enter a URL directly to use an online image. This menu offers options for adjusting the size of the image, including cropping or scaling the image, and setting its vertical and horizontal position, centering it as needed. ### Link Create rich links with a custom preview that includes a title, description, image and tag, providing additional context to engage the user. [[512]] If you choose to add an image, clicking on Add Image will open a menu on the left side where you can upload an image from your device or enter a URL to use an online image. This menu also allows you to adjust the size, vertical position and horizontal position of the image, in a similar way as with the Image component. In addition, you can make the content of the link dynamic by clicking on the nut-shaped edit button. This will allow you to use Velocity Template Language (VTL) to adapt the preview to different contexts and situations within the flow. ### Card Create interactive cards within a flow, where you can add an image, title, and short description. As in the Image and Link components, you can add an image by clicking on Add Image, which will open a menu on the left side with the same options as the Image and Link components. [[513]] The Card node includes one button by default, and you can add more if necessary. If you add several buttons, each one can be configured to connect to an External function or External link, or to define an event. When all buttons are configured as External function or External link, the flow follows a single branch. However, if any of the buttons is configured for an event, an additional branch will be generated in the flow for each button configured for events. This node allows you to enable or disable text interaction, keep the conversation active, or allow user input. You can also make the content dynamic in order to adapt the card to different contexts within the flow using Velocity Template Language (VTL). ### Video Adds the URL of a video, but YouTube videos are not supported. You can also paste the URL of an image in the Poster field to use it as a thumbnail or cover of the video. [[514]] It is possible to make the link content dynamic using Velocity Template Language (VTL) to adapt the video preview to different contexts and situations within the flow. In addition, the component offers the option of adding buttons to connect to an external link, define a specific event or action, and set variables that can be used as input in other interactions within the flow. Each additional button can create a new branch in the flow, unless it is configured to connect to an External function or External link, in which case it will follow the same branch. ### YouTube Allows you to insert YouTube videos into your flow. [[515]] To set up this node, paste the ID of the video in the \"Id\" field in the left menu, which appears when you click on the edit icon. Make sure to enter only the part of the URL that comes after the equal sign (=). For example, if the URL is `https://www.youtube.com/watch?v=h_5nxFWjmZc`, just enter `h_5nxFWjmZc` in the corresponding field. The video thumbnails are generated automatically, so there is no need to add an additional image. The component also offers the option to add as many buttons as you deem necessary. In addition, you can make the content dynamic using Velocity Template Language (VTL), which allows you to adapt the video presentation to different contexts and situations within the flow. ### Quick Reply Adds buttons that you can use to start actions or redirect to other flows. Each button is configured by entering its name and selecting an action option. The Event option allows you to direct the reply to the desired flow. With External Link, you can redirect the user to a website, taking them out of the current flow. Finally, the External function allows you to execute an external function. [[516]] This node also offers additional behavior settings. You can disable text interaction, keep the conversation active, or enable user input. By default, the Enabled User Input option is active, which means that, if you add multiple buttons, a branch will be generated in the flow for each button configured as an Event. Buttons configured as External function or External link will not create new branches and will follow the same path in the flow. ### Datepicker Allows you to select a date within a flow, displaying fields to choose the month, day and year. [[517]] It is possible to configure the format in which the date will be displayed, select which dates will be available, connect the date picker to a specific event, and assign",
          "evaluationResultRowContextDocId": "26b674b4-d387-4cb4-a16f-b97b7e0d8204",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "fad4a33b-4299-424c-840a-d8e9aad83bcc",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?494",
          "evaluationResultRowContextName": "494",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.8,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "3bfbb00c-d334-423d-afaa-b49d1e40c3ea",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0041817800",
      "evaluationResultRowEndDate": "2025-05-13T11:13:24",
      "evaluationResultRowOutput": "Un **Flow </B>en Globant Enterprise AI es una secuencia de nodos que gestionan la interacci\u00f3n con el usuario mediante mensajes, la captura de informaci\u00f3n, el uso de asistentes, y otros componentes para brindar una experiencia conversacional o resolutiva. Sirve para dise\u00f1ar y controlar c\u00f3mo se desarrolla una conversaci\u00f3n o proceso de asistente virtual.\n\n### Para qu\u00e9 sirve un Flow\n- Permite organizar y dise\u00f1ar la estructura de la interacci\u00f3n con el usuario.\n- Controla el flujo de mensajes, preguntas, respuestas y acciones automatizadas.\n- Puede incluir nodos para mensajes, entrada de usuario, asistentes, RAG Assistants, multimedia, botones interactivos, y otros componentes.\n- Gestiona variables y contexto para que las respuestas sean coherentes y adaptadas.\n- Facilita la creaci\u00f3n de asistentes con capacidades personalizadas y multiling\u00fces.\n\n### C\u00f3mo crear un Flow\n1. Ingresa al **Globant Enterprise AI Backoffice </B>y selecciona el proyecto en el combo \"Project Dynamic\".\n2. En el men\u00fa lateral, selecciona la opci\u00f3n **Flows </B>para abrir el Flow Builder.\n3. Haz clic en el bot\u00f3n **Create </B>para iniciar un nuevo Flow.\n4. En la ventana emergente, completa los siguientes campos:\n   - **Name**: Nombre descriptivo del Flow.\n   - **Description**: (Opcional) Breve descripci\u00f3n del prop\u00f3sito del Flow.\n   - **Type**: Selecciona el tipo de Flow, por ejemplo, **Assistant Flow </B>para asistentes conversacionales.\n   - **Select languages**: Define el idioma o los idiomas en que el Flow estar\u00e1 configurado.\n5. Haz clic en **Create </B>para generar el Flow.\n6. Luego, accede al Flow para **Gestionar </B>y agregar nodos como mensajes, entrada de usuario, asistentes, etc.\n7. Configura cada nodo seg\u00fan las opciones disponibles para definir su comportamiento, variables, y conectividad.\n8. Puedes renombrar el nodo inicial \"start\" seg\u00fan convenga y personalizar los mensajes iniciales.\n9. Finalmente, guarda y prueba el Flow para validar que funcione seg\u00fan lo esperado.\n\nEste proceso permite dise\u00f1ar conversaciones din\u00e1micas y personalizadas para distintos casos de uso, con soporte para m\u00faltiples idiomas y usos avanzados mediante la integraci\u00f3n con asistentes AI.\n\nToda esta informaci\u00f3n est\u00e1 descrita en detalle en la documentaci\u00f3n oficial: [Gu\u00eda para crear un Flow](https://wiki.genexus.com/enterprise-ai/wiki?322) y [Interacciones para crear un Flow](https://wiki.genexus.com/enterprise-ai/wiki?494) [3][4].\n\n---\n\nSi necesitas ayuda con la configuraci\u00f3n de nodos espec\u00edficos o c\u00f3mo probar un Flow, puedo asistirte tambi\u00e9n.",
      "evaluationResultRowStartDate": "2025-05-13T11:13:14",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "8968"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# How to expose a Flow through WhatsApp Client Once you have created your first Flow in Globant Enterprise AI, following the steps in [How to create a Flow](322.html), you may be interested in exposing it in various communication and management platforms. Here is a step-by-step guide on how to expose a Flow through WhatsApp client. ## Step 1: Create an Account and Application on Facebook Business To get started, go to [Facebook Business](https://business.facebook.com/business/loginpage/?next=https%3A%2F%2Fdevelopers.facebook.com%2Fapps) and [register as a Meta developer](https://developers.facebook.com/docs/development/register?locale=en_US). From there, you will be able to manage applications and access the WhatsApp Business API. Once you have your Meta developer account, go to the Facebook Developer Portal and select the option to [create an application](https://developers.facebook.com/docs/development/create-an-app). Next, follow the steps below: 1. Click on **Create App** ![Create App](./images/550.png)\n2. In the pop-up window, click on **Select an app type** to define the app type. ![Select an app type](./images/551.png)\n3. Choose **Business** as the application type, and click on **Next** to continue. ![Choose Business](./images/552.png)\n4. In the window that opens, add a name for your application. Then click on **Create app**. ![Create app](./images/553.png)\n5. Go to the **Add products to your app** section. Select **WhatsApp** and click on **Set up**. ![Set up WhatsApp](./images/554.png) ## Step 2: Obtain WhatsApp API Credentials Next, you will need to configure the [WhatsApp Business API](https://developers.facebook.com/docs/whatsapp/business-management-api?locale=en_US). Go to WhatsApp > API Setup and get the following data: ![API Setup](./images/555.png) - **Phone number:** This number will be associated with the WhatsApp Business account for sending and receiving messages. You will need to verify this number in the WhatsApp Business settings for it to be enabled. To associate your phone number, follow the steps in [How to connect your phone number to your WhatsApp Business account](https://www.facebook.com/business/help/456220311516626?id=2129163877102343).\n- **Phone number ID:** This ID is unique to the phone number you have registered. You can find it in the Send and receive messages section.\n- **WhatsApp Business Account ID:** This ID is also available in the Send and receive messages section.\n- **Access Token:** This token authenticates continuous requests, such as sending and receiving messages, ensuring that only authorized applications access the WhatsApp API for each interaction. Go to App Settings > Basic to get: - **App Secret:** This key authenticates the identity of your application during the initial configuration and adds a layer of security in the generation of access tokens. ![App Secret](./images/556.png) ## Step 3: Flow configuration Go to the Flow you wish to expose. Next, access Side Navigation Menu > [Configuration](371.html) > Integrations > Whatsapp Client and fill in the required data: - WhatsApp Endpoint: This field should include the URL of the WhatsApp API endpoint, usually https://graph.facebook.com/vXX.0, where XX represents the API version you are using (e.g. v17.0).\n- WhatsApp App secret = App Secret\n- WhatsApp JWT = Access Token\n- WhatsApp phone number = Phone number\n- WhatsApp phone ID = Phone number ID\n- WhatsApp Business Account ID = WhatsApp Business Account ID\n- WhatsApp verify = Any text Once you have completed all the fields, click on **Confirm**. ## Step 4: WebHook configuration Go to WhatsApp > Configuration and in the Callback URL field enter the following link: `https://botbuilder.fluentlab.ai/fluentlab/whatsapp/<Bot-Id>`. To get the `<Bot-Id>`, go to the environment where you are building the Flow. In the Side Navigation Menu, click on the [Overview](371.html) option. In the Verify Token field, enter the same text you set up earlier in WhatsApp Verify. ![Verify Token](./images/557.png) After entering the **Callback URL** and **Verify Token**, click on the **Verify and Save** button to complete the verification. Once you have verified the configuration, in the same section, look for the list of **Webhook fields**. Here, make sure that the **messages** field is set to **Subscribed**. This allows WhatsApp to send notifications about messages to your application. ![Webhook fields](./images/558.png) > **Note**: Keep in mind that exposing a flow via WhatsApp may involve additional costs. See details in [Conversation-Based Pricing](https://developers.facebook.com/docs/whatsapp/pricing?locale=en). ## See Also [WhatsApp Business Platform](https://developers.facebook.com/docs/whatsapp?locale=en_US)",
          "evaluationResultRowContextDocId": "5574ccc7-e940-48f9-a11b-76f7e189817c",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "0d02b112-1772-4f14-991c-7a53d768d044",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?549",
          "evaluationResultRowContextName": "549",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# How to expose a Flow through WhatsApp Client via B2Chat Once you have created your first Flow in Globant Enterprise AI, following the steps in [How to create a Flow](322.html), you may be interested in exposing it in various communication and management platforms. Here is a step-by-step guide on how to expose a Flow through WhatsApp Client via B2Chat. ## Step 1: Create an application key 1. Access the Flow you wish to expose. In the Side Navigation Menu, select [Configuration](371.html) and then Application Keys.\n2. Click on the Generate Key button.\n3. In the Type field, select API and in Key Name, enter a name such as \"B2ChatIntegration\". Next, confirm the operation by clicking on Save.\n4. Copy the generated key using the copy icon next to the key. For more details, see **Step 2: Create an application key** in the complete guide [How to connect a Flow to B2Chat](674.html). ## Step 2: Get the Bot id Go to the Side Navigation Menu, and click on the [Overview](371.html) option in the Overview section. There you will see a summary of the Flow including the Bot id, along with other details such as the Flow's name and purpose. The Bot id is the alphanumeric identifier that appears below the name of your Flow. ![Bot id](./images/395.png) Copy the Bot id. ## Step 3: Build B2Chat webhook B2Chat uses this URL to answer messages and reach the bot. The URL format is: `https://botbuilder.fluentlab.ai/fluentlab/whatsappb2c/<bot-id>?apikey=<api-key>` - `<bot-id>`: This is the identifier you obtained in Step 2: Get the Bot id\n- `<api-key>`: This is the application key you generated in Step 1: Create an application key. ## Step 4: Request B2Chat Credentials To connect your Flow to WhatsApp via B2Chat, you must [contact Globant Enterprise AI support](274.html) and provide the **B2Chat webhook** built in Step 3: Build B2Chat webhook. With this information, they will provide you with the necessary credentials for the integration, which include: - **B2Chat Client ID:** Unique client identifier to authenticate the connection.\n- **B2Chat Signing Secret:** Signing Secret to verify the integrity of incoming and outgoing messages.\n- **B2Chat Endpoint:** URL of the B2Chat endpoint for API requests.\n- **B2Chat Security Endpoint:** URL of the B2Chat security endpoint, which is used for additional authentication and security verification. ## Step 5: Configure the Integration in Globant Enterprise AI Go to the Flow you want to expose. Then go to **Side Navigation Menu** > [**Configuration**](371.html) > **Integrations** > **WhatsApp Client via B2Chat** and fill in the fields with the provided data: - B2Chat Client ID\n- B2Chat Signing Secret\n- B2Chat Endpoint\n- B2Chat Security Endpoint To enable push notifications, select the **Push notification allowed** checkbox. Lastly, click on **Confirm**. > **Note**: Keep in mind that exposing a flow via WhatsApp may involve additional costs. See details in [Conversation-Based Pricing](https://developers.facebook.com/docs/whatsapp/pricing?locale=en).",
          "evaluationResultRowContextDocId": "4cef5feb-9248-444a-875b-b3d57869ea6b",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "9648c70c-8db2-44a7-9b4c-0b65223bf9b2",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?548",
          "evaluationResultRowContextName": "548",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# How to expose a Flow through Teams Once you have created your first Flow in Globant Enterprise AI, following the steps in [How to create a Flow](322.html), you may be interested in exposing it in various communication and management platforms. Here is a step-by-step guide on how to expose a Flow through Teams. ## Step 1: Create a Teams bot Go to [Teams developer portal](https://dev.teams.microsoft.com/). There, click on **Tools**, select the **Create your first bot** button and fill in the bot name. ![Image](./images/392.png) ## Step 2: Configure end point Once you have created the bot, go back to the main **Tools** screen and select the newly created bot; in this case, **Documentation Assistant**. ![Image](./images/393.png) When you click on it, a window will open with a menu on the left side containing the following options: ![Image](./images/394.png) Configure, Channels, Meeting event subscriptions and Client secrets. In the **Configure** option, fill in the **Endpoint address** field with the following URL: `https://botbuilder.fluentlab.ai/fluentlab/api/v1/webhook/teams/<bot-id>`. In this URL, the `<bot-id>` segment must be replaced with the ID of your Flow. To get the `<bot-id>`, go to the environment where you are building the flow. In the [Side Navigation Menu](371.html), click on the **Overview** option within the **Overview** section. There, you will see a summary of the Flow containing the `<bot-id>` along with other details, such as the Flow's name and purpose. The `<bot-id>` is the alphanumeric identifier that appears below the name of your Flow. ![Image](./images/395.png) ## Step 3: Configure secret Next, click on **Client secrets** and select **Add a client secret for your bot.** ![Image](./images/396.png) This will open a pop-up dialog showing the message **New client secret generated**. Make sure to save the **New client secret generated**, as you will only be able to view it once. This client secret will be used later in the Flow configuration. ![Image](./images/397.png) ## Step 4: Create a Teams app Go to the **Apps** section of the menu and click on **Create your first app**. Next, enter the name of your app. ![Image](./images/398.png) Once you have created your app, a window will automatically open for you to configure the basic app information. You will need to complete the following fields:\n![Image](./images/399.png) App names: - Short name: Short name of the app.\n- Full name: Full name of the app. Descriptions: - Short description: Short description of the app.\n- Long description: Detailed description of the app. Developer information: - Developer or company name: Developer or company name.\n- URL: Website of the developer or company. App URLs: - Privacy policy: URL of the privacy policy.\n- Terms of use: URL of the terms of use. Application (client) ID: - Bot ID: It is automatically generated when the bot is created. To view the ID, go to Tools > Bot Management. ![Image](./images/400.png) ## Step 5: Configure app features Go to **Configure > App Features** in the left menu and select **Bot**. ![Image](./images/401.png) In the window that opens, select the bot you created in \"Step 1: Create a Teams bot\" in the combo titled **Select an existing bot**. Next, choose \"Upload and download files\" in **What can your bot do?**. Then, choose \"Personal\" in **Select the scopes where people can use your bot** and click on the \"Save\" button. ![Image](./images/1197.png) ## Step 6: Configure as multi-tenant Navigate to the following URL: [https://portal.azure.com/#view/Microsoft_AAD_RegisteredApps/ApplicationMenuBlade//Authentication/appId/&lt;bot-id&gt;/isMSAApp/](https://portal.azure.com/#view/Microsoft_AAD_RegisteredApps/ApplicationMenuBlade//Authentication/appId/&lt;bot-id&gt;/isMSAApp/), replacing `<bot-id>` with the bot ID. Click on \"Supported account types\" and on the next screen select \"Multi-tenant\". Finally, click on \"Save\". ## Step 7: Flow configuration Go to the Flow you want to expose. Next, go to the [Side Navigation Menu](371.html) **> Configuration > Integrations** and click on the **Add Integration button.** ![Image](./images/402.png) Select \"Teams\" from the drop-down menu and fill in the required data: - Client Id: this is the ID of the bot created in Teams.\n- Client secret: is the secret of the client generated in \"Step 3: Configure secret\". ![Image](./images/403.png) ## Step 8: Test the bot in teams To test your application in Teams, first go to the [Teams developer portal](https://dev.teams.microsoft.com/) and then to the **Apps** section. Select your app and click on the **Publish** button in the upper right corner. ![Image](./images/404.png) You will have several options available; in this case, select \"Download the app package\" to download your application package. Next, open the Microsoft Teams application, go to \"Apps\" and select \"Manage your apps\". Here, proceed to \"Upload an app\" and upload the package you downloaded. ![Image](./images/405.png) ## See Also [Distribute your Microsoft Teams app](https://learn.microsoft.com/en-us/microsoftteams/platform/concepts/deploy-and-publish/apps-publish-overview)",
          "evaluationResultRowContextDocId": "3a5ca104-b946-431b-af53-a2027499fe3e",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "c4dfa8a3-c045-4c52-bb23-808b802964fe",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?391",
          "evaluationResultRowContextName": "391",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "- New Average Request Time metric added in the Project Dashboard. ### January 8th - The option formerly known as 'Search Documents' has been improved and renamed to RAG Assistant (Retrieval Augmented Generation) to provide an optimized experience when searching and generating information. #### Frontend - Feedback is provided during conversations with RAG Assistants, indicating where you are in the process.\n- 'Response streaming' support for RAG Assistants.\n- Settings are hidden when selecting an assistant, except when 'Chat with LLMs' is selected. ## 2023 ### December 19th - Fixed: Too Many Redirects when accessing Playground using a browser in Spanish language. ### December 6th - New backoffice design.\n- Access to the Playground from the backoffice to chat with the assistants defined in the project.\n- Upload images for analysis with GPT-4 Vision.\n- Google Analytics support at the frontend.\n- Keep a conversation thread when chatting with documents.\n- An email notification is sent automatically when a new member is invited to join the organization or project. ### November 28th - First version officially released!! ### November 6th - The following OpenAI models are supported: GTP-4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), and GPT-4 Vision (gpt-4-vision-preview). ### October 18th - [Manage quotas per project](71.html): It is possible to set quota limits to control project expenses or usage. ### October 11th - [AI-Driven Load Balancing](32.html): The platform automatically manages the Load Balancing process when you work with generative AI providers, efficiently addressing the limits imposed by LLM platforms.\n\n- In the [Organization API](22.html), the ability to set and manage usage limits on projects through the [POST /project](22.html) and [GET /project/{id}](22.html) endpoints has been added.\n- [Quota Limit](71.html) now includes improvements such as highlighting the active quota in green, offering options to cancel active quotas, among others. ### October 17th - Rebranding to Globant Enteprise AI\n- Improvements in RAG - New option not to [compact the conversation history](297.html) - Support for Redis, MongoDB Atlas and PG Vector [vectorstores](267.html) - New endpoint to get an embedding. Check details here: [Embeddings API](422.html)\n- Playground improvements\n- File management improvements\n- New [LLMs supported](200.html) - NVIDIA provider with new models supported - nvidia.nemotron-mini-4b-instruct - meta.llama-3.1-8b-instruct - meta.llama-3.1-70b-instruct - meta.llama-3.1-405b-instruct - meta.llama-3.2-3b-instruct - Groq provider supported - groq/llama-3.1-70b-versatile - groq/llama-3.2-11b-vision-preview - groq/llama-3.2-3b-previewgroq/llama-3.2-1b-preview\n- New [embeddings models](447.html) added - Vertex AI: - vertex_ai/textembedding-gecko - vertex_ai/text-embedding-004 - vertex_ai/textembedding-gecko-multilingual - Nvidia: - nvidia/nvclip - nvidia/nv-embed-v1 - nvidia/baai.bge-m3 - nvidia/snowflake.arctic-embed-l - nvidia/nv-embedqa-mistral-7b-v2 - nvidia/embed-qa-4 - nvidia/nv-embedqa-e5-v5 ### September 25th - [Support for file](346.html) processing with prompt-based assistants. This will enable many scenarios, such as uploading documents and summarizing, extracting, and checking information, etc. Also, depending on the model used by the assistant, it will be able to process audio, video, or images.\n- [Support for multi-modal](345.html) LLMs allow processing docs, audio, video, and images in models like GPT-4o or Gemini Pro.\n- Chat with data assistants - The model used to build the queries was updated with GPT-4o, which improves the quality of the generated query. - Configure the query builder server by organization and project. This means you can connect with different DBMS from each project when building Chat with data assistants. - Show an explanation of how the query was built.\n- New [Playground Interface](349.html) design - New design - Upload documents from the front end to chat with them.\n- [Flows builder](321.html) - There will be two types of Flows, one more oriented to build a conversational UI and the other to build assistant flows. Access to these flows will only be available through [Chat API](34.html) or through the [channels](384.html) offered by Flows.\n- New models hosted in AWS Bedrock added: - Amazon Titan Express v1 - Amazon Titan Lite v1 - Anthropic Claude 3 Haiku - Anthropic Claude 3 Sonnet - Anthropic Claude 3.5 Sonnet - Cohere Command - Meta Llama 3 8B - Meta LLama 3 70B\n- It is now possible to provide clear guidance on the assistant's capabilities, allowing you to add information such as descriptions, features, and example prompts. This configuration can be done from the Backoffice, [Start Page](408.html), or WelcomeData section of the [Assistant API](35.html) and [RAG Assistants API](29.html) endpoints.\n- RAG Assistants - New endpoint in [RAG Assistants API](29.html): using [DELETE/profile/{name}/documents](29.html), all documents associated with the specified RAG Assistant are deleted. ### August 9th - Support of new models - GPT-4o mini\n- RAG Assistants - New option called [CLEANUP](60.html) allows to delete the documents associated to a RAG Assistant. - When creating a new assistant, the following defaults are updated: - [Chunk Count](69.html) updated from 2 to 5. - [Model Name](68.html) from gpt-3.5-turbo to gpt-4o-mini.\n- Data Analyst Assistant - New [Conversational Module](291.html) for Data Analyst Assistant.\n- Considerations - **Important**: [max_tokens integer below minimum value](290.html) error may occur if you are using the _max_tokens_ value as -1; those Assistants must be updated to use a correct positive value. ### July 4th - Permissions by assistants in role-based access control. A new [role and permissions management system](242.html) has been implemented to provide greater flexibility and control. Please read [Migration to the new Roles and Permissions Management System](264.html).\n- Dashboard performance optimizations. Please read [Using the Request Summary Table for Improved Dashboard Performance](273.html).\n- RAG Improvements - New option to customize the [context](205.html). - Support for [Max Marginal Relevance](206.html) - Support for _similarity_hybrid_ and _semantic_hybrid_ for [Azure AISearch](207.html) - Advanced options for SelfQuery retriever prompt case. See [Self Query Use Case](150.html) - Support for Parent Document retrieval option. See [Parent Document Retrieval Strategy](204.html) ### June 10th - [Enterprise AI Proxy](19.html) is deprecated. Use [Chat API](34.html) instead.\n- Support for new LLMs - OpenAI new model GPT-4o - Models in Google Vertex - Gemini 1.0 Pro - Gemini 1.5 Flash preview-0514 - Gemini 1.5 Pro preview-0514 - Claude 3 Haiku - Claude 3 Opus - Claude 3 Sonnet\n- RAG Improvements - New option to initialize RAG Assistant based on another when creating a new RAG Assistant. - New option to export document list in View Documents over a RAG Assistant. - Added filter options when browsing Documents. - SelfQuery RAG retriever partial support for a [customized Prompt](150.html). - Support for text-embedding-004 in Google models to generate the embeddings.\n- Deprecated [Assistant API](35.html) endpoints. - /assistant/text/begin - /assistant/text\n- Support to deploy in Google Cloud Platform. ### May 8th - New [Chat with Data Assistant](159.html).\n- New [Ingestion SDK](172.html) to automate document ingestion in RAG assistants.\n- New models hosted in NVIDIA platform supported. See [Supported Chat Models](200.html) for more details.\n- New option to export information about projects and members available for the organization administrator.\n- New API to extend dataset for [Data Analyst Assistant 1.0](89.html).\n- New filter by user email in Requests.\n- Update default to use _text-embedding-3-small_ OpenAI Embeddings for new RAG assistants.\n- Support for gemini-1.5-pro-preview-0409 model added. ### April 3rd - New [API Assistant](110.html).\n- New [Data Analyst Assistant 1.0](89.html).\n- The user Dashboard displays the email to identify the most active user or the one who generates the most costs. ### March 11th - GeneXus Identity Provider is implemented, expanding the login options in the [Backoffice](42.html) of the production environment. This allows for login not only with Google but also with Apple or GeneXus Account.\n- It is possible to customize the icon for each assistant. ### February 29th - [Frontend](82.html) improvements in UI/UX.\n- Option to get feedback from end users when interacting with [RAG Assistant](44.html).\n- Gemini Pro LLM support.\n- New [Dashboard](562.html) with user metrics.\n\n# Updates Log This log shows the most important fixes or features added to the platform. ## Log ## 2025 ### April 10th - New [Globant Enterprise AI Lab](972.html) - The Lab is designed for defining, managing, and orchestrating autonomous AI agents. It provides a standardized model for representing agents, their capabilities, and their interactions within complex workflows. The core components of the Lab include: - Agents & Tools: This module allows for the definition and management of individual agents and their resources, such as skills and tools. It serves as a central hub for cataloging and managing the agent workforce. - Agentic Processes: This component enables the definition of processes based on tasks executed by Agents. These workflows facilitate collaboration among agents to achieve larger objectives. More details at [How to create an Agentic Process](985.html). - Agent RunTime: This module provides the execution environment for agentic workflows, where agents perform tasks based on their skills and interact with artifacts, driven by events and the flow of knowledge. - The Lab aims to meet the growing demand for intelligent, self-sufficient AI agents capable of collaborating and solving complex problems with minimal human intervention. It offers a flexible and adaptable model, allowing for the creation and management of a diverse range of agents, from co-pilots working alongside humans to fully automated agents executing complex tasks. Implemented as a module of Globant Enterprise AI, the Lab supports the development of intelligent agents that can work autonomously or in collaboration with humans and other agents.\n- New features in Flows - Agent Integration Component: You can now directly integrate agents created with the AI Lab into a Flow. These agents can be exposed through platforms like WhatsApp, Teams, or Slack. - [File Upload Support](1053.html) from WhatsApp: Flows now support receiving file attachments such as documents, images, audio, and video directly from WhatsApp interactions. - Audio and Video Attachment in Web Chat: The Web Chat component now allows users to attach audio and video files, enhancing the interaction experience.\n- New Features for the Data Analyst Agent - Reduced Configuration Requirements: The setup needed to enable the assistant to respond to a wide range of questions has been minimized. - Enhanced Analysis Module: An additional analysis module has been incorporated to complement the responses with relevant business conclusions and interpretations of the obtained data.\n- New metrics to track processed tokens. - A new view has been added to each project dashboard to monitor token-based consumption. More details at [Tokens tab in Dashboard](965.html). - To enable this feature, you must run the initialization process described in [Reprocessing Requests and Loading the Summary Table](1015.html).\n- LLMs: - New Gemini 2.5 Pro (via providers Vertex AI and Gemini): Gemini 2.5 is Google\u2019s latest reasoning model, engineered to tackle increasingly complex challenges. This model is designed for tasks that demand advanced analytical thinking and robust problem-solving capabilities. More details at [LLM API](663.html). - Migration to Gemini 2.0 series (Vertex AI): Based on recommendations from Vertex AI, we have migrated from the legacy Gemini 1.0 and 1.5 models to the more advanced Gemini 2.0 series, offering improved performance, scalability, and integration capabilities. For comprehensive information, please refer to the [Deprecated Models](Deprecated Models). - New Azure OpenAI models (o1, o1-mini and o3-mini): We have expanded our model availability by introducing these models via Azure, providing the same high-quality capabilities as those offered through the OpenAI provider. - DeepSeek-R1 via AWS Bedrock: Recently added through a Serverless API, DeepSeek-R1 offers reliable inference with a substantial 128K token context window and up to 32K maximum output tokens. - OpenAI's new models with built-in web search tool: These specialized models integrate web search capabilities directly into the Chat Completions API, enabling them to both interpret and execute search queries in real time. - openai/gpt-4o-search-preview - openai/gpt-4o-mini-search-preview - New models - Beta only: - gemini/gemma-3-27b-it: Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 has a large, 128K context window, and multilingual support in over 140 languages. - SambaNova: - DeepSeek-R1: This Provider offers the fastest performance for running DeepSeek, processing up to 198 tokens per second per user, with a 16K token context window. The model is hosted in US data centers; privately and securely. - DeepSeek-V3-0324: This model significantly outperforms its predecessor with enhanced reasoning benchmarks, improved code executability and refined web aesthetics, and superior Chinese writing aligned with the R1 style. It also offers better multi-turn interactive rewriting, translation quality, detailed report analysis, and more accurate function calling. - QwQ-32B: SambaNova provides access to QwQ-32B-Preview, the best open source test-time compute model released by Alibaba. - Llama 4 collection by Meta: - Llama 4 Scout: A 17B-parameter multimodal MoE model with 16 experts that excels in text and image understanding. The model is currently in beta and is available via providers Nvidia, Groq and SambaNova. Via Groq, it supports a 128k tokens context window with fast inference at 460 tokens/sec, while SambaNova Cloud runs at 697 tokens/second/user. - Llama 4 Maverick: Available via Nvidia, this 17-billion-parameter model featuring 128 experts supports a 32k-token context window. ### March 10th - New LLMs: - GPT-4.5 - Claude 3.7 Sonnet (Providers Anthropic, Vertex AI and AWS Bedrock) - Updates in Gemini 2.0 series: - vertex_ai/gemini-2.0-flash-lite-preview-02-05 - vertex_ai/gemini-2.0-flash-thinking-exp-01-21\n- RAG Revision #6 - Support for o3-mini, gpt-4.5-preview, claude-3-7-sonnet-20250219, new DeepSeek, Gemini2* and sambanova LLM providers. - New pinecone provider available for [embeddings](447.html) and [rerankers](638.html). - The [CleanUp](60.html) action message has been corrected to clearly specify that it will permanently delete the RAG Assistant files and update the information in the RDS. - Added [usage](712.html) element on every response. - Improvements when changing the [LLM](69.html)/[Embeddings](68.html) settings; all models and providers are normalized to be selected from standard combo-box items; use the [override mechanism](209.html) if you need other options. - Support for [guardrails](840.html). - New [documentAggregation](298.html) property to decide how sources are grouped and returned.\n- It is possible to [provide feedback](170.html) on the response of the Chat with Data Assistant in the Frontend.\n\n- The new [Evaluation APIs](893.html) introduce key functionalities through three interconnected APIs: [DataSet API](839.html), [Evaluation Plan API](849.html), and [Evaluation Result API](856.html). This version is primarily designed for users with a data science profile and is mainly accessed via APIs, complemented by a series of Jupyter notebooks that demonstrate their use. For a comprehensive guide on how to use these APIs, you can refer to [How to evaluate an AI Assistant](881.html) and the [EvaluationAPITutorial.ipynb](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?887,,) notebook, which provide practical examples and code for working through the evaluation process.\n- File attachment support in Flows (version 0.9).\n- Support for Full Story integration in the Workspace/Playground to generate user access statistics in Full Story.\n- In the [LLM API](663.html), for models that have descriptions in the specified languages, the `descriptions` property is included in the Response, which contains the descriptions in the available languages, such as `Spanish`, `English` and `Japanese`.\n- [Data Analyst Assistant 2.0](886.html) version presents important improvements, simplifying the interaction with the data by reducing the main components to just two: **Dispatcher** and **Thinker**. In addition, the metadata structure is automatically generated when loading the datasets, streamlining the setup process. For more information, see [How to create a Data Analyst Assistant](105.html).\n- The option to consult version-specific documentation is now available. Articles with versions show the option \u201cOther document versions\u201d in the header. Clicking on \u201cOther document versions\u201d brings up a menu that allows you to choose between the most recent version (\u201cLatest\u201d) or earlier versions (e.g. \u201c2025-02 or prior\u201d). If you select a version other than \u201cLatest\u201d, a message appears: \u201cThis is not the latest version of this document; to access the latest version, click here\u201d. This message provides a direct link to the most up-to-date documentation. [Components Version Update](822.html) ### February 10th - New documentation with details about [Supported Chart Types](731.html).\n- New [Usage Limits API](802.html).\n- Flows - Logs associated with conversations are accessible from the [conversation history](378.html).\n- RAG Revision #5 - Ingestion supports new [file formats](137.html). For on-premises installations, update the _FILE_TYPES_ parameter. - New optional _cleanProcessing_ parameter when using [Parent Document Retrieval Strategy](204.html). - [Embeddings API](422.html) cache support; disabled by default from a [RAG assistant](68.html). - New [mode](68.html) property for using multimodal embeddings and creating a [Multimodal RAG](783.html).\n- New endpoint [GET /accessControl/apitoken/validate](22.html) returns information about the organization and project associated with the provided apitoken.\n- New LLMs: - Already in production - **OpenAI**: o3-mini - Already in Beta - **DeepSeek**: - deepseek/deepseek-reasoner - deepseek/deepseek-chat - azure/deepseek-r1 - nvidia/deepseek-ai-deepseek-r1 - groq/deepseek-r1-distill-llama-70b - sambanova/DeepSeek-R1-Distill-Llama-70B - Updates in Gemini 2.0 series: - gemini-2.0-flash-thinking-exp-01-21 (Via Providers Gemini and Vertex AI) - gemini/gemini-2.0-flash-lite-preview - gemini/gemini-2.0-pro-exp - vertex_ai/gemini-2.0-flash-001 - sambanova/Llama-3.1-Tulu-3-405B [Components Version Update](822.html) ### January 13th - Internationalization, [Backoffice](42.html), and frontend support for Japanese.\n- Invitations now include information about the organization and project in the subject.\n- New LLMs - Already in Production - **OpenAI**: o1 (2024-12-17 version) - Already in Beta - New Providers: **Cohere** - **Cohere**: Cohere-r\n- [Guardrails](682.html) configured by assistant.\n- [Rerank API](625.html) to semantically order a list of document chunks given a query.\n- New optional RAG [Retrieve and Rerank](620.html) adds an extra layer of precision to ensure that only the most relevant information reaches the model used in the generation step. ## 2024 ### December 12th - [Automatic Creation of Default Assistant](640.html)\n- [Organization Usage Limits](666.html): It is possible to set quota limits to control organization expenses or usage.\n- Chat with Data Assistant - [Show details](170.html) about the generated query in the Playground. - Support in [Chat API](34.html) to interact with Chat with Data Assistant.\n- Flows - Support for markdown when showing the response on the different channels supported by Flows (web, Slack, WhatsApp, and Teams). - New component for connecting flows to the agent overflow console (Human-in-the-loop) via B2Chat. Please read [How to connect a Flow to B2Chat](674.html).\n- RAG - [Chroma](537.html), [Elasticsearch](579.html), [OpenSearch](580.html) vectorstore support. - [Improvements](581.html) when processing PDF documents (update to RAG module revision #4 or higher). - Integration with [AWS Knowledge Bases](577.html) at query time.\n- Data Analyst Assistant - Option to update metadata options. - New version by default in new Data Analyst assistants.\n- New LLMs - OpenAI: gpt-4o-2024-11-20 - AWS Bedrock: Anthropic Claude 3.5 Haiku - Amazon Nova models (Micro, Lite, and Pro) - Llama 3.1 405B on Vertex AI - Beta: - Support for providers Cerebras, SambaNova and xAI (Grok models). - All new Gemini Experimental models.\n- Security - It is possible to define a user 'all-members@domain.com' to give access to the Frontend to all users of the domain '@domain.com'. Permissions are enforced based on the roles assigned to this user. Please read [Allowing Frontend Access for Users from Specific Domains](657.html) ### November 12th - Flows execution integrated into the Playground\n- New [LLMs support](200.html) - **OpenAI**: o1-preview and o1-mini - **Claude Sonnet 3.5 v2 - Providers**: Anthropic, Vertex AI, and AWS Bedrock - **Llama 3.2 models - Providers**: Vertex AI and AWS Bedrock\n- Chat with data assistants - Possibility to edit metadata, [entities](170.html), and attribute descriptions. - The Properties tab has been renamed to [Settings](170.html) along with the options that can be configured in it.\n- RAG - New [returnSourceDocuments](298.html) option to disable returning the documents section used to answer the question. - New [step](298.html) option to use the assistant as a retrieval tool. - Support for [custom history](85.html) in conversations using the _chat_history_ variable.\n- Stand-alone Frontend based on the new Playground UI - Options to customize the Frontend to use the client logo, color palette, welcome message, and descriptions. - Feature to collect feedback (thumbs up/down) in each response. - Google Analytics support.\n- Data Analyst Assistant - Support to upload large CSV files.",
          "evaluationResultRowContextDocId": "68ace6a6-8b1b-4736-a466-9ed0a67b97a9",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "dc7c510c-46cb-4ed1-b1a9-bb0086ebb7cd",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?31",
          "evaluationResultRowContextName": "31",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.9,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "4f2d843f-3ba4-429d-9f66-fb7094248b99",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0046265400",
      "evaluationResultRowEndDate": "2025-05-13T11:13:35",
      "evaluationResultRowOutput": "S\u00ed, puedes publicar (exponer) un Flow tanto en **Microsoft Teams </B>como en **WhatsApp </B>utilizando Globant Enterprise AI. Aqu\u00ed te resumo c\u00f3mo hacerlo para cada plataforma seg\u00fan la documentaci\u00f3n:\n\n---\n\n### Publicar un Flow en Microsoft Teams\n\n1. **Crear un bot en Teams </B>a trav\u00e9s del [Teams developer portal](https://dev.teams.microsoft.com/).\n2. **Configurar el endpoint </B>del bot con la URL que incluye el ID del Flow (`bot-id`).\n3. **Generar un client secret </B>para el bot.\n4. **Crear una app en Teams </B>vinculada al bot.\n5. **Configurar las caracter\u00edsticas de la app**, agregando el bot, permisos y scopes.\n6. **Configurar la app como multi-tenant </B>en Azure.\n7. **Integrar el Flow </B>con Teams en la configuraci\u00f3n del Flow, utilizando el Client Id y Client Secret creados.\n8. **Probar el bot </B>en Teams subiendo la app al cliente Teams.\n\nToda esta gu\u00eda paso a paso est\u00e1 disponible en detalle en la documentaci\u00f3n:  \nhttps://wiki.genexus.com/enterprise-ai/wiki?391\n\n---\n\n### Publicar un Flow en WhatsApp\n\nExisten dos formas principales para exponer el Flow a WhatsApp:\n\n#### 1. Mediante WhatsApp Client oficial (WhatsApp Business API):\n\n- Crea una cuenta y aplicaci\u00f3n en Facebook Business y Meta developer portal.\n- Obt\u00e9n las credenciales necesarias (Phone number, phone ID, WhatsApp Business Account ID, Access Token, App Secret).\n- Configura el Flow con esos datos.\n- Configura el webhook con la URL que incluye el Bot ID del Flow.\n- Verifica y suscribe campos al webhook para recibir notificaciones.\n- Ten en cuenta que esta forma puede generar costos asociados.\n\nM\u00e1s informaci\u00f3n detallada aqu\u00ed:  \nhttps://wiki.genexus.com/enterprise-ai/wiki?549\n\n#### 2. Mediante WhatsApp Client a trav\u00e9s de B2Chat:\n\n- Genera una Application Key en el Flow.\n- Obt\u00e9n el Bot ID del Flow.\n- Construye un webhook con el formato que incluye el Bot ID y la API key.\n- Solicita credenciales de B2Chat con soporte de Globant Enterprise AI.\n- Configura la integraci\u00f3n en el Flow con las credenciales recibidas.\n- Permite notificaciones push si lo deseas.\n- Tambi\u00e9n puede implicar costos por el uso de WhatsApp.\n\nInformaci\u00f3n detallada:  \nhttps://wiki.genexus.com/enterprise-ai/wiki?548\n\n---\n\n### Conclusi\u00f3n\n\nS\u00ed, puedes publicar un Flow en Teams o WhatsApp. La plataforma provee instrucciones claras y espec\u00edficas para conectar y exponer tus Flows a estas aplicaciones mediante bots y APIs respectivamente, tomando en cuenta que ciertas configuraciones y permisos son necesarios, adem\u00e1s de posibles costos adicionales para WhatsApp.\n\n---\n\nSi quieres la gu\u00eda completa o necesitas ayuda con alg\u00fan paso en particular, dime y te puedo proveer detalles espec\u00edficos.  \n\nReferencias:  \n- Exponer Flow en Teams [1](https://wiki.genexus.com/enterprise-ai/wiki?391)  \n- Exponer Flow en WhatsApp Client via B2Chat [2](https://wiki.genexus.com/enterprise-ai/wiki?548)  \n- Exponer Flow en WhatsApp Client [3](https://wiki.genexus.com/enterprise-ai/wiki?549)",
      "evaluationResultRowStartDate": "2025-05-13T11:13:24",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "9660"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# RAG Assistants configuration All interaction with this component is configured through the RAG Assistants section. A default RAG Assistant is created during initialization and later it can be modified or new ones created to change the assistant's behavior. RAG Assistant is located in the Project Options section and allows you to customize options specific to each RAG Assistant by clicking on UPDATE. ![RAG Assistant](./images/62.png) The set of options is organized into the following subsections: ![Options Subsections](./images/63.jpg) - [General Information](68.html): key details that allow you to quickly understand the assistant's capabilities and features.\n- [Prompt](69.html): information on how the assistant is configured to receive and process user queries.\n- [Retrieval](70.html): specifies how the information is retrieved. ### [See Also](#See+Also) [Index Configuration](145.html) [Using variables in Prompts](85.html) [Context Prompt Template](205.html) [Start Page](408.html)",
          "evaluationResultRowContextDocId": "1a844a6d-9741-4178-bc4f-ce0030f01f09",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "00b456f3-dc10-40a3-97a8-1e9a4de2d80e",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?64",
          "evaluationResultRowContextName": "64",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "is defined; for example, \"lastUserInput\". This variable contains the user's most recent input and will be used as the basis for the query on the selected documents or tags. Similarly, in the Output section, there is also a Variable field that specifies the variable in which the output generated by that node will be stored. In this case, we are interested in saving the RAG Assistant\u2019s response in a new variable called lastResponse. Therefore, the Use as response option is disabled. Clicking on the pencil icon to the right of the node opens the \"State Configuration\" menu where you can modify the following items: - **Acceptable confidence level**: The default value is 0.2, which indicates the level of confidence in the match of the chunks that the RAG obtains after searching the vector database. Only responses that meet or exceed this confidence level will be considered valid.\n\n- **Include conversation history**: Enabled by default. It ensures that the conversation history is included in the context of the interaction, allowing the assistant to take previous messages into account when generating responses.\n- **Include context variables**: Enabled by default. It allows context variables stored during the conversation to be sent to the assistants so that they can be used within the prompts.\n- **Show sources**: Enabled by default. It allows the assistant to show the sources of the information used to generate the response, which helps to provide transparency and to validate the accuracy of the response.\n- **Handle empty response**: Disabled by default. It handles situations where the assistant is unable to generate a valid response. When enabled, it opens a new flow branch (as shown in the image), allowing you to configure specific actions to follow in case no response is available. In this new branch, you can add any of the components available in the Interactions menu on the left to continue the interaction as needed. ![Handle Empty Response](./images/337.png) In this case, if the RAG Assistant finds the answer in the documents, you want the flow to call an assistant to translate the response to Japanese and store it in the same input variable (lastResponse). If no answer is found, the flow goes to a second RAG Assistant configured with documents used in Globant Enterprise AI courses. The variable that goes into the RAG Assistant is lastUserInput, which contains the user's last input. The response generated by this RAG Assistant is stored in the lastResponse variable. ![RAG Assistant Configuration](./images/338.png) ### Go To Node Finally, the Go To node is used to connect different flows within the interaction. This node allows you to select the flow to which you want to redirect the end user based on their current interaction. ![Go To Node](./images/339.png) Clicking on the blank bar (corresponding to the drop-down menu) displays a list of available flows: ![Available Flows](./images/340.png) Here you can select and define the flow to which you want to redirect the user. To review or modify any of these flows, you can access the Flows menu on the left side of the screen. In addition, \"Pick from canvas\" allows you to select a node directly from the canvas, facilitating navigation and connection between different parts of the Flow. The 'Go' icon next to the node allows you to navigate directly to the node to which it is connected. In this case, there are two 'Go To' nodes: - The first 'Go To': Returns to the 'User Input' node. Since the previous node (which translates to Japanese) does not store the response in a new variable and simply redirects to the 'User Input', the response received by the end user is the answer to their question translated to Japanese.\n- The second 'Go To': Sends the response to the 'Assistant' node, which is responsible for translating the generated response to Japanese using the lastResponse variable. This variable contains the response obtained from either of the two RAG Assistants previously executed in the flow. In this way, the flow ensures that the end user receives the response in Japanese, regardless of whether the response was generated by the first RAG Assistant or the second. ![Flow Diagram](./images/341.png) ## Step 3: Test flow Once you have configured the flow, make sure to click on the 'Save' button located at the top left of the screen. To test the flow you have created, you have three options available from the same design window: ![Test Flow Options](./images/342.png) 1. Click on the arrow icon (similar to the play button) to run the flow ('Run Flow').\n2. Click on the message icon to open the testing assistant (\u201cOpen Testing Assistant\u201d). ![Testing Assistant](./images/343.png)\n3. Click on the icon to open in a new window (the box with an arrow) to access the demo page (\u201cGo to Demo Page\u201d). ![Demo Page](./images/344.png) The first two options will open a window on the right side of the screen, while the third option will open a new window in the browser.\n\n# How to create a Flow This is a step-by-step guide to create and test an Assistants [Flow](321.html). ## Step 1: Create New First, log in to the [Globant Enterprise AI Backoffice](42.html). In the Project Dynamic combo box, select the project you want to work with (in this case, Default is used). Next, on the left side of the screen, you will find the backoffice menu. In this menu, click on Flows. ![Flow Menu](./images/324.png) By clicking on Flows in the Globant Enterprise AI Backoffice, a new window opens in the browser with the Flow Builder, where the Flows associated with the selected Globant Enterprise AI project are created and managed. When accessing Globant Enterprise AI\u2019s Flow Builder for the first time, a welcome screen like the one shown in the image below is displayed. From this screen, you can start creating a new flow by clicking on the \"Create\" button. ![Welcome Screen](./images/325.png) When you click on \"Create\", a pop-up window opens in which you must fill in the following information: 1. **Name**: Descriptive name for the flow, allowing it to be easily identified.\n2. **Description**: This field is optional, but it is recommended to add a brief description of the flow to clarify its purpose or content.\n3. **Type**: Allows you to select the type of flow to create, with the options \"Chat Flow\" or \"Assistant Flow\". Selecting one of these types will determine specific configurations and characteristics of the flow. In this case, Assistant Flow is selected.\n\n4. **Select languages**: Sets the language in which the flow will be configured, and allows you to define the language of the hard-coded messages. Multiple languages can be selected, so the same message can be available in different languages. Once you have completed these fields, you can click on the \"Create\" button. ![Create Flow](./images/326.png) ## Step 2: Manage the Start flow Once you have clicked on \"Create,\" you will return to the welcome screen. There, click on the \"Manage\" button of the newly created flow to add the assistants you want and customize their behavior. ![Manage Flow](./images/327.png) Clicking on the \"Manage\" button displays the first flow, which is created automatically. This initial flow, called \"start,\" establishes a basic configuration that you can customize, adding or modifying interactions, messages and assistants according to your needs. ![Start Flow](./images/328.png) ### Start flow The initial flow, called \"start,\" is automatically generated with the ID input.welcome. ![Start Flow ID](./images/329.png) ### Start (Intent: input.welcome) Node The Start (Intent: input.welcome) node marks the beginning of the conversation flow. This node is activated every time the flow is triggered, and its main purpose is to start the flow and, from there, the other nodes that manage the interaction with the user are connected. To rename the start node, go to the left menu, find Flows > start, click on the three dots next to it, select \"Rename Flow\" and, after typing the new name, press Enter to save the change. ![Rename Flow](./images/330.png) ### Message Node The next node, Message, is in charge of sending the welcome message or any other initial message configured. In this case, the message is: \"Hello {name}, how can I help you?\". Here, {name} represents a dynamic variable that is filled with the user's name. The message displayed in this node is customizable, and the text can be formatted by selecting different styles, such as Normal, Heading 1, among others, from the drop-down menu that appears where it says \"Normal\". This allows you to adjust the text style according to your needs. ![Message Node](./images/331.png) On the top bar of the node, you can see several formatting buttons such as bold, italic, underline, among others, to customize the text. In addition, there are icons that allow you to add lists, links and emojis. The last button, which has the shape of a small drawer, allows you to insert dynamic variables in the message. Clicking on this button displays a list of the variables available in the system, and it is also possible to define new custom variables. This Message node can be deleted by clicking on the trash can icon to the right of the node. It is also possible to add a variant by clicking on the \"plus\" (+) button at the top right. ### User Input Node The flow then proceeds to capture the user input through the User Input node, storing it in the lastUserInput variable for later use. ### Assistant Node In the Assistant node, you must configure the assistant to perform specific actions. This node uses the information stored in the lastUserInput variable, which contains the text entered by the end user in the User Input node. ![Assistant Node](./images/332.png) To configure the assistant, click on the \"Select an assistant\" field. By default, \"No assistant selected\" will be displayed. From the drop-down menu, choose the assistant you wish to use. **Note**: The assistants available are those that have been defined in the associated project, which was configured in the first step. The assistants can be any of those defined in Globant Enterprise AI, including [Data Analyst Assistant](886.html), [API Assistant](110.html), [Chat with Data Assistant](159.html). Once selected, the Prompt field will display the message that defines the task of the assistant. This message is read-only and can\u2019t be edited. The assistant's response can be stored in the lastUserInput variable or in a new variable, depending on your needs. By default, the Use as response option is enabled, which causes the assistant's response to be used directly in the lastUserInput variable. If you prefer to store the response in a new variable, disable Use as response. This will enable the Output field, where you can select or create a new variable. ![Assistant Output](./images/333.png) In addition, you can enable the option to read the assistant's response in JSON format. By enabling this option, you will have the ability to map the fields of that JSON to variables previously defined in your configuration. This allows for more precise integration of the assistant's responses in more complex workflows or in systems that require a specific format for data handling. The pencil icon to the right of the node corresponds to the Edit button. By clicking on it, you can enable or disable the Include conversation history and Include context variables options. By default, Include conversation history is enabled. It allows the assistant to access the conversation history to generate more coherent and contextual responses. The Include context variables option is also enabled by default. This option sends to the assistant variables defined in the flow, such as previously captured data or configured values, so that they can be used within prompts. This node can be deleted by clicking on the trash can icon to the right of the node. In this case, the Assistant node is deleted: ![Delete Assistant Node](./images/334.png) ### Insert RAG Assistant or Assistant Node On the left menu, below the Interactions category, you will find the different components that can be added to the flow. In particular, in the GENERATIVE section, you can choose between adding an Assistant or a RAG Assistant. ![Insert RAG Assistant](./images/335.png) In this case, you need to add a RAG Assistant. To do so, click on the RAG Assistant option in the GENERATIVE section. Next, drag the component to the desired location in the flow. In this case, you can insert it between the User input and Go To nodes. When you add a new RAG node, you must click on the \"Select an assistant\" field. By default, the option \"No assistant selected\" will appear. From the drop-down menu, choose the RAG assistant you wish to use. Once you select the RAG assistant, the next field, \"Select a document\", allows you to choose a specific document that will act as the source for responding to the end user's request. If you select a document, the assistant will only use that document as a reference. ![Select Document](./images/336.png) Alternatively, you can use the \"Indicates the documents' tags\" field. Here you can add the tags defined in [Step 2: Upload RAG documents](60.html), which are used to filter the documents. This allows the assistant to search only in the documents that match the indicated tags to respond to the end user's request. In the Input section, a field called Variable is displayed. There, the variable to be used as input for that node",
          "evaluationResultRowContextDocId": "09ebe65a-fb9d-4f45-8146-084558fef786",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "1bd35e93-c522-4fd4-9c9d-4b502405637a",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?322",
          "evaluationResultRowContextName": "322",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "``` ## Step 3: Creating the Flow Follow the steps described in [[322]] to create the Flow called Mall Chat Assistant with the following structure: [[364]] To add variables to the Flow, click on the Variable option in the LOGICAL section of the [[371##Flows menu|Flows Menu]]. Next, drag the component below the User input node into the flow. In this case, a variable has been added: today. **Today variable:** It is defined by clicking on the down arrow inside the variables node and selecting the option + Add new variable. Next, enter the name today and press Enter. Deactivate the Clear option. To set the value of this variable, click on the edit icon to the right of the node. In the left menu, click on the Open in new window icon, and in the window that opens (Value) add `$today`. ## Step 4: Implementing the Derivation Logic Every end user query must be evaluated to determine which assistant is the most appropriate to answer. This is implemented with IF conditions in the flow, where the end user's intentions are compared with the available assistants. To add the Conditional component, go to the Interactions section and, below LOGICAL in the Flow menu, drag the component and drop it between the Assistant and Go to nodes. [[365]] Dragging the Conditional node generates a branch with two paths: one labeled If and one labeled Else. This structure allows you to evaluate a condition and direct the flow according to the result. To add an additional condition, click on the Add condition option to the right of the first node of the branch: [[366]] After clicking on Add condition in the first node of the branch, an additional node of Expression type is added, just like the first one. These Expression nodes allow evaluating different conditional expressions. [[367]] To add the variable to be evaluated, click on the word Variable, located at the bottom of each node. In the first two nodes of Expression type, you can see how each condition evaluates the `orchestrationResponse` variable. Depending on the value of this variable, the flow is directed to different assistants: either to the Movie Ticket Assistant or to the Mall Shops Catalog. If neither of these conditions is met, the flow continues through the Else branch, where by default there is a Go to node. A Message node has been added before the Go to node so that the flow can display a specific message to the end user. Then, the corresponding nodes are added to handle the responses according to the evaluated condition. [[368]] A node of API Assistant type that is configured for the Movie Ticket Assistant and handles queries related to the movie theater in the shopping mall is placed below the first node of Expression type (which checks if `orchestrationResponse` is equal to \"Movie Ticket Assistant\"). A node of RAG Assistant type that is configured for the Mall Shop Catalog Assistant and provides information about the stores in the mall is added below the second node of Expression type (which checks if `orchestrationResponse` is equal to \"Mall Shops Catalog\"). Finally, below each node a Go to is added, which redirects back to the User Input node. Once the assistant has processed and answered the query, this allows the flow to return to the entry point to receive new user interactions, ensuring a continuous and fluid conversation. [[369]] ## Step 5: Testing and Adjusting the Mall Chat Assistant Once you have configured the flow, make sure to click on the 'Save' button located at the top left of the screen. This step is essential to save all changes made to the flow. To test the flow you have created, you have three options available from the same design window: 1. Click on the arrow icon (similar to the play button) to run the flow ('Run Flow').\n2. Click on the message icon to open the testing assistant ('Open Testing Assistant').\n3. Click on the open in new window icon (the box with an arrow) to access the demo page ('Go to Demo Page'). The first two options will open a window on the right side of the screen, while the third option will open a new window in the browser, allowing you to see how the flow runs in a demo environment. During testing, you can run various queries to make sure that the flow responds correctly. For example, you can ask \"Where can I purchase books?\" and the flow should correctly identify the query and direct it to the Mall Shop Catalog, where the end user will be informed about the availability of stores that sell books. Then, you can ask \"What movies can I watch?\" and verify that the flow correctly redirects to the Movie Ticket Assistant to get the list of available movies. If the answer is ambiguous, as in \"I would like to watch the last one\", the orchestrator should use the context of the conversation to identify the last movie mentioned and continue the interaction with the Movie Ticket Assistant. In situations where the query does not correspond to any assistant\u2014such as \"What is Linux?\"\u2014the orchestrator should handle the response using a fallback and provide the information directly from the model, so that the flow covers all possible end user interactions. These tests will allow you to adjust the flow as needed, ensuring that the Mall Chat Assistant runs efficiently and delivers a smooth and accurate experience for end users. {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "57a2c916-1a55-4579-abb9-419226e4e2c1",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "a5a61015-65f6-498a-a646-ef7e464d2c7c",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?363",
          "evaluationResultRowContextName": "363",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# File support for multimodal models Multimodal models allow interactions that go beyond traditional text-based prompts. These models can understand and process information from a variety of sources, including text, file documents, images, audio, and video. This means they can analyze, process, and generate content based on a richer and more complete set of inputs. Globant Enterprise AI implements multimodal models, allowing you to interact with models such as Gemini and OpenAI in a completely new way. By combining text with files, a world of possibilities opens up for analyzing, processing, and generating content. These models can directly receive files alongside your prompts, eliminating the need to manually extract and inject file content. This direct file integration offers several advantages. - Simplifies file handling. You no longer need to extract and inject file content into the prompt.\n- Files are sent directly to the model, streamlining the process and enabling direct model interaction.\n- Empowers you to interact directly with files within your prompts, making it easier to analyze, process, and generate content based on their contents. This eliminates the need for manual extraction and injection, simplifying your workflow.\n- Leverages the information contained in files to provide more comprehensive and insightful responses, enhancing their capabilities. ## File References in Prompts You can reference files in your prompts using a specific format: `{file:filename}`. For example, to reference a file named \"report.pdf\" you would write: `{file:report}`. > **Note**: This reference format only works for files that have already been uploaded to your project. If you're uploading a file for the first time, you'll need to use the standard file upload mechanism within the platform. ## How Files are Managed in Your Project 1. **File Storage**: All files uploaded to your project are stored securely and can be accessed by your prompts.\n2. **File Access**: When you reference a file in your prompt, the system automatically retrieves it and sends it to the multimodal model.\n3. **File Types**: The system supports a wide range of file types, including text documents, images, audio files, and more. ## Sample Suppose you have a document called \"product_specs.txt\" loaded in your project. You can use it in your prompt as follows: ```plaintext\n\"Please summarize the key features of the product described in {file:product_specs}\".\n``` The multimodal model will then access the \"product_specs.txt\" file and use its contents to generate a summary of the product features. ## See Also [Supported models and file formats](406.html)",
          "evaluationResultRowContextDocId": "381b4a5d-cb99-40b8-a58e-e67f82082418",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "c3492fb9-4714-4c86-83bf-d9b5f4f84931",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?345",
          "evaluationResultRowContextName": "345",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Start Page The Globant Enterprise AI Start Page allows offering end users a clear guide to the assistant's capabilities and examples of use. To achieve this, the Start Page allows you to define the following elements that guide end users in their interaction with the assistant: - Define a welcome message that clearly describes the purpose of the assistant. - Describe the assistant's key functions, explaining the problems it is designed to solve, the actions it can perform correctly and its known limitations, ensuring that the end user has realistic expectations about its performance.\n- Add sample prompts that guide the user on how to interact with the assistant. These prompts can be specific requests such as \"Show me the composition of the company\u2019s profit during the last quarter\" or \"Create a bar chart with the sales data for the first 6 months of the year\", which provide insight into the interactions that the assistant can effectively handle. These guidelines are not only fundamental to improving the end user experience, but can also be leveraged in technical scenarios such as batch evaluations or synthetic data generation, providing a clear framework for understanding and analyzing the assistant's behavior in more advanced contexts. ## Start Page Presentation Depending on the type of assistant you are configuring, the layout of the Start Page may vary slightly. There are two presentations: one for the Assistants ([Data Analyst Assistant](886.html), [API Assistant](110.html), [Chat with Data Assistant](159.html)) and one for the [RAG Assistants](44.html). Despite these visual differences, both versions share the same essential functionalities. To configure the Start Page for both Assistants and RAG Assistants, first go to the corresponding page in the [Backoffice](42.html) to work with your data and click on 'UPDATE'. After that, depending on the type of assistant, the following tabs will be displayed: - Assistants: 'General Information' and 'Start Page'.\n- RAG Assistants: 'General Information', 'Prompt', 'Retrieval' and 'Start Page'. ## Assistants In the Assistants Start Page option, first you will find the option to configure the Welcome Message, where you define a title and a description to explain how the assistant will help the end user. Clicking on the 'CONFIRM' button will save this data and the message will appear in the Playground when accessing the assistant, helping the user to get started. In addition, you can add information about specific features of the assistant using the CREATE FEATURE button. Clicking on it opens a window for you to enter a title and a short description about the feature you are describing. After entering the title and description, click on CONFIRM to save the defined feature. Once saved, the feature will be displayed in the Playground along with its description. You can also set up sample prompts by clicking on CREATE EXAMPLE PROMPT. These predefined prompts will appear in the Playground and, when selected, will be completed automatically, facilitating the end user's interaction with the assistant and showing practical suggestions to perform certain tasks. Clicking on the CREATE EXAMPLE PROMPT button opens a window to enter the following information: - Name: Specify a name for the prompt.\n- Description: This field is optional; you can add a brief explanation describing the functionality of the prompt and how it should be used.\n- Text: This field is mandatory and here you define the content that will be used as an example in the prompt. The assistant will take this text to perform the configured task. Once you have entered the information, click on CONFIRM to save the prompt. ## RAG Assistants The RAG Assistants Start Page has a different layout compared to that of Assistants. Instead of the Welcome Message section used in Assistants, for RAG Assistants this section is called Basic Information. Here you define a title and description that clearly explain how the assistant will help end users in their tasks. This information is essential to provide users with information about the assistant's capabilities. Once you have completed the title and description, click on CONFIRM to save the changes. In RAG Assistants, instead of two buttons to create functionalities and examples of prompts, there is only one button: CREATE INFORMATION ITEM. When you click on CREATE INFORMATION ITEM, a window will open where you can enter a title, a description and the text that will make up the body of the information item. In addition, in this window you must select the type of item you want to create: Prompt or Feature. If you choose Prompt, you will define specific examples that will guide the end user when interacting with the assistant. If you select Feature, you will be describing a specific property of the assistant, clearly explaining its usefulness and how it can be used. Once you have completed the required fields, click on the SAVE button, and then click on CONFIRM (in Start Page) to save your changes.",
          "evaluationResultRowContextDocId": "0e63b8cf-b481-4530-8320-a108aaca5167",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "efe1c8ac-500f-4a1b-96a8-d27ee10be0de",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?408",
          "evaluationResultRowContextName": "408",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.7,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "50ba6bea-a58a-49a9-b9c3-d3ab2da79e5d",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0031221800",
      "evaluationResultRowEndDate": "2025-05-13T11:13:43",
      "evaluationResultRowOutput": "Puedes usar un archivo como contexto del prompt de un asistente multimodal en Globant Enterprise AI haciendo referencia directamente al archivo en el texto del prompt con el formato siguiente:\n\n```\n{file:nombre_del_archivo}\n```\n\nPor ejemplo, si tienes un archivo llamado \"product_specs.txt\" cargado en tu proyecto, puedes incluirlo en el prompt as\u00ed:\n\n```\n\"Por favor resume las caracter\u00edsticas clave del producto descritas en {file:product_specs}\".\n```\n\nEl modelo multimodal tomar\u00e1 el contenido del archivo referenciado y lo utilizar\u00e1 para generar la respuesta. Esto simplifica el manejo de archivos porque no necesitas extraer manualmente el contenido del archivo: el sistema lo env\u00eda directamente al modelo para su procesamiento.\n\n### Requisitos importantes:\n- El archivo debe haber sido previamente cargado en tu proyecto.\n- Esta funcionalidad est\u00e1 disponible con modelos multimodales compatibles, como Gemini y OpenAI.\n\nEsta funcionalidad permite que el asistente analice, procese y genere contenido basado en informaci\u00f3n contenida dentro de documentos, im\u00e1genes, audio o video anexados.\n\nM\u00e1s detalles sobre esta funcionalidad la puedes encontrar en la documentaci\u00f3n oficial:  \n[File support for multimodal models](https://wiki.genexus.com/enterprise-ai/wiki?345) [5]\n\n---\n\nSi necesitas soporte para subir el archivo o detalles sobre tipos de archivos compatibles, tambi\u00e9n est\u00e1 especificado en esa secci\u00f3n.\n\n---\n\n### Resumen\n\n- Sube el archivo a tu proyecto.\n- En el prompt escribe `{file:nombre_del_archivo}` para que el asistente lo use en la generaci\u00f3n de la respuesta.\n- Aseg\u00farate de estar usando un modelo multimedial compatible.\n\nEsto facilita el uso de archivos como contexto sin manejo manual de contenido dentro de los prompts.\n\n[5](https://wiki.genexus.com/enterprise-ai/wiki?345)",
      "evaluationResultRowStartDate": "2025-05-13T11:13:35",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "6864"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#Evaluation Plan API The Evaluation Plan API allows you to manage and execute evaluation plans for AI Assistants. These plans define metrics and weights to evaluate the performance of an AI Assistant. Each evaluation plan includes the Assistant to be evaluated, the dataset used and a list of system metrics with their respective weights. The metrics can be, for example, accuracy or fluency, and each has an identifier (Id) and a weight within the plan. The API provides several endpoints to manage these plans, such as creating, updating, deleting and querying evaluation plans, as well as adding, updating or deleting metrics associated with each plan. In addition, all available metrics can be queried. <blockquote>The following endpoints require a Globant Enterprise AI API token related to <strong>project</strong> scope.</blockquote> Check the [[20##Generic Variables|generic variables]] needed to use the API. ## Endpoints | **Method** | **Path** | **Description** |\n|---|---|---|\n| `GET` | `/evaluationPlanApi/evaluationPlans` | Retrieves a list of all evaluation plans. |\n| `POST` | `/evaluationPlanApi/evaluationPlan` | Creates a new evaluation plan. |\n| `GET` | `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}` | Retrieves a specific evaluation plan by ID. |\n| `PUT` | `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}` | Updates a specific evaluation plan by ID. |\n| `DELETE`| `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}` | Deletes a specific evaluation plan by ID. |\n| `GET` | `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetrics` | Retrieves system metrics associated with a specific evaluation plan. |\n| `POST` | `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetric` | Adds a system metric to a specific evaluation plan. |\n| `GET` | `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetric/{systemMetricId}` | Retrieves a specific system metric from an evaluation plan. |\n| `PUT` | `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetric/{systemMetricId}` | Updates a specific system metric within an evaluation plan. |\n| `DELETE`| `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetric/{systemMetricId}` | Deletes a specific system metric from an evaluation plan. |\n| `GET` | `/evaluationPlanApi/systemMetrics` | Retrieves a list of all available system metrics. |\n| `GET` | `/evaluationPlanApi/systemMetric/{systemMetricId}` | Retrieves a specific system metric by ID. |\n| `POST` | `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}` | Executes a specific evaluation plan. | ## GET/evaluationPlanApi/evaluationPlans Retrieves a list of all evaluation plans. ### Request * Method: GET\n* Path: `/evaluationPlanApi/evaluationPlans`\n* Request Body: Empty ### Response The response will be a JSON array containing objects, each representing an evaluation plan. The exact structure of the objects will depend on the implementation, but it will likely include fields such as `evaluationPlanId`, `evaluationPlanName`, `evaluationPlanType`, `evaluationPlanAssistantId`, `evaluationPlanAssistantName`, `evaluationPlanAssistantRevision`, `evaluationPlanProfileName`, `dataSetId` (optional), and a list of `systemMetrics` (each with `systemMetricId` and `systemMetricWeight`). Error information will be included if the request fails. ```json\n{ \"evaluationPlans\": ~~[~~ { \"evaluationPlanId\": \"UUID\", \"evaluationPlanName\": \"string\", \"evaluationPlanType\": \"string\", \"evaluationPlanAssistantId\": \"UUID\", \"evaluationPlanAssistantName\": \"string\", \"evaluationPlanAssistantRevision\": \"string\", \"evaluationPlanProfileName\": \"string\", \"dataSetId\": \"string\", \"systemMetrics\": ~~[~~ { \"systemMetricId\": \"string\", \"systemMetricWeight\": 0.0 } ] } ~~]~~, \"errors\": ~~[~~ ~~]~~\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/evaluationPlanApi/evaluationPlans\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Accept: application/json\"\n``` ## POST/evaluationPlanApi/evaluationPlan Creates a new evaluation plan. ### Request * Method: POST\n* Path: `/evaluationPlanApi/evaluationPlan` #### Request Body A JSON object with the following structure: ```json\n{ \"evaluationPlanName\": \"string\", //Name of the evaluation plan \"evaluationPlanType\": \"string\", //Type of assistant (e.g., \"TextPromptAssistant\", \"RAG Assistant\") \"evaluationPlanAssistantId\": \"UUID\", //ID of the assistant (optional, required for TextPromptAssistant) \"evaluationPlanAssistantName\": \"string\", //Name of the assistant (optional, required for TextPromptAssistant) \"evaluationPlanAssistantRevision\": \"string\", //Revision of the assistant (optional, required for TextPromptAssistant) \"evaluationPlanProfileName\": \"string\", //Name of the RAG profile (optional, required for RAG Assistant) \"dataSetId\": \"string\", //ID of the dataset (optional) \"systemMetrics\": ~~[~~ //Array of system metrics { \"systemMetricId\": \"string\", //ID of the system metric \"systemMetricWeight\": 0.0 //Weight of the system metric (between 0 and 1) } ~~]~~\n}\n``` ### Response A JSON object containing the newly created evaluation plan's data, including a newly generated `evaluationPlanId`. Error details will be included in the response body if the request fails. A typical successful response might look like this: ```json\n{ \"evaluationPlanId\": \"UUID\", \"evaluationPlanName\": \"string\", \"evaluationPlanType\": \"string\", \"evaluationPlanAssistantId\": \"UUID\", \"evaluationPlanAssistantName\": \"string\", \"evaluationPlanAssistantRevision\": \"string\", \"evaluationPlanProfileName\": \"string\", \"dataSetId\": \"string\", \"systemMetrics\": ~~[~~ { \"systemMetricId\": \"string\", \"systemMetricWeight\": \"number\" } ~~]~~, \"errors\": ~~[~~ { \"code\": \"integer\", \"message\": \"string\" } ~~]~~\n}\n``` ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/evaluationPlanApi/evaluationPlan\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Accept: application/json\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"evaluationPlanName\": \"My New Plan\", \"evaluationPlanType\": \"TextPromptAssistant\", \"evaluationPlanAssistantId\": \"existing-assistant-UUID\", \"systemMetrics\": ~~[~~ {\"systemMetricId\": \"metricA\", \"systemMetricWeight\": 0.6}, {\"systemMetricId\": \"metricB\", \"systemMetricWeight\": 0.4} ~~]~~ }'\n``` ## GET/evaluationPlanApi/evaluationPlan/{evaluationPlanId} Retrieves a specific evaluation plan using its ID. ### GET /evaluationPlan/{evaluationPlanId} This endpoint retrieves a specific evaluation plan using its ID. ### Request * Method: GET\n* Path: `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}`\n\ncurl -X GET \"$BASE_URL/evaluationPlanApi/evaluationPlan/a1b2c3d4-e5f6-7890-1234-567890abcdef/evaluationPlanSystemMetric/metricB\" -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" -H \"Accept: application/json\"\n``` ## PUT/evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetric/{systemMetricId} Updates a specific system metric within an existing evaluation plan. ### Request * Method: PUT\n* Path:`/evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetric/{systemMetricId}` #### Request Body A JSON object containing the updated `systemMetricWeight`. The `systemMetricId` should *not* be included in the request body; it is specified in the URL. ```json\n{ \"systemMetricWeight\": \"number\"\n}\n``` ### Response On success, the response might contain the updated system metric or a simple success message. If an error occurs, the response body will contain error details. Here is a successful response example: ```json\n{ \"systemMetricId\": \"string\", \"systemMetricWeight\": \"number\", \"errors\": []\n}\n``` ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/evaluationPlanApi/evaluationPlan/a1b2c3d4-e5f6-7890-1234-567890abcdef/evaluationPlanSystemMetric/metricB\" -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" -d '{ \"systemMetricWeight\": 0.7 }'\n``` ## DELETE/evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetric/{systemMetricId} Deletes a specific system metric from an evaluation plan. ### Request * Method: DELETE\n* Path: `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetric/{systemMetricId}`\n* Request Body: Empty ### Response The response body will typically be empty or contain a minimal success message on successful deletion. If an error occurs, the response body will contain details about the error. A successful response might look like this (though an empty body is also common): ```json\n{ \"message\": \"System metric deleted successfully.\"\n}\n``` ### cURL Sample ```bash\ncurl -X DELETE \"$BASE_URL/evaluationPlanApi/evaluationPlan/a1b2c3d4-e5f6-7890-1234-567890abcdef/evaluationPlanSystemMetric/metricB\" -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" -H \"Accept: application/json\"\n``` ## GET/evaluationPlanApi/systemMetrics Retrieves a list of all available system metrics that can be used in evaluation plans. ### Request * Method: GET\n* Path: `/evaluationPlanApi/systemMetrics`\n* Request Body: Empty ### Response A JSON object containing an array of system metric objects. Each object will likely contain at least a `systemMetricId` and a `systemMetricName` (or similar descriptive field). Here is a successful response example: ```json\n{ \"systemMetrics\": ~~[~~ { \"systemMetricId\": \"string\", \"systemMetricName\": \"string\" } ~~]~~, \"errors\": ~~[~~ { \"code\": \"integer\", \"message\": \"string\" } ~~]~~\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/evaluationPlanApi/systemMetrics\" -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" -H \"Accept: application/json\"\n``` ## GET/evaluationPlanApi/systemMetric/{systemMetricId} Retrieves a specific system metric using its ID. ### Request * Method: GET\n* Path: `/evaluationPlanApi/systemMetric/{systemMetricId}`\n* Request Body: Empty ### Response A JSON object representing the requested system metric. The exact fields will depend on the implementation, but it will likely include at least `systemMetricId` and a descriptive field like `systemMetricName` or `description`. Here is a successful response example: ```json\n{ \"systemMetricId\": \"string\", \"systemMetricName\": \"string\", \"description\": \"string\", \"errors\": ~~[~~ { \"code\": \"integer\", \"message\": \"string\" } ~~]~~\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/evaluationPlanApi/systemMetric/metricB\" -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" -H \"Accept: application/json\"\n``` ## POST/evaluationPlanApi/evaluationPlan/{evaluationPlanId} Initiates the execution of a previously defined evaluation plan. The evaluation plan's configuration (assistant, dataset, metrics, and weights) determines how the assessment is performed. This is likely an asynchronous operation; the response will not contain the full evaluation results immediately. ### Request * Method: POST\n* Path: `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}`\n* Request Body: Empty ### Response The evaluation plan execution was successfully initiated. The response body will likely contain information to track the progress of the evaluation, such as an executionId. ```json\n{ \"executionId\": \"UUID\", \"status\": \"string\", \"message\": \"string\", \"errors\": ~~[~~ { \"code\": \"integer\", \"message\": \"string\" } ~~]~~\n}\n``` ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/evaluationPlanApi/evaluationPlan/a1b2c3d4-e5f6-7890-1234-567890abcdef\" -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" -H \"Accept: application/json\"\n``` ## See Also [[881]] {{{12}}}{{{17}}}\n##MARKDOWN##\n\n* Request Body: Empty ### Response A JSON object representing the requested evaluation plan. The structure will be similar to the one returned by a POST request, including fields like `evaluationPlanId`, `evaluationPlanName`, `evaluationPlanType`, `evaluationPlanAssistantId`, `evaluationPlanAssistantName`, `evaluationPlanAssistantRevision`, `evaluationPlanProfileName`, `dataSetId`, and `systemMetrics`. If the plan is not found or an error occurs, the response body will contain error details. Here is a successful response example: ```json\n{ \"evaluationPlanId\": \"UUID\", \"evaluationPlanName\": \"string\", \"evaluationPlanType\": \"string\", \"evaluationPlanAssistantId\": \"UUID\", \"evaluationPlanAssistantName\": \"string\", \"evaluationPlanAssistantRevision\": \"string\", \"evaluationPlanProfileName\": \"string\", \"dataSetId\": \"string\", \"systemMetrics\": ~~[~~ { \"systemMetricId\": \"string\", \"systemMetricWeight\": \"number\" } ~~]~~, \"errors\": ~~[~~ { \"code\": \"integer\", \"message\": \"string\" } ~~]~~\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/evaluationPlanApi/evaluationPlan/a1b2c3d4-e5f6-7890-1234-567890abcdef\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Accept: application/json\"\n``` ## PUT/evaluationPlanApi/evaluationPlan/{evaluationPlanId} Updates an existing evaluation plan. ### Request * Method: PUT\n* Path: `evaluationPlanApi/evaluationPlan/{evaluationPlanId}` #### Request Body A JSON object with the same structure as the request body for the POST /evaluationPlan endpoint. You can update any of the fields (name, type, assistant ID, profile name, dataset ID, system metrics). You *must* include the `evaluationPlanId` in the request body for the update to work correctly. Only the fields you provide in the request body will be updated; others will remain unchanged. ```json\n{ \"evaluationPlanId\": \"UUID\", \"evaluationPlanName\": \"string\", \"evaluationPlanType\": \"string\", \"evaluationPlanAssistantId\": \"UUID\", \"systemMetrics\": ~~[~~ { \"systemMetricId\": \"string\", \"systemMetricWeight\": \"number\" } ~~]~~\n}\n``` ### Response A JSON object representing the *updated* evaluation plan. This will include the updated fields and any unchanged fields from the original plan. Error details will be included if the request fails. Here is a successful response example: ```json\n{ \"evaluationPlanId\": \"UUID\", \"evaluationPlanName\": \"string\", \"evaluationPlanType\": \"string\", \"evaluationPlanAssistantId\": \"UUID\", \"evaluationPlanAssistantName\": \"string\", \"evaluationPlanAssistantRevision\": \"string\", \"evaluationPlanProfileName\": \"string\", \"dataSetId\": \"string\", \"systemMetrics\": ~~[~~ { \"systemMetricId\": \"string\", \"systemMetricWeight\": \"number\" } ~~]~~, \"errors\": ~~[~~ { \"code\": \"integer\", \"message\": \"string\" } ~~]~~\n}\n``` ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/evaluationPlanApi/evaluationPlan/a1b2c3d4-e5f6-7890-1234-567890abcdef\" -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" -d '{ \"evaluationPlanId\": \"a1b2c3d4-e5f6-7890-1234-567890abcdef\", \"evaluationPlanName\": \"Updated Plan Name\", \"evaluationPlanType\": \"TextPromptAssistant\", \"evaluationPlanAssistantId\": \"new-assistant-UUID\", \"systemMetrics\": ~~[~~ {\"systemMetricId\": \"metricZ\", \"systemMetricWeight\": 0.9} ~~]~~ }'\n``` ### DELETE/evaluationPlanApi/evaluationPlan/{evaluationPlanId} Deletes a specific evaluation plan. ### Request * Method: DELETE\n* Path: `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}`\n* Request Body: Empty ### Response The response body will typically be empty or contain a minimal success message on successful deletion. If an error occurs, the response body will contain details about the error. A successful response might look like this (though an empty body is also common): ```json\n{ \"message\": \"Evaluation plan deleted successfully.\"\n}\n``` ### cURL Sample ```bash\ncurl -X DELETE \"$BASE_URL/evaluationPlanApi/evaluationPlan/a1b2c3d4-e5f6-7890-1234-567890abcdef\" -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" -H \"Accept: application/json\"\n``` ## GET/evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetrics Retrieves the system metrics associated with a specific evaluation plan. ### Request * Method: GET\n* Path: `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetrics`\n* Request Body: Empty ### Response A JSON object containing an array of system metrics associated with the specified evaluation plan. Each system metric object will likely contain at least `systemMetricId` and `systemMetricWeight`. Here is a successful response example: ```json\n{ \"systemMetrics\": ~~[~~ { \"systemMetricId\": \"string\", \"systemMetricWeight\": \"number\" } ~~]~~, \"errors\": ~~[~~ { \"code\": \"integer\", \"message\": \"string\" } ~~]~~\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/evaluationPlanApi/evaluationPlan/a1b2c3d4-e5f6-7890-1234-567890abcdef/evaluationPlanSystemMetrics\" -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" -H \"Accept: application/json\"\n``` ## POST/evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetric Adds a new system metric to an existing evaluation plan. ### Request * Method: POST\n* Path: `evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetric` #### Request Body A JSON object specifying the system metric to add: ```json\n{ \"systemMetricId\": \"string\", \"systemMetricWeight\": \"number\"\n}\n``` ### Response On success, the response body might contain the updated list of system metrics for the evaluation plan or a simple success message. If an error occurs, the response body will contain details about the error. Here is a successful response example: ```json\n{ \"systemMetrics\": ~~[~~ { \"systemMetricId\": \"string\", \"systemMetricWeight\": \"number\" } ~~]~~, \"errors\": ~~[~~ { \"code\": \"integer\", \"message\": \"string\" } ~~]~~\n}\n``` ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/evaluationPlanApi/evaluationPlan/a1b2c3d4-e5f6-7890-1234-567890abcdef/evaluationPlanSystemMetric\" -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" -d '{ \"systemMetricId\": \"metricC\", \"systemMetricWeight\": 0.2 }'\n``` ## GET/evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetric/{systemMetricId} Retrieves a specific system metric from a given evaluation plan. ### Request * Method: GET\n* Path: `/evaluationPlanApi/evaluationPlan/{evaluationPlanId}/evaluationPlanSystemMetric/{systemMetricId}`\n* Request Body: Empty ### Response A JSON object representing the requested system metric. It will at least contain `systemMetricId` and `systemMetricWeight`. Here is a successful response example: ```json\n{ \"systemMetricId\": \"string\", \"systemMetricWeight\": \"number\", \"errors\": ~~[~~ { \"code\": \"integer\", \"message\": \"string\" } ~~]~~\n}\n``` ### cURL Sample ```bash",
          "evaluationResultRowContextDocId": "cd13b648-0892-4a46-99aa-c613754d8534",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "0fc2127f-cfc1-4363-b061-6b00df63ac03",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?849",
          "evaluationResultRowContextName": "849",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Troubleshooting Flows with RAG when Action is Not Taken if Responses are Not Found In **Globant Enterprise AI** [[321|Flows]], it is possible to add a [[44|RAG]] type node that can be configured to perform an action if it doesn't find the answer in the provided documents. This way, when the answer is found in the documents, the flow follows a certain action; otherwise, it executes another one, such as searching in another RAG, redirecting to another assistant or any other option by means of a new node. However, on some occasions, the flow may continue without performing the configured action when the response is not found in the first RAG. To avoid this problem, it is necessary to follow these steps: 1. Ask a question that is relevant to the context of your RAG. The purpose is for the RAG to be able to match with a high confidence level, which implies that the question should be aligned with the information available in the documents configured for the RAG. 2. Access the [[42]], go to the **[[563]]** section, and look for the interaction corresponding to the question asked in step 1. 3. Select **SearchChat** in the **Module** column, and verify that the **Assistant Name** column lists the RAG assistant that answered the question in step 1, and that the **Input** column lists the question asked in step 1. 4. Go to the **General (JSON)** tab and search within the **Provider Response** for the highest score assigned to the response given by the RAG. Keep in mind that it is not enough to consider only the highest score, it is also crucial to evaluate whether the answer makes sense in the context of the question asked. For example, if a question is asked that clearly cannot be answered by the RAG, such as \"Who is Mickey Mouse?\", and the score is low, such as 0.2, but the response indicates that it cannot provide relevant information, this reflects low confidence and an irrelevant response. In these cases, it is advisable to derive the flow to a \"no match\" logic to avoid inaccurate answers. 5. Go back to the Flow Builder, locate the corresponding flow, and select the RAG node that was configured to handle the interaction. 6. Edit the RAG node by clicking on the **Edit** icon to the right of the node, and in the left side menu (**State Configuration**), set the **Acceptable Confidence Level**. This parameter defines the minimum confidence threshold that the system will accept before determining whether the response provided is adequate.\nThe default value of Acceptable Confidence Level is 0.2\u2014recommended by the GPT model\u2014but it may vary depending on the amount of information in the RAG, the documents available and the model configured.\nIf in step 4 you noticed a high score, for example, 0.537278056, you can set the Acceptable Confidence Level to 0.5, making sure that only answers with a confidence greater than or equal to this value are accepted.\nIf you prefer the assistant to refer to a \"no match\" flow when the score is low (as in the Mickey Mouse example), set a higher confidence level. If, instead, you want the assistant to continue within the same flow, providing a response indicating that it doesn't have enough information, you can maintain a lower confidence level. {{{12}}}{{{17}}}\n##MARKDOWN##",
          "evaluationResultRowContextDocId": "d610330e-af78-40c7-a2c2-79c4ae726d72",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "22aa0de3-8dca-4bc7-8908-d05c2175a107",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?445",
          "evaluationResultRowContextName": "445",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Chat with Documents API **Deprecated**: Replaced by [Chat API](34.html). This API enables searches or queries on the indexed content. In this documentation, you will find detailed information on the available endpoints and how to interact with them. To manage settings associated with your RAG Assistants (for example, k, model, historyCount), read the [RAG Assistants Section](64.html). Check the [generic variables](20.html) needed to use the API. > The following endpoints require a Globant Enterprise AI API token related to **project** scope. ## [Endpoints](#Endpoints) The Globant Enterprise AI Chat with documents API provides the following endpoints: | Method | Path | Description |\n|---|---|---|\n| POST | /execute | Executes a search query| ## [POST/execute ](#POST%2Fexecute+) Executes a search query based on a specific profile and question. ### [Parameters](#Parameters) | Parameter | Type | Description |\n|---|---|---|\n| id | string | Identifier for the conversation |\n| profile | string | Profile to search |\n| question | string | Question to ask |\n| [variables](85.html) | collection | A list of key/value properties (optional) |\n| filters | collection | List of filters to apply (optional) | For conversations with history, use the optional `id` element to refer to a particular conversation. These conversations will keep the `History Count` parameter from your [RAG Assistants](64.html). If no `id` value is set, no history will be considered and your query will be treated as a one-off. The `variables` parameter is used to fill in an associated prompt with values. #### [Filters](#Filters) The `filters` parameter is used as a logical condition statement; check the detail [here](179.html); these are predefined filters you can use. | Filter | Description |\n|---|---|\n| id | Document GUID returned during [insertion](29.html) |\n| name | Original document name |\n| extension | Original document extension |\n| source | Document source, in general, a URL | To use specific ones, remember to ingest documents with the correct metadata. A valid filters section is: ```json\n\"filters\": [ {\"key\": \"extension\", \"operator\": \"$ne\", \"value\": \"pdf\"}, {\"key\": \"name\", \"operator\": \"$eq\", \"value\": \"sample\"}, {\"key\": \"year\", \"operator\": \"$gte\", \"value\": 2000} /* year added during ingestion */\n]\n``` ### [Request Body](#Request+Body) ```json\n{ \"id\": \"string\", /* optional */ \"profile\": \"string\", \"question\": \"string\", \"variables\": [ {\"key\": \"string\", \"value\": \"string\"}, ... ], \"filters\": [ { \"key\": \"string\", \"operator\": \"string\", /* Optional */ \"value\": \"string or number\" }, ... ]\n}\n``` ### [Response](#Response) StatusCode `200` is displayed with Content-Type: application/json ```json\n{ \"documents\": [ // optional: could not return elements when no information is present { \"pageContent\": \"Example page content\", // mandatory \"score\": 0.9, // optional \"metadata\": { // mandatory \"source\": \"Example source\", // optional \"url\": \"Example source\", // optional \"description\": \"Example description\", // optional \"id\": \"12345\", // mandatory \"name\": \"Example document\", // mandatory \"extension\": \"docx\", // mandatory \"loc\": { \"lines\": { // optional section \"from\": 1, \"to\": 1, }, \"pageNumber\": 1 // optional } } } ], \"id\": \"someId\", // mandatory \"requestId\": \"someId\", // mandatory \"text\": \"Example reply\", // mandatory \"result\": { // mandatory \"success\": true, // mandatory \"messages\": [ // optional \"Search query executed successfully\" ] }\n}\n``` The returned `score` element (when available) measures the semantic similarity between the `question` and the associated `pageContent`; a value between 0 and 1 where 1 is closest. You can use the `requestId` element to review the Request detail in the console. StatusCode `200` is shown with Content-Type: application/json, when RAG Assistant does not exist or is disabled: ```json \"error\": { \"code\": 1101, \"message\": \"Search Index Profile Name not found\" }, \"status\": \"failed\", \"success\": false, \"text\": \"\"\n}\n``` ### [Sample cURL Request](#Sample+cURL+Request) ```bash\n# Simple case\ncurl -X POST -H \"Content-Type: application/json\" -H \"Authorization: $SAIA_PROJECT_APITOKEN\" -d '{ \"profile\": \"Default\", \"question\": \"Explain to me what is SAIA?\"\n}' $BASE_URL/v1/search/execute\n# Using variables and filters\ncurl -X POST -H \"Content-Type: application/json\" -H \"Authorization: $SAIA_PROJECT_APITOKEN\" -d '{ \"profile\": \"Default\", \"question\": \"Again, explain to me what is SAIA?\", \"variables\": [ {\"key\": \"type\",\"value\": \"Doc\"} ], \"filters\": [ {\"key\": \"extension\", \"operator\": \"$ne\", \"value\": \"pdf\"}, {\"key\": \"name\", \"operator\": \"$eq\", \"value\": \"sample\"}, {\"key\": \"year\", \"operator\": \"$gte\", \"value\": 2000} ]\n}' $BASE_URL/v1/search/execute ```",
          "evaluationResultRowContextDocId": "c09d9433-e6eb-43a3-97ec-f1b7c9664405",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "42b99614-04b3-40e2-b0b2-ffe1b843eed3",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?33",
          "evaluationResultRowContextName": "33",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Feedback API This API enables you to associate feedback from the Assistant answers. Check the [[20##Generic Variables|generic variables]] needed to use the API. ## Endpoints Feedback API provides the following endpoints: |**Method**|**Path**|**Description**|\n|---|---|---|\n|`POST`|`/feedback/request/{requestId}`|Add feedback to a request associated with a user's execution; note the `requestId` parameters are returned on every assistant reply.| ## POST/feedback/request/{requestId} Updates an assistant feedback using the associated request identifier (`requestId`). |**Parameter**|**Type**|**Description**|\n|---|---|---|\n|`origin`|`string`|Origin for the feedback, use `user-feedback`|\n|`answerScore`|`number`|Associated feedback: `1` good, `2` bad |\n|`comments`|`string`|Associated feedback comment (optional)| ### Request body <pre class=\"Code\">\n{ \"origin\": \"user-feedback\", \"answerScore\": 2, \"comments\": \"this is a wrong anwer!!\"\n}\n</pre> The expected response is a 200 `statusCode` and empty (`{}`) json ## Samples [regions]\n[region|cURL]\n```\ncurl --location '$BASE_URL/v1/feedback/request/{requestId}' \\\n--header 'Content-Type: application/json' \\\n--header 'X-Saia-Cache-Enabled: false' \\\n--header 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n--data '{ \"origin\": \"user-feedback\", \"answerScore\": 2, \"comments\": \"bad answer!\"\n}'\n```\n[/region]\n[region|Python]\n```python\nimport requests\nimport json url = f\"{BASE_URL}/v1/feedback/request/{requestId}\" payload = json.dumps({ \"origin\": \"user-feedback\", \"answerScore\": 2, \"comments\": \"bad answer!\"\n})\nheaders = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {SAIA_PROJECT_APITOKEN}',\n} response = requests.request(\"POST\", url, headers=headers, data=payload)\n```\n[/region]\n[/regions] ## See Also [[22|Organization API]] [[20|API Reference]] {{{12}}} {{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "0bab3fbb-55ec-4326-86df-4603b0620ab2",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "a7a54af4-5d72-489b-9a2f-a3d69275a335",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?407",
          "evaluationResultRowContextName": "407",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#RAG samples with Chat API This article describes samples for interacting with [[44|RAG Assistants]] using the [[34]]. Below, you will find the parameters and structures needed to run queries with filters, variables, and conversational context. ## Executing queries with filters and variables The request body allows you to customize the query through the following parameters: | **Parameter**| **Type** | **Description** |\n| --- | --- | --- |\n| threadId | string | Identifier for the conversation.|\n| model<sup>1</sup> | string | Assistant to use considering the format `saia:search:assistant_name`.|\n| messages<sup>1</sup> | collection| List of messages including the associated question.|\n| [[85|variables]]| collection| A list of `key/value` properties (optional).|\n| filters| collection| List of filters to apply (optional).|\n| stream | boolean| Streaming support.| <sup>1</sup>mandatory For conversations with history, use the optional `threadId` element to refer to a particular conversation. These conversations will keep the History Count parameter from your [[64|RAG Assistants]]. If no `threadId` value is set, no history will be considered and your query will be treated as a one-off. The **variables** parameter is used to fill in an associated prompt with values. ### Filters The [[33##Filters|filters]] parameter is used as a logical condition statement. Check the details in [[179]]. These are predefined filters you can use: |**Filter**| **Description**| |--- | --- |\n|id| Document GUID returned during [[29##POST/profile/{name}/document|insertion]].| |name| Original document name.| |extension|Original document extension.|\n|source|Document source; in general, a URL.| To use specific metadata filters, remember to ingest documents with the correct metadata. #### **Filter section sample** A valid filter section is as follows: ```\n\"filters\": ~~[~~ {\"key\": \"extension\", \"operator\": \"$ne\", \"value\": \"pdf\"}, /* any extension different from pdf */ {\"key\": \"name\", \"operator\": \"$eq\", \"value\": \"sample\"}, /* filename is sample*/ {\"key\": \"year\", \"operator\": \"$gte\", \"value\": 2000} /* year greather than or equal 2000, notice the year was added during ingestion */\n~~]~~\n``` ### Request body Below is the structure of a valid Request body: ```\n{ \"threadId\": \"string\", /* optional */ \"messages\": ~~[~~ { \"role\": \"user\", \"content\": \"question\" // string } ~~]~~, \"variables\": ~~[~~ /* optional */ {\"key\": \"string\", \"value\": \"string\"}, ... ~~]~~, \"filters\": ~~[~~ /* optional */ { \"key\": \"string\", \"operator\": \"string\", /* Optional */ \"value\": \"string or number\" }, ... ~~]~~\n}\n``` ### Response Below is a sample Response structure with StatusCode `200` and `Content-Type: application/json`: ```\n{ \"documents\": ~~[~~ // optional: could not return elements when no information is present { \"pageContent\": \"Example page content\", // mandatory \"score\": 0.9, // optional \"metadata\": { // mandatory \"source\": \"Example source\", // optional \"url\": \"Example source\", // optional \"description\": \"Example description\", // optional \"id\": \"12345\", // mandatory \"name\": \"Example document\", // mandatory \"extension\": \"doc\", // mandatory // other key-value parameters can be exposed depending on the ingestion } } ~~]~~, \"id\": \"someId\", // mandatory \"requestId\": \"someId\", // mandatory \"text\": \"Example reply\", // mandatory \"result\": { // mandatory \"success\": true, // mandatory \"messages\": ~~[~~ // optional \"error messages if available\" ~~]~~ }, \"usage\": { // optional \"total_tokens\": number, \"completion_tokens\": number, \"prompt_tokens\": number, \"prompt_cost\": number, \"completion_cost\": number, \"total_cost\": number, \"currency\": \"USD\", \"prompt_tokens_details\": { \"cached_tokens\": number }, \"completion_tokens_details\": { \"reasoning_tokens\": number } }\n}\n``` The returned score element (when available) measures the semantic similarity between the `question` and the associated `pageContent`. This score is a value between `0` and `1`, where `1` represents the closest similarity. You can use the `requestId` element to review the details of the [[563]] in the [[42]]. ### Usage The `usage` element is available since the 2025-03 release. [regions]\n[region|Standard response sample]\n<pre class=\"Code\">\n{ \"documents\": [...], \"text\": \"here the response...\", \"result\": { \"success\": true, \"messages\": [...] }, \"requestId\": \"GUID\", \"usage\": { \"total_tokens\": 958, \"completion_tokens\": 73, \"prompt_tokens\": 885, \"prompt_cost\": 0.000132, \"completion_cost\": 0.0000438, \"total_cost\": 0.0021759, \"currency\": \"USD\" }\n}\n</pre>\n[/region]\n[region|Stream response sample]\nWhen using the stream option, the usage element will be returned within the `saia.rag.documents` event.\n<pre class=\"Code\">\n{ \"event\": \"saia.rag.documents\", \"data\": { \"documents\": [...], \"text\": \"\", \"result\": { \"success\": true, \"messages\": [] }, \"usage\": { \"total_tokens\": 1024, \"completion_tokens\": 56, \"prompt_tokens\": 968, \"prompt_cost\": 0.0001449, \"completion_cost\": 0.0000336, \"total_cost\": 0.00217854, \"currency\": \"USD\", \"prompt_tokens_details\": { \"cached_tokens\": 0 }, \"completion_tokens_details\": { \"reasoning_tokens\": 0 } } }\n}\n</pre>\n[/region]\n[/regions] ### StatusCode StatusCode `200` is shown with `Content-Type: application/json`, when RAG Assistant does not exist or is disabled: ```\n\"error\": { \"code\": 1101, \"message\": \"Search Index Profile Name not found\" }, \"status\": \"failed\", \"success\": false, \"text\": \"\"\n}\n``` ## Simple query sample The default RAG assistant is consulted. ### Request Body ```\n{ \"model\": \"saia:search:Default\", \"messages\": ~~[~~ { \"role\": \"user\", \"content\": \"Explain to me what is Enterprise AI?\" } ~~]~~\n}\n``` ### cURL Sample ```\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"saia:search:Default\", \"messages\": ~~[~~ { \"role\": \"user\", \"content\": \"Explain to me what is Enterprise AI?\" } ~~]~~\n}'\n``` ## Sample with variables and filters ### Request body ```\n{ \"model\": \"saia:search:Default\", \"messages\": ~~[~~ { \"role\": \"user\", \"content\": \"Again, explain to me what is Enterprise AI?\" } ~~]~~, \"filters\": ~~[~~ {\"key\": \"extension\", \"operator\": \"$ne\", \"value\": \"pdf\"}, {\"key\": \"name\", \"operator\": \"$eq\", \"value\": \"sample\"}, {\"key\": \"year\", \"operator\": \"$gte\", \"value\": 2000} ~~]~~, \"variables\": ~~[~~ {\"key\": \"type\", \"value\": \"Doc\"} ~~]~~\n}\n``` ### cURL Sample ```\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"saia:search:Default\", \"messages\": ~~[~~ { \"role\": \"user\", \"content\": \"Again, explain to me what is Enterprise AI?\" } ~~]~~, \"filters\": ~~[~~ {\"key\": \"extension\", \"operator\": \"$ne\", \"value\": \"pdf\"}, {\"key\": \"name\", \"operator\": \"$eq\", \"value\": \"sample\"}, {\"key\": \"year\", \"operator\": \"$gte\", \"value\": 2000} ~~]~~, \"variables\": ~~[~~ {\"key\": \"type\", \"value\": \"Doc\"} ~~]~~\n}'\n``` {{{12}}}{{{17}}}\n##MARKDOWN##",
          "evaluationResultRowContextDocId": "2c975a3b-88bd-40cf-bdd4-a57f33e6eea1",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "eee273dd-6643-401d-b9a4-0a95f687b69b",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?712",
          "evaluationResultRowContextName": "712",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.8,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "5386353a-1ae5-4960-b2e0-2acf9ba2ebfe",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0043402800",
      "evaluationResultRowEndDate": "2025-05-13T11:13:51",
      "evaluationResultRowOutput": "The `score` element in the response measures the semantic similarity between the user's question and the associated document's content (`pageContent`). It is a numeric value between `0` and `1`, where `1` represents the closest or highest similarity. This score helps determine how relevant or confident the system is about the answer provided based on the indexed documents.\n\nFor example, a higher score indicates that the document's content closely matches the user's query, implying a more accurate or relevant response.\n\nThis is used in the RAG Assistants and related search APIs to decide whether the response is acceptable or if fallback actions should be taken in conversation flows.\n\nYou can find this explanation on the \"RAG samples with Chat API\" page and in \"Chat with Documents API\" documentation:\n\n- [RAG samples with Chat API](https://wiki.genexus.com/enterprise-ai/wiki?712)  \n- [Chat with Documents API](https://wiki.genexus.com/enterprise-ai/wiki?33)  \n\nAlso, in troubleshooting RAG flows, the confidence score (`score`) is used to determine if the response should be accepted or routed differently depending on a configured threshold called **Acceptable Confidence Level**:  \n- [Troubleshooting Flows with RAG when Action is Not Taken if Responses are Not Found](https://wiki.genexus.com/enterprise-ai/wiki?445)  \n\n---\n\n### Summary\n- `score` represents semantic similarity between question and document content.\n- Range: 0 to 1 (1 = highest similarity).\n- Used to assess confidence in responses by RAG Assistants.\n- Influences conversational flow decisions based on confidence thresholds.\n\n[2](https://wiki.genexus.com/enterprise-ai/wiki?712)  \n[3](https://wiki.genexus.com/enterprise-ai/wiki?33)  \n[7](https://wiki.genexus.com/enterprise-ai/wiki?445)",
      "evaluationResultRowStartDate": "2025-05-13T11:13:43",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "9849"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "- New Average Request Time metric added in the Project Dashboard. ### January 8th - The option formerly known as 'Search Documents' has been improved and renamed to RAG Assistant (Retrieval Augmented Generation) to provide an optimized experience when searching and generating information. #### Frontend - Feedback is provided during conversations with RAG Assistants, indicating where you are in the process.\n- 'Response streaming' support for RAG Assistants.\n- Settings are hidden when selecting an assistant, except when 'Chat with LLMs' is selected. ## 2023 ### December 19th - Fixed: Too Many Redirects when accessing Playground using a browser in Spanish language. ### December 6th - New backoffice design.\n- Access to the Playground from the backoffice to chat with the assistants defined in the project.\n- Upload images for analysis with GPT-4 Vision.\n- Google Analytics support at the frontend.\n- Keep a conversation thread when chatting with documents.\n- An email notification is sent automatically when a new member is invited to join the organization or project. ### November 28th - First version officially released!! ### November 6th - The following OpenAI models are supported: GTP-4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), and GPT-4 Vision (gpt-4-vision-preview). ### October 18th - [Manage quotas per project](71.html): It is possible to set quota limits to control project expenses or usage. ### October 11th - [AI-Driven Load Balancing](32.html): The platform automatically manages the Load Balancing process when you work with generative AI providers, efficiently addressing the limits imposed by LLM platforms.\n\n- In the [Organization API](22.html), the ability to set and manage usage limits on projects through the [POST /project](22.html) and [GET /project/{id}](22.html) endpoints has been added.\n- [Quota Limit](71.html) now includes improvements such as highlighting the active quota in green, offering options to cancel active quotas, among others. ### October 17th - Rebranding to Globant Enteprise AI\n- Improvements in RAG - New option not to [compact the conversation history](297.html) - Support for Redis, MongoDB Atlas and PG Vector [vectorstores](267.html) - New endpoint to get an embedding. Check details here: [Embeddings API](422.html)\n- Playground improvements\n- File management improvements\n- New [LLMs supported](200.html) - NVIDIA provider with new models supported - nvidia.nemotron-mini-4b-instruct - meta.llama-3.1-8b-instruct - meta.llama-3.1-70b-instruct - meta.llama-3.1-405b-instruct - meta.llama-3.2-3b-instruct - Groq provider supported - groq/llama-3.1-70b-versatile - groq/llama-3.2-11b-vision-preview - groq/llama-3.2-3b-previewgroq/llama-3.2-1b-preview\n- New [embeddings models](447.html) added - Vertex AI: - vertex_ai/textembedding-gecko - vertex_ai/text-embedding-004 - vertex_ai/textembedding-gecko-multilingual - Nvidia: - nvidia/nvclip - nvidia/nv-embed-v1 - nvidia/baai.bge-m3 - nvidia/snowflake.arctic-embed-l - nvidia/nv-embedqa-mistral-7b-v2 - nvidia/embed-qa-4 - nvidia/nv-embedqa-e5-v5 ### September 25th - [Support for file](346.html) processing with prompt-based assistants. This will enable many scenarios, such as uploading documents and summarizing, extracting, and checking information, etc. Also, depending on the model used by the assistant, it will be able to process audio, video, or images.\n- [Support for multi-modal](345.html) LLMs allow processing docs, audio, video, and images in models like GPT-4o or Gemini Pro.\n- Chat with data assistants - The model used to build the queries was updated with GPT-4o, which improves the quality of the generated query. - Configure the query builder server by organization and project. This means you can connect with different DBMS from each project when building Chat with data assistants. - Show an explanation of how the query was built.\n- New [Playground Interface](349.html) design - New design - Upload documents from the front end to chat with them.\n- [Flows builder](321.html) - There will be two types of Flows, one more oriented to build a conversational UI and the other to build assistant flows. Access to these flows will only be available through [Chat API](34.html) or through the [channels](384.html) offered by Flows.\n- New models hosted in AWS Bedrock added: - Amazon Titan Express v1 - Amazon Titan Lite v1 - Anthropic Claude 3 Haiku - Anthropic Claude 3 Sonnet - Anthropic Claude 3.5 Sonnet - Cohere Command - Meta Llama 3 8B - Meta LLama 3 70B\n- It is now possible to provide clear guidance on the assistant's capabilities, allowing you to add information such as descriptions, features, and example prompts. This configuration can be done from the Backoffice, [Start Page](408.html), or WelcomeData section of the [Assistant API](35.html) and [RAG Assistants API](29.html) endpoints.\n- RAG Assistants - New endpoint in [RAG Assistants API](29.html): using [DELETE/profile/{name}/documents](29.html), all documents associated with the specified RAG Assistant are deleted. ### August 9th - Support of new models - GPT-4o mini\n- RAG Assistants - New option called [CLEANUP](60.html) allows to delete the documents associated to a RAG Assistant. - When creating a new assistant, the following defaults are updated: - [Chunk Count](69.html) updated from 2 to 5. - [Model Name](68.html) from gpt-3.5-turbo to gpt-4o-mini.\n- Data Analyst Assistant - New [Conversational Module](291.html) for Data Analyst Assistant.\n- Considerations - **Important**: [max_tokens integer below minimum value](290.html) error may occur if you are using the _max_tokens_ value as -1; those Assistants must be updated to use a correct positive value. ### July 4th - Permissions by assistants in role-based access control. A new [role and permissions management system](242.html) has been implemented to provide greater flexibility and control. Please read [Migration to the new Roles and Permissions Management System](264.html).\n- Dashboard performance optimizations. Please read [Using the Request Summary Table for Improved Dashboard Performance](273.html).\n- RAG Improvements - New option to customize the [context](205.html). - Support for [Max Marginal Relevance](206.html) - Support for _similarity_hybrid_ and _semantic_hybrid_ for [Azure AISearch](207.html) - Advanced options for SelfQuery retriever prompt case. See [Self Query Use Case](150.html) - Support for Parent Document retrieval option. See [Parent Document Retrieval Strategy](204.html) ### June 10th - [Enterprise AI Proxy](19.html) is deprecated. Use [Chat API](34.html) instead.\n- Support for new LLMs - OpenAI new model GPT-4o - Models in Google Vertex - Gemini 1.0 Pro - Gemini 1.5 Flash preview-0514 - Gemini 1.5 Pro preview-0514 - Claude 3 Haiku - Claude 3 Opus - Claude 3 Sonnet\n- RAG Improvements - New option to initialize RAG Assistant based on another when creating a new RAG Assistant. - New option to export document list in View Documents over a RAG Assistant. - Added filter options when browsing Documents. - SelfQuery RAG retriever partial support for a [customized Prompt](150.html). - Support for text-embedding-004 in Google models to generate the embeddings.\n- Deprecated [Assistant API](35.html) endpoints. - /assistant/text/begin - /assistant/text\n- Support to deploy in Google Cloud Platform. ### May 8th - New [Chat with Data Assistant](159.html).\n- New [Ingestion SDK](172.html) to automate document ingestion in RAG assistants.\n- New models hosted in NVIDIA platform supported. See [Supported Chat Models](200.html) for more details.\n- New option to export information about projects and members available for the organization administrator.\n- New API to extend dataset for [Data Analyst Assistant 1.0](89.html).\n- New filter by user email in Requests.\n- Update default to use _text-embedding-3-small_ OpenAI Embeddings for new RAG assistants.\n- Support for gemini-1.5-pro-preview-0409 model added. ### April 3rd - New [API Assistant](110.html).\n- New [Data Analyst Assistant 1.0](89.html).\n- The user Dashboard displays the email to identify the most active user or the one who generates the most costs. ### March 11th - GeneXus Identity Provider is implemented, expanding the login options in the [Backoffice](42.html) of the production environment. This allows for login not only with Google but also with Apple or GeneXus Account.\n- It is possible to customize the icon for each assistant. ### February 29th - [Frontend](82.html) improvements in UI/UX.\n- Option to get feedback from end users when interacting with [RAG Assistant](44.html).\n- Gemini Pro LLM support.\n- New [Dashboard](562.html) with user metrics.",
          "evaluationResultRowContextDocId": "68ace6a6-8b1b-4736-a466-9ed0a67b97a9",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "befea3f6-2082-4087-bc3b-cb0c94ab1ee0",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?31",
          "evaluationResultRowContextName": "31",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Usage Limits API This API allows managing usage limits for both projects and organizations, providing functionality to define, retrieve, update, and delete usage limits. To use the API, check the [[20##Generic Variables|generic variables]] needed. In addition, read the explanation about usage limits in [[666]] and [[71]]. ## Endpoints | **Method** | **Path** | **Description** |\n|---|---|---|\n| `POST` | `/usageLimits/organizations/{organization}/limits` | Defines a new usage limit for an organization |\n| `GET` | `/usageLimits/organizations/{organization}/limits/latest` | Retrieves the latest usage limit defined for the organization |\n| `GET` | `/usageLimits/organizations/{organization}/limits` | Retrieves all usage limits defined for the organization |\n| `DELETE` | `/usageLimits/organizations/{organization}/limits/{id}` | Deletes a usage limit for the organization |\n| `PUT` | `/usageLimits/organizations/{organization}/limits/{id}/hardLimit` | Modifies the `hardLimit` of an active usage limit for the organization |\n| `PUT` | `/usageLimits/organizations/{organization}/limits/{id}/softLimit` | Modifies the `softLimit` of an active usage limit for the organization |\n| `PUT` | `/usageLimits/organizations/{organization}/limits/{id}/renewalStatus` | Modifies the `renewalStatus` of an active usage limit for the organization |\n| `POST` | `/usageLimits/organizations/{organization}/projects/{project}/limits` | Defines a new usage limit for a project |\n| `GET` | `/usageLimits/organizations/{organization}/projects/{project}/limits` | Retrieves all usage limits for a project |\n| `GET` | `/usageLimits/organizations/{organization}/projects/{project}/limits/latest` | Retrieves the latest usage limit defined for a project |\n| `GET` | `/usageLimits/organizations/{organization}/projects/{project}/limits/active` | Retrieves the active usage limit for a project |\n| `DELETE` | `/usageLimits/organizations/{organization}/projects/{project}/limits/{id}` | Deletes an active usage limit for a project |\n| `PUT` | `/usageLimits/organizations/{organization}/projects/{project}/limits/{id}/hardLimit` | Modifies the `hardLimit` for an active usage limit for a project |\n| `PUT` | `/usageLimits/organizations/{organization}/projects/{project}/limits/{id}/softLimit` | Modifies the `softLimit` for an active usage limit for a project |\n| `PUT` | `/usageLimits/organizations/{organization}/projects/{project}/limits/{id}/renewalStatus` | Modifies the `renewalStatus` for an active usage limit for a project | ## POST/usageLimits/organizations/{organization}/limits Defines a new usage limit for an organization. <blockquote>This endpoint requires a Globant Enterprise AI API token related to <strong>organization</strong> scope.</blockquote> ### Parameters | Name | Type | Description |\n|---|---|---|\n| `subscriptionType` | `string` | Type of subscription. Options: `Freemium`, `Daily`, `Weekly`, `Monthly` |\n| `usageUnit` | `string` | Unit for the usage limit. Options: `Cost` |\n| `softLimit` | `number` | Soft usage limit. |\n| `hardLimit` | `number` | Hard usage limit. |\n| `renewalStatus` | `string` | Whether the limit is renewable. Options: `Renewable`, `NonRenewable` | ### Request Body ```json\n{ \"subscriptionType\": \"string\", // Options: Freemium, Daily, Weekly, Monthly \"usageUnit\": \"string\", // Options: Cost (Only Cost is allowed for organization limits) \"softLimit\": number, // Recommended usage limit \"hardLimit\": number, // Maximum allowed usage \"renewalStatus\": \"string\" // Options: Renewable, NonRenewable\n}\n``` ### Response If the endpoint is successful, it will return a `201 Created` status with the usage limit data: ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` Error responses will return appropriate HTTP status codes (400 Bad Request, 401 Unauthorized, 403 Forbidden, 409 Conflict) with a JSON body containing an errors array: ```json\n{ \"errors\": [ { \"id\": integer, \"description\": string } ]\n}\n``` ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/v2/usageLimits/organizations/${organization}/limits\" \\ -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" \\ -H \"Accept: application/json\" \\ -d '{ \"subscriptionType\": \"Monthly\", \"usageUnit\": \"Cost\", \"softLimit\": 10.00, \"hardLimit\": 20.00, \"renewalStatus\": \"Renewable\" }'\n``` ## GET/usageLimits/organizations/{organization}/limits/latest Retrieves the latest usage limit defined for the organization. <blockquote>This endpoint requires a Globant Enterprise AI API token related to <strong>organization</strong> scope.</blockquote> ### Response If the endpoint is successful, it will return a `200 OK` status with the latest usage limit data: ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v2/usageLimits/organizations/${organization}/limits/latest\" \\ -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" \\ -H \"Accept: application/json\" ``` ## GET/usageLimits/organizations/{organization}/limits Retrieves all usage limits defined for a specified organization. <blockquote>This endpoint requires a Globant Enterprise AI API token related to <strong>organization</strong> scope.</blockquote> ### Response Returns a JSON object containing an array of usageLimits for the specified organization. If no usage limits exist for the organization, an empty array will be returned. ```json\n{ \"usageLimits\": [ { \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Organization\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime }, // ... more usageLimits (if any) ]\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v2/usageLimits/organizations/${organization}/limits\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" ``` ## DELETE/usageLimits/organizations/{organization}/limits/{id} Deletes an active usage limit for the organization. <blockquote>This endpoint requires a Globant Enterprise AI API token related to <strong>organization</strong> scope.</blockquote> ### Response If the endpoint is successful, it will return a `200 OK` status with the limit status set to \"Cancelled\": ```json\n\n{ \"usageLimits\": [ { \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Project\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime }, // ... more usageLimits (if any) ]\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v2/usageLimits/organizations/${organization}/projects/${project}/limits\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" ``` ## GET/usageLimits/organizations/{organization}/projects/{project}/limits/latest Retrieves the latest usage limit defined for a specified project within a given organization. <blockquote>This endpoint can be used whether the Globant Enterprise AI API token is related to the organization scope or to the project scope.</blockquote> ### Response Returns a JSON object representing the latest usage limit for the project. If no usage limit exists, a 404 Not Found error is returned. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Project\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v2/usageLimits/organizations/${organization}/projects/${project}/limits/latest\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" ``` ## GET/usageLimits/organizations/{organization}/projects/{project}/limits/active Retrieves the currently active usage limit for a specified project within a given organization. <blockquote>This endpoint can be used whether the Globant Enterprise AI API token is related to the organization scope or to the project scope.</blockquote> ### Response Returns a JSON object representing the active usage limit for the project. If no active usage limit exists, a 404 Not Found error is returned. The response structure is the same as for other successful limit retrieval endpoints. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Project\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, // Will be 1 (Active) \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v2/usageLimits/organizations/${organization}/projects/${project}/limits/active\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" ``` ## DELETE/usageLimits/organizations/{organization}/projects/{project}/limits/{id} Deletes a specified usage limit for a project. <blockquote>This endpoint can be used whether the Globant Enterprise AI API token is related to the organization scope or to the project scope.</blockquote> ### Response Returns the deleted usage limit object. The status field will be updated to 4 (Cancelled). The response structure is consistent with other successful limit retrieval endpoints. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Project\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, // Will be 4 (Cancelled) \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` Error responses will return appropriate HTTP status codes (401 Unauthorized, 403 Forbidden, 404 Not Found) with a JSON body containing an errors array (see example in previous responses). ### cURL Sample ```bash\ncurl -X DELETE \"$BASE_URL/v2/usageLimits/organizations/${organization}/projects/${project}/limits/${id}\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\"\n``` ## PUT/usageLimits/organizations/{organization}/projects/{project}/limits/{id}/hardLimit Modifies the hardLimit property of an active usage limit for a specified project. <blockquote>This endpoint can be used whether the Globant Enterprise AI API token is related to the organization scope or to the project scope.</blockquote> ### Request Body ```json\n{ \"hardLimit\": number\n}\n``` ### Response Returns the updated usage limit object. The hardLimit field will reflect the new value. The response structure is the same as for other successful limit retrieval endpoints. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Project\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` Error responses will return appropriate HTTP status codes (400 Bad Request, 401 Unauthorized, 403 Forbidden, 404 Not Found) with a JSON body containing an errors array (see example in previous responses). ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v2/usageLimits/organizations/${organization}/projects/${project}/limits/${id}/hardLimit\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" -d '{ \"hardLimit\": 3000 }'\n``` ## PUT/usageLimits/organizations/{organization}/projects/{project}/limits/{id}/softLimit Modifies the softLimit property of an active usage limit for a specified project. <blockquote>This endpoint can be used whether the Globant Enterprise AI API token is related to the organization scope or to the project scope.</blockquote> ### Request Body ```json\n{ \"softLimit\": number\n}\n``` ### Response Returns the updated usage limit object. The softLimit field will reflect the new value. The response structure is the same as for other successful limit retrieval endpoints. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Project\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` Error responses will return appropriate HTTP status codes (400 Bad Request, 401 Unauthorized, 403 Forbidden, 404 Not Found) with a JSON body containing an errors array (see example in previous responses). ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v2/usageLimits/organizations/${organization}/projects/${project}/limits/${id}/softLimit\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" -d '{ \"softLimit\": 1500 }'\n\n``` ## PUT/usageLimits/organizations/{organization}/projects/{project}/limits/{id}/renewalStatus Modifies the renewalStatus property of an active usage limit for a specified project. <blockquote>This endpoint can be used whether the Globant Enterprise AI API token is related to the organization scope or to the project scope.</blockquote> ### Request Body ```json\n{ \"renewalStatus\": string // Options: Renewable, NonRenewable\n}\n``` ### Response Returns the updated usage limit object. The renewalStatus field will reflect the new value. The response structure is the same as for other successful limit retrieval endpoints. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Project\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` Error responses will return appropriate HTTP status codes (400 Bad Request, 401 Unauthorized, 403 Forbidden, 404 Not Found) with a JSON body containing an errors array (see example in previous responses). ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v2/usageLimits/organizations/${organization}/projects/${project}/limits/${id}/renewalStatus\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" -d '{ \"renewalStatus\": \"NonRenewable\" }'\n``` {{{12}}}{{{17}}}##MARKDOWN##\n\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, // Will be 4 (Cancelled) \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` ### cURL Sample ```bash\ncurl -X DELETE \"$BASE_URL/v2/usageLimits/organizations/${organization}/limits/${id}\" \\ -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" \\ -H \"Accept: application/json\"\n``` ## PUT/usageLimits/organizations/{organization}/limits/{id}/hardLimit Modifies the hardLimit property of an active usage limit for a specified organization. <blockquote>This endpoint requires a Globant Enterprise AI API token related to <strong>organization</strong> scope.</blockquote> ### Request Body ```json\n{ \"hardLimit\": number }\n``` ### Response Returns the updated usage limit object. The hardLimit field will reflect the new value. The response structure is the same as for other successful limit retrieval endpoints. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v2/usageLimits/organizations/${organization}/limits/${id}/hardLimit\" \\ -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" \\ -H \"Accept: application/json\" \\ -d '{ \"hardLimit\": 30.00 }'\n``` ## PUT/usageLimits/organizations/{organization}/limits/{id}/softLimit Modifies the softLimit property of an active usage limit for a specified organization. <blockquote>This endpoint requires a Globant Enterprise AI API token related to <strong>organization</strong> scope.</blockquote> ### Request Body ```json\n{ \"softLimit\": number\n}\n``` ### Response Returns the updated usage limit object. The softLimit field will reflect the new value. The response structure is the same as for other successful limit retrieval endpoints. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v2/usageLimits/organizations/${organization}/limits/${id}/softLimit\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" -d '{ \"softLimit\": 15.00 }'\n``` ## PUT/usageLimits/organizations/{organization}/limits/{id}/renewalStatus Modifies the renewalStatus property of an active usage limit for a specified organization. <blockquote>The following endpoints require a Globant Enterprise AI API token related to <strong>project</strong> scope.</blockquote> ### Request Body ```json\n{ \"renewalStatus\": string // Options: Renewable, NonRenewable\n}\n``` ### Response Returns the updated usage limit object. The renewalStatus field will reflect the new value. The response structure is the same as for other successful limit retrieval endpoints. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v2/usageLimits/organizations/${organization}/limits/${id}/renewalStatus\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" -d '{ \"renewalStatus\": \"Renewable\" }'\n``` ## POST/usageLimits/organizations/{organization}/projects/{project}/limits Defines a new usage limit for a specified project within a given organization. <blockquote>This endpoint can be used whether the Globant Enterprise AI API token is related to the organization scope or to the project scope.</blockquote> ### Parameters | Name | Type | Description |\n|---|---|---|\n| `subscriptionType` | `string` | Type of subscription. Options: `Freemium`, `Daily`, `Weekly`, `Monthly` |\n| `usageUnit` | `string` | Unit for the usage limit. Options: `Requests`, `Cost` |\n| `softLimit` | `number` | Soft usage limit. Must be less than or equal to the `hardLimit`. |\n| `hardLimit` | `number` | Hard usage limit. Must be greater than or equal to the `softLimit`. |\n| `renewalStatus` | `string` | Whether the limit is renewable. Options: `Renewable`, `NonRenewable` | ### Request Body ```json\n{ \"subscriptionType\": \"string\", // Options: Freemium, Daily, Weekly, Monthly \"usageUnit\": \"string\", // Options: Requests, Cost \"softLimit\": number, // Recommended usage limit \"hardLimit\": number, // Maximum allowed usage \"renewalStatus\": \"string\" // Options: Renewable, NonRenewable\n}\n``` ### Response Returns the newly created usage limit object, including a generated id, remainingUsage, usedAmount, validFrom, and validUntil. The response structure is consistent with other successful limit retrieval endpoints. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Project\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, // Will be 1 (Active) \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` Error responses will return appropriate HTTP status codes (400 Bad Request, 401 Unauthorized, 403 Forbidden, 404 Not Found, 409 Conflict) with a JSON body containing an errors array (see example in previous responses). ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/v2/usageLimits/organizations/${organization}/projects/${project}/limits\" \\ -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" \\ -H \"Accept: application/json\" \\ -d '{ \"subscriptionType\": \"Daily\", \"usageUnit\": \"Requests\", \"softLimit\": 1000, \"hardLimit\": 2000, \"renewalStatus\": \"Renewable\" }'\n``` ## GET/usageLimits/organizations/{organization}/projects/{project}/limits Retrieves all usage limits defined for a specified project within a given organization. <blockquote>This endpoint can be used whether the Globant Enterprise AI API token is related to the organization scope or to the project scope.</blockquote> ### Response Returns a JSON object containing an array of usageLimits for the specified project. If no usage limits exist for the project, an empty array will be returned. ```json",
          "evaluationResultRowContextDocId": "2e975d25-0835-4716-9e66-a0cff109e37d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "d69f46e5-659e-4f33-9710-41583550b482",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?802",
          "evaluationResultRowContextName": "802",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# API Tokens API Tokens play a crucial role in executing Globant Enterprise AI APIs. These tokens are required to access and use the functionality provided by the APIs. There are two types of API Tokens: Organization API Tokens and Project API Tokens. ### [Organization API Tokens](#Organization+API+Tokens) Certain operations require API Tokens with a higher scope, such as access to Project creation, updating, and deletion. Users with the necessary privileges can manage this type of API Tokens in order to work only with OrganizationAPI endpoints. These API Tokens are not intended to work at the project level and cannot be used to reference assistants or AI models. ### [Project API Tokens](#Project+API+Tokens) For each project, you can define multiple Project API Tokens. This allows for granular control and tracking of usage. It is important to note that API Tokens have project-wide reach. Moreover, the ability to assign API Tokens to specific projects allows for fine-grained access control, ensuring that only authorized individuals or systems can execute requests on specific projects. With this level of granularity, organizations can effectively manage access permissions, track usage patterns, and maintain control over their projects defined within the Globant Enterprise AI API.",
          "evaluationResultRowContextDocId": "146c275d-bb81-4af3-925b-a96fe8e2f48d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "de4e7d68-2426-4b86-b405-92c9928ff9e1",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?564",
          "evaluationResultRowContextName": "564",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.3,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "54f40b56-619c-4e46-a194-11518ed5d92d",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0034730200",
      "evaluationResultRowEndDate": "2025-05-13T11:13:56",
      "evaluationResultRowOutput": "The Organization API provides operations that require API tokens with a higher scope, specifically for managing organization-level functionalities such as:\n\n- Creating projects\n- Updating projects\n- Deleting projects\n\nThese API tokens are managed by users with the necessary privileges and are intended to work only with Organization API endpoints. They are **not </B>meant for project-level operations and cannot be used to reference assistants or AI models. This allows management at the organization level rather than at the granular project level.\n\nThis distinction between Organization API Tokens and Project API Tokens ensures proper access control according to the scope needed.\n\nYou can find this information summarized in the documentation under the **API Tokens </B>section.  \nReference: [1](https://wiki.genexus.com/enterprise-ai/wiki?564)",
      "evaluationResultRowStartDate": "2025-05-13T11:13:51",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "8347"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#How to create a Flow with internet search This is a step-by-step guide to create a conversational assistant with internet search using [[321]]. This Flow manages two assistants: a Chat Assistant that answers questions using internet search results, and another Chat Assistant that rephrases follow-up questions (based on the conversation history) as standalone questions. In addition, it includes a Search API component. ## Step 1: Configuring the Work Environment To get started, first, make sure you have access to the [[42]] and are familiar with the [[371|interface for creating Flows]]. ## Step 2: Creating Chat Assistants In this step, two [[566##Chat Assistant creation|Chat Assistants]] are created. They are called 'Response from Search' and 'Follow-up Rephrase'. ### 'Response from Search' Chat Assistant This Chat Assistant generates answers for the end user based solely on the results of an internet search, without using prior knowledge or inventing information. It takes the provided search results and uses them to answer the end user's question in the best possible way. If necessary, the answer can include rich text in HTML format, but Markdown should not be used to format the answer. **LLM Settings:** - AI Provider: Google VertexAI\n- Model: gemini-1.5-pro\n- Temperature: 0.10\n- Max Output Tokens: 8192 **Prompt:** ```\nINSTRUCTIONS\n---\nYour task is to generate a response for the user based on the results of an internet search.\nDO NOT USE PRIOR KNOWLEDGE AND DO NOT MAKE UP INFORMATION USE THE SEARCH RESULTS.\nGiven the following results from an internet search and NOT PRIOR KNOWLEDGE answer the user question the best you can. SEARCH RESULTS\n---\n{searchResults} RESPONSE FORMAT\n---\nFor the response you can generate RICH TEXT USING HTML if it is needed.\nDO NOT USE MARKDOWN TO FORMAT YOUR RESPONSE.\nFOR RICH TEXT DO NOT USE MARKDOWN USE HTML INSTEAD.\n``` ### 'Follow-up Rephrase' Chat Assistant This Chat Assistant rephrases a follow-up question based on the conversation history, ensuring it becomes a standalone question if needed. If the follow-up question already works as a standalone question, it simply repeats the question without any changes. **LLM Settings:** - AI Provider: Google VertexAI\n- Model: gemini-1.5-pro\n- Temperature: 0.10\n- Max Output Tokens: 8192 **Prompt:** ```\nGiven the conversation history and not prior knowledge, and a follow-up question, rephrase the follow-up question to be a standalone question IF IT IS NEEDED. If the question works as a standalone question, just repeat the question. RESPONSE ONLY WITH THE STANDALONE QUESTION AND NOTHING MORE; DO NOT INCLUDE ANY OTHER TEXT.\n``` ## Step 3: Creating the Flow Follow the steps described in [[322]] to create the Flow called 'Search Assistant' with the following structure: [[689]] ## Step 4: Implementing the API component To add the API component, go to the [[494|Interactions section]] and, below INTEGRATIONS in the Flow menu, drag the component between the Assistants nodes. [[690]] ## Step 5: Setting Up the Search API Go to [https://developers.google.com/custom-search/v1/introduction?hl=es-419|Google Search API documentation] and click on Get API key button. [[691]] In the pop-up window that opens, select or create a project from the Select or create project menu. [[692]] Select the Yes option to agree that the use of the services and related APIs is subject to compliance with the applicable Terms of Service. Then click on NEXT to continue with the setup process. In the next window, click on the SHOW KEY button to reveal the API Key. Copy this API key, as it will be required to configure the API node in your flow. [[693]] ## Step 6: Creating a Programmable Search Engine Go to [https://programmablesearchengine.google.com/controlpanel/all|Programmable Search Engine] and click on the Add button. [[694]] In the pop-up window that opens, fill in the necessary fields to create the search engine: [[695]] - **Name of your search engine:** In the \"Search engine name\" field, enter a name for the search engine, such as Search Assistant.\n- **What to search for:** Select the \"Search the entire web\" option to have the engine search the entire web. If you prefer to search only specific sites, you can select the \"Search specific sites or pages\" option and add the necessary URLs.\n- **Search settings:** If you wish, you can adjust additional settings such as image search or SafeSearch filter.\n- **Captcha:** Select the \"I am not a robot\" checkbox.\n- **Create:** At the end of the window, click on the Create button to finalize the creation of the search engine. In the window that appears, you must click on the Customize button. [[696]] This will take you to the Overview section of the search engine you have just created. In this section, you should find and copy the Search engine ID, which is a unique value associated with your search engine. This ID is required to configure the API node in your flow. [[697]] ## Step 7: Configuring the API Node With the Search engine ID and API key, you can now configure the API node. First, you must click on the Edit button that appears when you position the mouse over the API node. [[698]] Once you have clicked on Edit, the State Configuration menu will be displayed on the left side. [[699]] Within this menu, in the URL section, you must configure the URL with the following value: ```\nhttps://www.googleapis.com/customsearch/v1?key=<API key>&cx=<Search engine ID>&q=<query>\n``` Be sure to replace `<API key>` and `<Search engine ID>` with the previously obtained values. In the API node, go to the **Parameters** section and click on **+ Add new** to add a new parameter. Then, configure the parameter with the following details: - **Parameter name:** q.\n- **Value:** lastResponse (the value obtained from the previous response). Next, in the State Configuration section, you must make sure to mark q as **Query params**. [[700]] In the **Responses** section, make sure to map the items property to the **searchResults** variable. If searchResults is not available, you can click on Add new variable to create it. [[701]] ## Step 8: Configuring the Assistant Node After you have configured the API node, go to the Assistant node, which is located just below the API node in your flow. In this node, find the Variable section and change the input variable to searchResults. [[702]] ## Step 9: Testing the Flow Finally, save the changes and test the flow. You can use the test window or the Playground to make sure everything works correctly, verifying that the Assistant responds properly using the search results. | [[704]] | [[705]] | {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "526b3969-e9ac-4188-a986-6b0013b11c4d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "4d4a21c6-6d24-4ce7-bfc6-553d9f539c59",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?688",
          "evaluationResultRowContextName": "688",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# How to expose a Flow through WhatsApp Client via B2Chat Once you have created your first Flow in Globant Enterprise AI, following the steps in [How to create a Flow](322.html), you may be interested in exposing it in various communication and management platforms. Here is a step-by-step guide on how to expose a Flow through WhatsApp Client via B2Chat. ## Step 1: Create an application key 1. Access the Flow you wish to expose. In the Side Navigation Menu, select [Configuration](371.html) and then Application Keys.\n2. Click on the Generate Key button.\n3. In the Type field, select API and in Key Name, enter a name such as \"B2ChatIntegration\". Next, confirm the operation by clicking on Save.\n4. Copy the generated key using the copy icon next to the key. For more details, see **Step 2: Create an application key** in the complete guide [How to connect a Flow to B2Chat](674.html). ## Step 2: Get the Bot id Go to the Side Navigation Menu, and click on the [Overview](371.html) option in the Overview section. There you will see a summary of the Flow including the Bot id, along with other details such as the Flow's name and purpose. The Bot id is the alphanumeric identifier that appears below the name of your Flow. ![Bot id](./images/395.png) Copy the Bot id. ## Step 3: Build B2Chat webhook B2Chat uses this URL to answer messages and reach the bot. The URL format is: `https://botbuilder.fluentlab.ai/fluentlab/whatsappb2c/<bot-id>?apikey=<api-key>` - `<bot-id>`: This is the identifier you obtained in Step 2: Get the Bot id\n- `<api-key>`: This is the application key you generated in Step 1: Create an application key. ## Step 4: Request B2Chat Credentials To connect your Flow to WhatsApp via B2Chat, you must [contact Globant Enterprise AI support](274.html) and provide the **B2Chat webhook** built in Step 3: Build B2Chat webhook. With this information, they will provide you with the necessary credentials for the integration, which include: - **B2Chat Client ID:** Unique client identifier to authenticate the connection.\n- **B2Chat Signing Secret:** Signing Secret to verify the integrity of incoming and outgoing messages.\n- **B2Chat Endpoint:** URL of the B2Chat endpoint for API requests.\n- **B2Chat Security Endpoint:** URL of the B2Chat security endpoint, which is used for additional authentication and security verification. ## Step 5: Configure the Integration in Globant Enterprise AI Go to the Flow you want to expose. Then go to **Side Navigation Menu** > [**Configuration**](371.html) > **Integrations** > **WhatsApp Client via B2Chat** and fill in the fields with the provided data: - B2Chat Client ID\n- B2Chat Signing Secret\n- B2Chat Endpoint\n- B2Chat Security Endpoint To enable push notifications, select the **Push notification allowed** checkbox. Lastly, click on **Confirm**. > **Note**: Keep in mind that exposing a flow via WhatsApp may involve additional costs. See details in [Conversation-Based Pricing](https://developers.facebook.com/docs/whatsapp/pricing?locale=en).",
          "evaluationResultRowContextDocId": "4cef5feb-9248-444a-875b-b3d57869ea6b",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "b10fa045-4b32-4fa6-8122-9b0e07a21619",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?548",
          "evaluationResultRowContextName": "548",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# How to connect a Flow to B2Chat Once you have created your first Flow in Globant Enterprise AI following the steps in [How to create a Flow](322.html), you might want to integrate it with different communication platforms, including [B2Chat](https://www.b2chat.io/en/features/console/), to manage agent overflow scenarios (Human-in-the-Loop). Here is a step-by-step guide on how to connect a Flow through B2Chat. ## Step 1: Get a username and password from B2Chat Go to the URL [https://app.b2chat.io](https://app.b2chat.io) and log in. In the menu on the left, click on General Setting and then select Integrations. ![Image](./images/675.png) After accessing the Integrations section, select BotApi. There you will see the values corresponding to the username and password you will need. ![Image](./images/676.png) ## Step 2: Create an application key 1. Access the Flow you wish to expose. In the Side Navigation Menu, select [Configuration](371.html) and then Application Keys.\n2. Click on the Generate Key button. ![Image](./images/474.png)\n3. In the Type field, select API and in Key Name, enter a name such as \"B2ChatIntegration\". Next, confirm the operation by clicking on Save. ![Image](./images/825.png)\n4. Copy the generated key using the copy icon next to the key. ![Image](./images/826.png) ## Step 3: Get the Bot id Go to the Side Navigation Menu, and click on the [Overview](371.html) option in the Overview section. There you will see a summary of the Flow including the Bot id, along with other details such as the Flow's name and purpose. The Bot id is the alphanumeric identifier that appears below the name of your Flow. ![Image](./images/395.png) Copy the Bot id. ## Step 4: Configure the flow Access the Side Navigation Menu > [Configuration](371.html) > Integrations > B2Chat and fill in the required data: - **Endpoint:** It is the B2Chat URL https://api.b2chat.io\n- **B2Chat webhook:** B2Chat uses this URL to answer messages and reach the bot. The URL format is: `https://botbuilder.fluentlab.ai/fluentlab/b2chat/<bot-id>?x-api-key=<api>` - `<bot-id>`: This is the identifier you obtained in Step 3: Get the Bot id - `<api>`: This is the application key you generated in Step 2: Create an application key.\n- B2Chat username\n- B2Chat password ## Step 5: Configure interactions Once you have configured B2Chat, click on Flow. When doing so, you will notice that now there is a new space available called **B2Chat** in the Interactions Section: ![Image](./images/677.png) To create a basic agent escalation flow, you must drag the B2Chat Resolver component into the flow. This component is key because escalation to a human agent will take place at the point where it is inserted. ![Image](./images/678.png) ## Step 6: Testing To test the connection to B2Chat, use the **Playground** or one of the methods available in Flow (such as Run Flow, Open Testing Assistant, or Go to Demo Page). ![Image](./images/679.png) Once the flow reaches the B2Chat Resolver, escalation to the human agent will take place. Then, in the B2Chat console, the conversation will appear in the \"Chats\" tab, and the agent will be able to respond from there. ### B2Chat console ![Image](./images/680.png) ### Run Flow ![Image](./images/681.png) For the conversation to end in the B2Chat console, the agent must click on \"Close\". By doing so, the flow will continue to run and activate the next nodes according to the selected configuration.",
          "evaluationResultRowContextDocId": "05d7b228-0dc7-4063-94b7-fa30f602d80f",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "cbf9e6cb-34fb-41d8-8ef7-ca6721bbf604",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?674",
          "evaluationResultRowContextName": "674",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "{ \"model\": \"saia:flow:87507723-3e3b-47f6-a1d0-aa53370c71d2\", /* Where 87507723-3e3b-47f6-a1d0-aa53370c71d2 is the bot_id*/ \"messages\": { \"role\": \"user\", \"content\": \"Question for flow\" }\n}\n``` ### cURL Sample ```bash\ncurl --location \"$BASE_URL/chat\" \\\n-H \"Content-Type: application/json;charset=utf-8\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"saia-conversation-id: $CONVERSATION_ID\" \\\n--data '{ \"model\": \"saia:flow:87507723-3e3b-47f6-a1d0-aa53370c71d2\", \"messages\": { \"role\": \"user\", \"content\": \"Question for flow\" }\n}'\n``` ## How to integrate Globant Enterprise AI with third-party SDKs ### cURL ```bash\ncurl -X POST \"$BASE_URL/chat\" \\\n-H 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n-H 'X-Saia-Cache-Enabled: false' \\\n-H 'Content-Type: application/json' \\\n-data '{ \"model\": \"openai/gpt-4o\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a professional Translator. Translate the user text to English. Just output one word. \" }, { \"role\": \"user\", \"content\": \"Hola\" } ], \"stream\": false\n}'\n``` Note that in the `model` parameter, you must specify the model in the format `provider/nameModel`. ### OpenAI SDK for Python ```python\nfrom openai import OpenAI api_key = \"$(SAIA_API_KEY)\"\napi_base = \"https://api.saia.ai/chat\" openai = OpenAI(api_key=api_key, base_url=api_base) completion = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}]) print(completion.choices[0].message.content)\n``` ### OpenAI SDK for TypeScript ```typescript\nconst { Configuration, OpenAIApi } = require(\"openai\"); const configuration = new Configuration({ apiKey: SAIA_APITOKEN, basePath: \"https://api.saia.ai/chat\", });\nconst openai = new OpenAIApi(configuration); async function main() { const chatCompletion = await openai.createChatCompletion({ model: \"gpt-4o-mini\", messages: [{role: \"user\", content: \"Hello world\"}], }); console.log(chatCompletion.data.choices[0].message);\n} main();\n``` ## How to call Gemini 1.5 pro ### cURL ```bash\ncurl -X POST \"$BASE_URL/chat\" \\\n-H 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n-H 'X-Saia-Cache-Enabled: false' \\\n-H 'Content-Type: application/json' \\\n-data '{ \"model\": \"vertex_ai/gemini-1.5-pro-preview-0409\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a professional Translator. Translate the user text to English. Just output one word. \" }, { \"role\": \"user\", \"content\": \"Hola\" } ], \"stream\": false\n}'\n``` ## Creating and Using an Assistant with Variables First, you need to create an assistant with the [variables](85.html), and then pass the variables when you use it. ### cURL - Creating the Assistant ```bash\ncurl --location 'https://api.saia.ai/v1/assistant' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer $SAIA_APITOKEN' \\\n--data '{ \"type\": \"chat\", \"name\": \"Test-variables\", \"prompt\": \"You are a translator. Translate to {language}.\"\n}'\n``` ### cURL - Using the Assistant ```bash\ncurl --location 'https://api.saia.ai/chat' \\\n--header 'Saia-Auth: $SAIA_APITOKEN' \\\n--header 'X-Saia-Cache-Enabled: false' \\\n--header 'Content-Type: application/json' \\\n--data '{ \"model\": \"saia:assistant:Test-variables\", \"messages\": [ { \"role\": \"user\", \"content\": \"Hello\" } ], \"variables\": [ { \"key\": \"language\", \"value\": \"French\" } ], \"stream\": false\n}'\n```\n\n# Chat API The Chat API simplifies how businesses connect to various AI models across different providers. It acts as a single point of access, eliminating the need to manage multiple SDKs or APIs. This means you can interact with any supported LLM using just one SDK. In addition, the API centralizes the use of any `Assistant` in a single entry point. Check the [generic variables](20.html) needed to use the API. ## Endpoint The general endpoint is as follows: | Method | Path |\n|---|---|\n| POST | /chat or /chat/completions | The `/chat/completions` and `/chat` endpoints are supported and you can choose either endpoint. Both have the same interface (Request/Response) as the [OpenAI Chat Completion API](https://platform.openai.com/docs/api-reference/chat/create). These endpoints can be called using the OpenAI SDKs. ## POST/chat ### Request - Method: POST\n- Path: $BASE_URL/chat\n- Headers: - Content-Type: application/json - Authorization: Bearer $SAIA_PROJECT_APITOKEN - saia-conversation-id: $CONVERSATION_ID This header only applies to [Flows](321.html). It is necessary to maintain the context of the conversation. If omitted, each message will be treated as a new conversation without context. The value can be any alphanumeric string and changing it will start a new conversation. Conversations are valid for 10 minutes of inactivity, after which they are automatically deleted. Currently, if the response takes more than 60 seconds to be generated, it will not be received. Therefore, this endpoint is suitable only for simple Flows that do not require many interaction steps.\n- Request Body The payload will vary depending on the selected `Assistant`. The general pattern is as follows: ```json\n{ \"model\": \"string\", // mandatory \"messages\": [ ... ], // at least one message \"stream\": boolean // optional\n}\n``` The `model` needs to address the assistant `type` and `name` or `bot_id`, depending on the Type. Then, the parameters will vary depending on the `type`. Its format is as follows: ```json\n\"model\": \"saia:<assistant_type>:<assistant_name>|<bot_id>\"\n``` The `bot_id`, found in the [OVERVIEW](371.html), located in the Flow Builder **Side Navigation Menu**, is used if the `type` is `flow`. | Type | Description |\n|---|---|\n| `assistant` | Identifies an [Assistant API](35.html), [Data Analyst Assistant](886.html), [Chat with Data Assistant](159.html) and [API Assistant](110.html) |\n| `search` | Identifies a [RAG Assistant](64.html) |\n| `flow` | Identifies a [Flow](321.html) | The `messages` element defines the desired messages to be added. The minimal value needs to be the following, where the `content` details the user input. ```json\n{ \"role\": \"string\", /* user, system and may support others depending on the selected model */ \"content\": \"string\"\n}\n``` You can add additional parameters. Below are possible body samples. ## Assistant Sample ```json\n{ \"model\": \"saia:assistant:translate-to-spanish\", /* Using a Standard Assistant named 'translate-to-spanish' */ \"messages\": [ { \"role\": \"user\", \"content\": \"Hi, welcome to Globant Enterprise AI!!\" } ], \"stream\": true\n}\n``` The expected result is to `stream` the translated content depending on the Prompt defined. ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"saia:assistant:translate-to-spanish\", \"messages\": [ { \"role\": \"user\", \"content\": \"Hi, welcome to Globant Enterprise AI!!\" } ], \"stream\": true\n}'\n``` ## Data Analyst Sample ```json\n{ \"model\": \"saia:assistant:myDataAnalystAssistant\", /* Using a DataAnalyst Assistant named 'myDataAnalystAssistant'. It can also be referenced by its \"assistantId\" value (uuid) instead of the name. */ \"messages\": [ { \"role\": \"user\", \"content\": \"What was the month with the highest Ancillaries sales?\" } ], \"stream\": true\n}\n``` ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"saia:assistant:myDataAnalystAssistant\", \"messages\": [ { \"role\": \"user\", \"content\": \"What was the month with the highest Ancillaries sales?\" } ], \"stream\": true\n}'\n``` ## Chat with Data Assistant Sample ```json\n{ \"model\": \"saia:assistant:ChatWithData\", \"messages\": [ { \"role\": \"user\", \"content\": \"How many covid deaths were there in Singapore 2022?\" } ]\n}\n``` ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"saia:assistant:ChatWithData\", \"messages\": [ { \"role\": \"user\", \"content\": \"How many covid deaths were there in Singapore 2022?\" } ], \"stream\": false\n}'\n``` If the request is processed successfully, the response is returned in JSON format, which includes two key components: the query, represented by `key = \u201cquery\u201d`, and the result of the execution, accessible through `key = \u201cdataSet\u201d`. ```json\n{ \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"{\\\"query\\\":{\\\"id\\\":\\\"db7c1e09-c059-4b96-9847-f174b1af6974\\\",\\\"name\\\":\\\"CovidDeathsInSingapore2022\\\",\\\"description\\\":\\\"Covid Deaths In Singapore2022\\\",\\\"expression\\\":\\\"Query CovidDeathsInSingapore2022 [OutputType='Table', Paging='False']\\\\r\\\\n{\\\\r\\\\n\\\\tSum(NewDeaths) [Name='Element01']\\\\r\\\\n\\\\t#filters\\\\r\\\\n\\\\t\\\\tCountryISO = 'SG'\\\\r\\\\n\\\\t\\\\tYear(Date) = 2022\\\\r\\\\n\\\\t#end\\\\r\\\\n}\\\",\\\"removeDuplicates\\\":false,\\\"outputType\\\":\\\"Table\\\",\\\"paging\\\":false},\\\"dataSet\\\":{\\\"id\\\":\\\"3dc46b4b-700a-42d2-8cd0-34669979c590\\\",\\\"pageNumber\\\":1,\\\"totalPages\\\":1,\\\"totalRows\\\":1,\\\"metadata\\\":[{\\\"name\\\":\\\"Element01\\\",\\\"type\\\":\\\"datum\\\",\\\"caption\\\":\\\"Sum of New Deaths\\\",\\\"dataType\\\":\\\"number\\\"}],\\\"data\\\":[{\\\"Element01\\\":\\\"386\\\"}]}}\" }\n}\n``` ## API Assistant Sample ```json\n{ \"model\": \"saia:assistant:test-openapi-weather-assistant\", \"stream\": false, \"messages\": [ { \"role\": \"user\", \"content\": \"weather in madrid\" } ]\n}\n``` ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"saia:assistant:test-openapi-weather-assistant\", \"messages\": [ { \"role\": \"user\", \"content\": \"weather in madrid\" } ], \"stream\": false\n}'\n``` ## RAG Sample ```json\n{ \"threadId\": \"uuid_as_string\", /* conversation identifier (optional) */ \"model\": \"saia:search:Default\", /* Using the Default RAG Assistant */ \"messages\": [ { \"role\": \"user\", \"content\": \"Summarize the features of Globant Enterprise AI\" } ], \"stream\": true\n}\n``` The expected result is to query the Default RAG Assistant and stream a reply once the sources are obtained. ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"saia:search:Default\", \"messages\": [ { \"role\": \"user\", \"content\": \"Summarize the features of Globant Enterprise AI\" } ], \"stream\": true\n}'\n``` More information: [RAG samples with Chat API](712.html) ## Flow Sample ```json",
          "evaluationResultRowContextDocId": "07864e36-b44e-42b1-97d9-8d18a85be63b",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "ce3585c7-5cfd-414c-9050-ff56bcadf6dd",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?34",
          "evaluationResultRowContextName": "34",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Enterprise AI For GeneXus Documentation, please visit [wiki.genexus.com](https://wiki.genexus.com/) <div style=\"background:#f8f8f8; padding:100px 128px\"> <span style=\"display: block; color: #004742; font-size: 46px; font-weight: 700; margin-bottom: 16px; line-height: 54px;\">Globant Enterprise AI Documentation</span> Delve into all the documentation of Globant Enterprise AI and stay ahead with insights into upcoming features. [Start reading](https://wiki.genexus.com/enterprise-ai/wiki?15,GeneXus+Enterprise+AI+Overview) [Learn with training videos](https://training.genexus.com/es/aprendiendo/cursos/inteligencia-artificial/genexus-enterprise-ai/introduccion-al-curso-6105673) --- - [Overview](15.html)\n- [API Reference](20.html) - [Chat API](34.html) - [Organization API](22.html) - [RAG Assistant API](29.html) - [Chat with Documents API](33.html) - [Embeddings API](422.html) - [Assistant API](35.html) - [File API](450.html) - [Feedback API](407.html) - [Error Codes](23.html)\n- [Using variables in Prompts](85.html)\n- [Tokens](293.html)\n- [Backoffice](42.html)\n- [Frontend](82.html)\n- [Playground](349.html)\n- [Start Page](408.html)\n- [Data Analyst Assistant](89.html) - [How to use Data Analyst Assistant](105.html) - [How to set the language in Data Analyst Assistant](464.html) - [Conversational Module](291.html)\n- [API Assistant](110.html) - [How to use API Assistant](123.html)\n- [Chat with Data Assistant](159.html) - [How to use Chat with Data Assistant](170.html) - [How to generate metadata for the Chat with Data Assistant](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?178,,) - [Chat with Data Assistant Use Guidelines](177.html) - [How to change the output format and filter results in Chat with Data Assistant](181.html)\n- [RAG Assistant](44.html) - [How to create and configure a RAG Assistant](60.html) - [File Formats](137.html) - [Use Guidelines](61.html) - [Update Configuration](64.html) - [Configuration - General Information](68.html) - [Configuration - Prompt](69.html) - [Configuration - Retrieval](70.html) - [Index Configuration](145.html) - [Supported Vector Store Provider](267.html) - [Advanced configuration](209.html) - [Context Prompt Template](205.html) - [VectorStore Search Options](206.html) - [Self Query Use Case](150.html) - [Parent Document Retrieval](204.html) - [Azure AISearch Search Options](207.html) - [Profile Metadata configuration](298.html)\n- [Flows](321.html) - [How to create a Flow](322.html) - [How to create a Flow with Conditional component](363.html) - [How to expose a Flow through Teams](391.html) - [How to expose a Flow through Slack](477.html) - [How to expose a Flow in a web page](473.html) - [User's Guide for Globant Enterprise AI Flow Builder](371.html) - [Management of Variables in Flows](470.html) - [Flows Conversation History](378.html) - [User Metrics in Flows](380.html) - [Message Metrics in Flows](382.html) - [Flows Configuration](384.html) - [Flows Section](389.html) - [Interactions to Create a Flow](494.html)\n- [User Access Rights Management](242.html) - [Roles](245.html) - [Members](253.html)\n- [File support for multimodal models](345.html) - [How to upload and manage Files via API](454.html) - [Supported models and file formats](406.html)\n- [File Support in Assistants](346.html)\n- [Ingestion SDK](172.html)\n- [AI-Driven Load Balancing](32.html)\n- [Managing quotas per project](71.html)\n- [Supported LLMs](200.html)\n- [Updates Log](31.html)\n- [Coming Next](30.html)\n- Troubleshooting - [File Upload](199.html) - [LLM](290.html) - [Embeddings](140.html) - [Vectorstores](308.html) - [Flows with RAG](445.html)",
          "evaluationResultRowContextDocId": "8b185fd7-17c6-4f63-88f2-154a2b6b3824",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "dbe17c48-80ca-4343-9afc-dcec8e524ced",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?8",
          "evaluationResultRowContextName": "8",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# API Assistant API Assistant is a feature of Globant Enterprise AI that allows you to define assistants to chat and interact with any external REST API compliant with the OpenAPI 3.0 specification or higher. This assistant leverages AI models and function calling capabilities to provide a dynamic way to chat with APIs using natural language. ## [Main features](#Main+features) Defining an API Assistant provides a range of advanced features designed to empower interaction with external APIs in an efficient and secure manner. Key features include: - **Compatibility with OpenAPI 3.0:** It is possible to interact with any API that is built following the OpenAPI 3.0 specification or later.\n- **Customizable instructions:** The user can provide specific text instructions to guide the AI assistant in making decisions and accurately handling requests.\n- **Model settings:** It allows you to adjust the AI model settings, such as temperature and the maximum number of tokens generated, to enhance and customize the assistant's answers according to the user's needs.\n- **Security:** It ensures the security of interactions through the use of API tokens. These tokens, which can be of Basic or Bearer type, are used in the Authorization header to authenticate and authorize requests, thus protecting information and data.\n- **Automated function calls:** It returns answers efficiently by making only the necessary API calls. This is achieved by Globant Enterprise AI's intelligent model and automated function calls handling end user requests.\n- **Multiple API calls:** The assistant\u2019s intelligence is shown by making sequential calls to different APIs to obtain complex results. For example, upon receiving a question such as 'What is the weather like in Tokyo and Montevideo?' the assistant identifies the need to query the weather API for each city mentioned. This ability to automatically handle multiple requests ensures a seamless experience for the user, who doesn't need to intervene in the process. ## [Prerequisites](#Prerequisites) Before starting to use API Assistant, it is important to ensure that certain prerequisites are met: - You need to access Globant Enterprise AI. To do so, you can visit the [official website](https://www.genexus.com/en/products/genexus-enterprise-ai), where you will find contact information and details on how to obtain access to the platform.\n- If your API is authenticated with an API Token (either Basic or Bearer), you must configure your requests to include this token in the authorization header, specifying the correct type. For example: `-H \"Authorization: Bearer $OPENAI_API_KEY\"`. In this way, when executing the call, the token will be used for the interaction.\n- Verify that the API Specification is documented according to OpenAPI 3.0 or higher, in JSON format. This is important, as APIs that do not comply with this specification may not be fully supported.\n- Avoid setting Message streaming because it is not supported. To define or update an API Assistant, you can use the [Globant Enterprise AI Backoffice](42.html) by following the steps described in [How to use API Assistant](123.html) or using the API.",
          "evaluationResultRowContextDocId": "b3f2c7ef-2bcc-4b6c-b879-cdeee55f7e13",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "fde7a172-96de-4a42-9ea6-2eafcc41bbcd",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?110",
          "evaluationResultRowContextName": "110",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataSetRowId": "55a63051-5cfb-4b88-852c-bbe40bacc3b6",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0041865800",
      "evaluationResultRowEndDate": "2025-05-13T11:14:09",
      "evaluationResultRowOutput": "To use the Chat API in Globant Enterprise AI, here is a summarized guide based on the official documentation:\n\n### Endpoint\n- **POST </B>request to either `/chat` or `/chat/completions` endpoint.\n- Both endpoints have the same interface and are compatible with OpenAI Chat Completion API standards.\n\n### Request Structure\n- **Headers**:\n  - `Content-Type: application/json`\n  - `Authorization: Bearer $SAIA_PROJECT_APITOKEN` (your project API token)\n  - `saia-conversation-id: $CONVERSATION_ID` (optional, used to maintain conversation context in Flows)\n  \n- **Body </B>(JSON):\n  - `\"model\"`: Specifies the assistant or flow to use in the format:\n    ```\n    \"saia:<assistant_type>:<assistant_name_or_bot_id>\"\n    ```\n    - Types include `assistant`, `search`, `flow`, etc.\n  - `\"messages\"`: List of messages forming the conversation. Each message must include `role` (e.g., \"user\", \"system\") and `content`.\n  - `\"stream\"` (optional): Boolean to specify if the response should be streamed.\n\n### Example Using a Flow\n```json\n{\n  \"model\": \"saia:flow:87507723-3e3b-47f6-a1d0-aa53370c71d2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Question for flow\"\n    }\n  ]\n}\n```\n\n### Example cURL Request\n```bash\ncurl --location \"$BASE_URL/chat\" \\\n-H \"Content-Type: application/json;charset=utf-8\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"saia-conversation-id: $CONVERSATION_ID\" \\\n--data '{ \n  \"model\": \"saia:flow:87507723-3e3b-47f6-a1d0-aa53370c71d2\",\n  \"messages\": { \"role\": \"user\", \"content\": \"Question for flow\" }\n}'\n```\n\n### Notes\n- `saia-conversation-id` is important to maintain the context in multi-turn conversations, particularly with Flows.\n- Conversation validity lasts 10 minutes of inactivity.\n- Long responses (>60 seconds) may not be received at the chat endpoint.\n- The `\"model\"` value varies depending on the type of assistant or flow you want to interact with.\n\nFor more detailed examples such as using different assistant types like Data Analyst Assistant, RAG Assistant, or API Assistant, you can refer to the documentation. The API is designed to unify interaction with various AI models through a single interface.\n\nYou can find the full detail of the Chat API usage here:  \n[Chat API Documentation](https://wiki.genexus.com/enterprise-ai/wiki?34) [2](https://wiki.genexus.com/enterprise-ai/wiki?34)",
      "evaluationResultRowStartDate": "2025-05-13T11:13:56",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "8785"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# File support for multimodal models Multimodal models allow interactions that go beyond traditional text-based prompts. These models can understand and process information from a variety of sources, including text, file documents, images, audio, and video. This means they can analyze, process, and generate content based on a richer and more complete set of inputs. Globant Enterprise AI implements multimodal models, allowing you to interact with models such as Gemini and OpenAI in a completely new way. By combining text with files, a world of possibilities opens up for analyzing, processing, and generating content. These models can directly receive files alongside your prompts, eliminating the need to manually extract and inject file content. This direct file integration offers several advantages. - Simplifies file handling. You no longer need to extract and inject file content into the prompt.\n- Files are sent directly to the model, streamlining the process and enabling direct model interaction.\n- Empowers you to interact directly with files within your prompts, making it easier to analyze, process, and generate content based on their contents. This eliminates the need for manual extraction and injection, simplifying your workflow.\n- Leverages the information contained in files to provide more comprehensive and insightful responses, enhancing their capabilities. ## File References in Prompts You can reference files in your prompts using a specific format: `{file:filename}`. For example, to reference a file named \"report.pdf\" you would write: `{file:report}`. > **Note**: This reference format only works for files that have already been uploaded to your project. If you're uploading a file for the first time, you'll need to use the standard file upload mechanism within the platform. ## How Files are Managed in Your Project 1. **File Storage**: All files uploaded to your project are stored securely and can be accessed by your prompts.\n2. **File Access**: When you reference a file in your prompt, the system automatically retrieves it and sends it to the multimodal model.\n3. **File Types**: The system supports a wide range of file types, including text documents, images, audio files, and more. ## Sample Suppose you have a document called \"product_specs.txt\" loaded in your project. You can use it in your prompt as follows: ```plaintext\n\"Please summarize the key features of the product described in {file:product_specs}\".\n``` The multimodal model will then access the \"product_specs.txt\" file and use its contents to generate a summary of the product features. ## See Also [Supported models and file formats](406.html)",
          "evaluationResultRowContextDocId": "381b4a5d-cb99-40b8-a58e-e67f82082418",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "24867c98-1c22-4ce3-a586-ded0a9c488a4",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?345",
          "evaluationResultRowContextName": "345",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# File Support in Assistants File Support in Assistants allows you to work with various file types, including documents, images, videos, and audio, to perform tasks like summarizing documents, extracting information, comparing files, and generating text based on file content. This opens up a world of possibilities for using assistants in new and innovative ways. ## Scenarios The end user should be able to interact with assistants using files in the following ways: - Summarize a document: Provide a concise summary of the document's content.\n- Extract specific information: Extract specific data points from a document, such as the amount, client name, and deadline from a contract.\n- Compare documents: Compare the information in two documents, identifying matching and differing clauses.\n- Generate text based on document content: Create new text based on the content of a document.\n- Ask questions based on uploaded documents: Similar to [RAG Assistants](44.html), but with the ability to upload new documents from the [Playground Interface](349.html) to add to the assistant's context. ## Functionality File Support in Assistants provides a robust set of features for managing and using files: - File Management: You can manage files associated with each project using CRUD operations (create, read, update, delete). A project-level API is available for file management, using the project identifier as the key. You can also configure a storage limit per project to manage storage space effectively.\n- File Referencing: You can reference files within assistant prompts using variables like `{file:filename}` or `{file:*}`. For example, you can use the prompt: \"Compare `{file:id1}` with `{file:id2}`\". The system seamlessly handles file uploads, context window size, and summarization, regardless of whether the model is multimodal or not.\n- File Types: File support extends beyond documents to include images and videos, using a similar syntax: `{file:image1}` or `{file:video1}`. For example, you can use the prompt: \"Extract the text from image `{file:id1}` and summarize its content\".\n- Frontend Upload: End users can easily select existing files or upload new ones when interacting with file-handling assistants. ## How to Use File Support in Assistants 1. Choose your assistant: Select the assistant that best suits your needs.\n2. Upload your file: Click on the \u201cUpload File\u201d button and select the file you want to use. The assistant will recognize the file type and process it accordingly.\n3. Write your prompt: Enter your instructions for the assistant in the prompt box. Be clear and specific about what you want the assistant to do.\n4. Use file references: To reference the uploaded file in your prompt, use the following syntax: - Documents: `{file:filename}` (for example, {file:contract}) - Images: `{file:image1}` - Videos: `{file:video1}` - Audio: `{file:audio1}`\n5. Submit your prompt: Click on the arrow icon (or press Enter on your keyboard) to send your prompt to the assistant.\n6. Review the results: The assistant will process your request and display the results. ## Samples ### Document: - Prompt: \u201cSummarize the key points of the document `{file:contract}`\u201d.\n- Response: \u201cThe contract outlines the terms of service for a software subscription, including pricing, payment terms, and support options\u201d. ### Image: - Prompt: \u201cDescribe the image `{file:product_image}`\u201d.\n- Response: \u201cThe image shows a red sports car with black leather interior\u201d. ### Video: - Prompt: \u201cSummarize the main points of the video `{file:tutorial}`\u201d.\n- Response: \u201cThe video demonstrates how to use the new software feature for creating custom reports\u201d. ### Audio: - Prompt: \u201cTranscribe the audio file `{file:meeting_recording}`\u201d.\n- Response: The assistant will provide a text transcript of the audio recording.",
          "evaluationResultRowContextDocId": "97680bd3-a82d-4d62-87ac-fba252965c5b",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "2da6a799-144e-428a-8d11-59bbdff30d60",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?346",
          "evaluationResultRowContextName": "346",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "Synchronization Gateway: Allows synchronization of multiple parallel paths. The process automatically waits at this point until all incoming paths are completed. - Enhanced Task Flexibility: Now, it supports multiple inputs and outputs per task, significantly expanding the complexity and richness of the processes you can model. - **Meta-Agent Iris** Improvements - Enhanced LLM Selection Experience: When creating or editing an agent with Iris, users now benefit from a refined LLM selection flow, improving usability and model configuration accuracy. - **Flows** - **File support for Teams & Slack**: You can now easily send documents, images, audio, and video files through Teams and Slack when you integrate a Flow into these conversational channels. - **Evaluation Module** Enhancements - New Metrics Introduced: - **Faithfulness**: Assesses how factually consistent a response is with the retrieved context. - **Hallucination**: Calculated as 1 - Faithfulness, indicating the level of fabricated information. - **Context Precision**: Measures the proportion of relevant information within the retrieved contexts, compared against a reference answer for a given user input. (Note: Current calculation does not yet consider the position of retrieved chunks.) - **Noise Sensitivity**: This would involve analyzing the relationship between **Assistant Accuracy** and **Context Precision** across successive runs of an evaluation plan, varying the number of chunks retrieved. It examines how much and in what way the quality of the generated response changes when irrelevant content is added to the retrieved context. ### [Work in progress](#Work+in+progress) This section lists features and improvements still in development, with no confirmed release date. - [Prompt Files](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?946,,) option: It allows you to upload files at the organization and project level so that the Chat Assistant you define can use them to answer questions.\n\n- New assistant configuration to generate images.\n- Options to export/import agents, processes and flows.\n- Evaluation module backoffice.\n- Workspace/Playground - Support for real-time audio. - Share chat option.\n\n# Coming Next Explore the key innovation points in this roadmap, which reveal the improvements and breakthroughs on the way. ### [Features and updates for the upcoming release](#Features+and+updates+for+the+upcoming+release) - Support for **Model Context Protocol (MCP)** to integrate external tools. - The **GEAI proxy** is a Python-based component that enables dynamic integration of external tools into **Globant Enterprise AI (GEAI)** via **MCP**. It acts as a bridge between GEAI and one or more MCP-compliant tool servers. - Once the MCP servers are properly configured and connected through the GEAI proxy, the tools they expose become automatically available in the **Lab \u2192 Tools** section of GEAI, ready for use by any Agent without additional setup. - See more information about this protocol [https://modelcontextprotocol.io/introduction](https://modelcontextprotocol.io/introduction) - See how to [import tools into GEAI using MCP tool servers](1179.html) - New [/responses](1180.html) Endpoint for AI Interactions - We\u2019ve introduced a new /responses endpoint in Globant Enterprise AI (GEAI), which is fully compatible with the OpenAI Responses API. This addition allows developers to submit prompts as plain text, invoke functions, or pass files such as PDFs and images. The endpoint simplifies AI integration by supporting a familiar request/response structure, enabling a smoother transition for teams already using OpenAI-based workflows. - New [Images API](1183.html) - A new API is available that lets you generate images from text prompts. Supported providers: OpenAI, Vertex AI and xAI. - LLMs: - New Gemini models: - **Gemini 2.5 Pro Preview 'I/O edition'**: Built on its predecessor with significantly enhanced coding abilities and improved reasoning for complex tasks. Designed for developers and advanced users, this edition refines performance across benchmarks and expands its problem-solving reach. Release date: May 6th, 2025. - **Gemini 2.5 Flash**: Google's latest model built for complex problem-solving. It allows users to activate thinking and set a thinking budget (1\u201324k tokens). Designed to balance reasoning and speed, it delivers better performance and accuracy by reasoning before responding. - Updates in OpenAI's \"o\" series: - **o3**: The most powerful reasoning model in the \"o\" family; it pushes the frontier across coding, math, science, visual perception, and more. - **o4-mini**: A smaller model optimized for fast, cost-efficient reasoning; it achieves remarkable performance for its size and cost, particularly in math, coding, and visual tasks. - **o1-pro**: Available through our Responses API, offering a faster, more flexible, and easier way to create agentic experiences. - Over the next few weeks, the o1\u2011preview model will be migrated to the new o3 model, while o1\u2011mini will move to o4\u2011mini. More info in [Deprecated Models](843.html). - Refer to the [LLMs with Reasoning Capabilities](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?1168,,) article for step-by-step guidance on how to use reasoning-enabled models through the API. - The new **GPT-4.1** model series by OpenAI is now available in the production environment, featuring significant improvements in coding, instruction following, and long-context handling\u2014along with their first-ever nano model. - **Grok 3** Model Family added, including two pairs of models: - Lightweight Variants: - *grok-3-mini-beta* and *grok-3-mini-fast-beta* support function calling and enhanced reasoning (with configurable effort levels) for tasks like meeting scheduling and basic customer support. Both variants deliver identical response quality; the difference lies in response latency, with the \"fast\" version optimized for quicker responses. - Flagship Variants: - *grok-3-beta* and *grok-3-fast-beta* are designed for enterprise use cases such as data extraction, coding, and text summarization. They bring deep domain expertise in fields like finance, healthcare, law, and science. Similar to the mini variants, these models have identical capabilities, with the \"fast\" version offering reduced response times at a higher cost. - **Llama 4 collection** by Meta: We continue to expand our coverage of this model family. Recently added Llama 4 Scout and Maverick through Vertex AI's serverless API. Also available in Beta: Llama 4 Maverick via Groq and SambaNova, and Llama 4 Scout through the Cerebras provider, which offers this model with an inference speed of up to 2,600 tokens per second. - **Llama Nemotron Collection**: The Llama Nemotron Ultra and Super models are now available in Beta as Nvidia NIM microservices. These are advanced reasoning models, post-trained to optimize performance on tasks such as RAG, tool calling, and alignment with human chat preferences. Both models support a context window of up to 128K tokens. - Introducing the **OpenRouter** Provider (Beta): - OpenRouter joins the GEAI model suite with its [Auto Router](https://openrouter.ai/openrouter/auto) meta-model, which analyzes each user query and dynamically routes it to the most suitable LLM. This workflow maximizes response quality while minimizing cost and latency, delivering the most efficient output possible. - **Qwen3 Family** recently added: The latest generation in the Qwen large language model series, features both dense and mixture-of-experts (MoE) architectures to excel in reasoning, multilingual support, and advanced agent tasks. Its unique ability to switch seamlessly between a thinking mode for complex reasoning and a non-thinking mode for efficient dialogue ensures versatile, high-quality performance. Significantly outperforming prior models like QwQ and Qwen2.5, Qwen3 delivers superior mathematics, coding, commonsense reasoning, creative writing, and interactive dialogue capabilities. - Better processing of error messages, for example on those cases where the LLMs return specific errors. - New [Python SDK for Globant Enterprise AI (PyGEAI)](1148.html). It's composed of libraries, tools, code samples, and other documentation that allows developers to interact with the platform more easily with a 16K token context window. - New [omni-parser API](1165.html) to get the content of different file types. - RAG - Support for new audio and video [formats](137.html). - New endpoints to [reindex documents](29.html). - New parameters available when [ingesting documents](1142.html). - startPage and endPage to selectively process what is needed. - media parameters such as mediaPrompt, dialogue, frameSamplingRate and so on. - fix parameter *truncate* is not supported when calling the [cohere-rerank-3.5](625.html) model. - **The Lab** Enhancements - **Flows Integration**: The definition and management of [Flows](389.html) are now fully integrated into the Lab. - **Agentic Processes**: - New Conditional Gateway: Introduces the ability to define branching paths based on natural language prompts, enabling dynamic decision-making within processes. - New",
          "evaluationResultRowContextDocId": "ca90e485-3ba1-425a-a26d-70eb84540e78",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "3122e462-036c-464b-b3e4-fba60e0ed727",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?30",
          "evaluationResultRowContextName": "30",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# .web File Format The `.web` file format is a JSON file with a `.web` extension used to configure website crawling for [RAG Assistant](44.html). ### .web File Format Parameters The following parameters are available in the `.web` file format: | **Element Name** | **DataType** | **Description** |\n|---|---|---|\n| url | string | Mandatory: the base URL to crawl. |\n| excludeDirs | string[] | Optional list of web page directories to exclude; they must be absolute references. Here you can exclude unnecessary dependencies referenced in the HTML. |\n| maxDepth | number | The maximum depth to crawl. By default, it is set to 3; the system controls it cannot have a higher value. |\n| timeout | number | Timeout for each request, in the unit of milliseconds. By default, it is set to 10000 (10 seconds). |\n| preventOutside | boolean | Whether to prevent crawling outside the root base URL; _true_ by default. |\n| selectors | string[] | List of page selectors to consider; it uses the _body_ tag by default, and can be used in the following combinations: <ul><li>`*` - universal selector;</li><li>`div` - tag name;</li><li>`.foo` - class name;</li><li>`#bar` - id;</li><li>`[baz]` - attribute presence;</li><li>`[baz=buzz]` - attribute value (with any operators and also quotes and case sensitivity modifiers - [syntax](https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors#syntax);</li><li>`+` and `>` combinators (other combinators are not supported).</li></ul> It is recommended to analyze some of the web pages of the site and select the main container where the valuable text is set. |\n| linkBrackets | boolean | Add link brackets when processing an anchor tag; _false_ by default. |\n| ignoreHref | boolean | Remove href references; _false_ by default. | ### Using the .web File Format Once the file is uploaded (as shown in [Upload documents](60.html)), the ingestion process will crawl the site starting in the base URL (_url_ parameter) considering the _maxDepth_ parameter (to follow associated links in each page up-to _maxDepth_) and will exclude all base URLs (specified in the _excludeDirs_ parameter). When the process is finished, the crawl status will be displayed as usual, in this case associated to the uploaded file (in general as _Success_): ![Crawl Status](./images/292.png) You can check the [Chunks file](60.html) with the result of the processing, and validate that the detected _plain text_ by the crawler makes sense (to be used as semantic information for a RAG assistant). ### Sample Suppose you want to crawl a Wikipedia article. In this case, a possible `wikipedia_sample.web` file to upload could be configured as follows: ```json\n{ \"url\": \"https://en.wikipedia.org/wiki/GeneXus\", \"maxDepth\": 1, \"preventOutside\": true, \"timeout\": 10000, \"selectors\": [ \"#firstHeading\", \"#bodyContent\" ], \"linkBrackets\": false, \"ignoreHref\": true, \"excludeDirs\": [ \"https://en.wikipedia.org/static/\", \"https://en.wikipedia.org/w/\" ]\n}\n``` Note the following: - **url:** The base URL of the Wikipedia article.\n- **maxDepth:** Only crawl one level of the site.\n- **selectors:** Use the selectors by ID (`firstHeading`, `bodyContent`) to only process the important content from the site.\n- **linkBrackets:** Remove link references to keep the plain text as clean as possible.\n- **excludeDirs:** Exclude a couple of directories. #### Resulting Chunks File The resulting Chunks file will look something like this: ```json\n[ ... { \"pageContent\": \"GENEXUS From Wikipedia, the free encyclopedia Computer programming tool\\nThis article is about a computer programming tool. For the Fear Factory album, see Genexus (album). GeneXusDeveloper(s)GeneXusInitial release1988; 36 years ago (1988)Stable release\\nGeneXus 17\\n...\", \"metadata\": { \"source\": \"https://en.wikipedia.org/wiki/GeneXus\", \"language\": \"en\", \"description\": \"GeneXus - Wikipedia\", \"id\": \"someGUID\" } }, { \"pageContent\": \"...can be generated include COBOL, Java, Objective-C, RPG, Ruby, Visual Basic, and Visual FoxPro.[3] Some of the DBMSs supported are\\nMicrosoft SQL Server, Oracle, IBM Db2, Informix, PostgreSQL, and MySQL. GeneXus was developed by Uruguayan company ARTech Consultores SRL which later renamed to Genexus SA.[4] The latest version is...\", \"metadata\": { \"source\": \"https://en.wikipedia.org/wiki/GeneXus\", \"language\": \"en\", \"description\": \"GeneXus - Wikipedia\", \"id\": \"someGUID\" } } ...\n]\n``` ### See Also [.custom File Format](201.html)",
          "evaluationResultRowContextDocId": "9020065f-bb77-40f1-8f88-9aeeac036331",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "7be3d46c-d25a-41b0-8dec-c4b360f1b535",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?202",
          "evaluationResultRowContextName": "202",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#How to upload and manage Files via API It is possible to work with previously uploaded files to perform various operations such as summaries or analysis. To perform these operations, first it is necessary to upload the file using the [[450]], confirm the success of the operation, and then reference it in the requests to the models that require the file. Here is a step-by-step guide on how to upload and manage files using the File API. ## Step 1: Uploading a File Using the File API\nThe first step is to upload the file you want to manage. To do this, use the [[450##POST /files|POST /files]] endpoint of the File API. This endpoint allows you to upload a file and receive a confirmation that the file has been successfully uploaded. The name of the file, which will be used to reference it, can be specified in the `fileName` parameter or, if not provided, it will be the name of the uploaded file. ### Getting your projectId and organizationId You can get your `$PROJECT_ID` and `$ORGANIZATION_ID` using the [[22##GET /accessControl/apitoken/validate|GET /accessControl/apitoken/validate]] endpoint of the [[22]]. Keep in mind that you can replace `$SAIA_ORGANIZATION_APITOKEN` with your `$SAIA_PROJECT_APITOKEN` if needed. ### Sample cURL Command\nTo load a file, use the following sample:\n```bash\ncurl -X POST \"$BASE_URL/v1/files\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: multipart/form-data\" \\ -H \"fileName: $FILE_NAME\" \\ -H \"folder: testFolder\" \\ -H \"organizationId: $ORGANIZATION_ID\" \\ -H \"projectId: $PROJECT_ID\" \\ -F \"file=@/path/to/your_file.mp4\" ``` If you include the `fileName` parameter, that will be the name you will use to reference the file later. If you don't include it, you will need to use the original name of the uploaded file. For example, if you upload `\"file=@/path/to/your_file.mp4\"`, the reference name will be exampleFile (without the extension). ## Step 2: Validating the File Upload\nOnce the file has been successfully uploaded, the response will include information confirming the success of the operation. You don't need to use a dataFileId, but you can check the response to ensure the file was uploaded correctly. ### Sample response\n```json\n{ \"dataFileId\": \"string\", \"dataFileUrl\": \"string\", \"success\": true\n}\n```\n## Step 3: Create a Chat Assistant (if not created already) After uploading the file, you need to create a [[708]] to process the uploaded content, if you haven't already created one. This is done through the [[35##POST/assistant|POST /assistant]] endpoint of the [[35]]. ### Sample cURL Command ```bash\ncurl -X POST \"$BASE_URL/v1/assistant\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"type\": \"chat\", \"name\": \"vertex_ai/gemini-2.0-flash-exp -Test File Upload\", \"prompt\": \"You are a helpful assistant\", \"llmSettings\": { \"providerName\": \"vertex_ai\", \"modelName\": \"gemini-2.0-flash-exp\", \"temperature\": 0.10, \"maxTokens\": 8192, \"uploadFiles\": true } }'\n``` ## Step 4: Referencing the File Once the Chat Assistant is set up, you can reference the file in a request to the Assistant for analysis or summary, using the [[34]] and specifying the file name in the request body. ### Sample cURL Command to Summarize a File: ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"saia:assistant:vertex_ai/gemini-2.0-flash-exp -Test File Upload\", /* Using the assistant created in Step 3 named 'vertex_ai/gemini-2.0-flash-exp -Test File Upload' */ \"messages\": ~~[~~ { \"role\": \"user\", \"content\": \"Provide the content {file:$FILE_NAME}\" } ~~]~~, \"stream\": false }'\n``` ## Alternative: Inquire About Files - Interact Directly with LLMs You can also interact with LLMs directly, asking questions about files like images or videos. [regions]\n[region|cURL]\nSample 1: Ask GPT-4o About an Image\n```bash\ncurl --location '&lt;BASE_URL&gt;/chat' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;SAIA_PROJECT_APITOKEN&gt;' \\\n--data '{ \"model\": \"openai/gpt-4o\", \"max_tokens\": 1024, \"temperature\": 0.1, \"messages\": ~~[~~ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": ~~[~~ { \"text\": \"Tell me what this is about?\", \"type\": \"text\" }, { \"image_url\": { \"url\": \"&lt;IMAGE_FILE_URL&gt;\" }, \"type\": \"image_url\" } ~~]~~ } ~~]~~\n}'\n```\n[/region]\n[region|Python]\nSample 2: Python Code - Sending Gemini a Video File for Analysis\n```python\nimport requests\nimport json\nimport base64 # Define the API endpoint and authentication headers\nurl = \"&lt;BASE_URL&gt;/chat/completions\"\nheaders = { 'Content-Type': 'application/json', 'Authorization': 'Bearer &lt;SAIA_PROJECT_APITOKEN&gt;'\n} # Function to encode a file in Base64 format\ndef encode_file(file_path): \"\"\"Encodes a file in Base64 format for API submission.\"\"\" with open(file_path, 'rb') as file: return base64.b64encode(file.read()).decode('utf-8') # Replace 'path_to_your_file.mp4' with the actual file path\nfile_path = \"path_to_your_file.mp4\"\nencoded_file = encode_file(file_path) # Construct the API request payload\npayload = { \"model\": \"vertex_ai/gemini-1.5-pro-002\", \"messages\": ~~[~~ { \"role\": \"user\", \"content\": ~~[~~ { \"type\": \"text\", \"text\": \"What's this file about?\" }, { \"type\": \"image_url\", \"image_url\": f\"data:video/mp4;base64,{encoded_file}\" } ~~]~~ } ~~]~~\n} # Send the request to the API\nresponse = requests.post(url, headers=headers, json=payload) # Print response details\nprint(\"Status Code:\", response.status_code)\nprint(\"\\nResponse:\")\nprint(json.dumps(response.json(), indent=2))\n```\n[/region]\n[/regions]\n{{{12}}}{{{17}}} ##MARKDOWN##",
          "evaluationResultRowContextDocId": "94fd1b65-0ca6-4b3c-8acf-eed9148e7023",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "80d6df46-20ff-4fab-9eda-7cee6a36cbfd",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?454",
          "evaluationResultRowContextName": "454",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Responses API This API provides access to the Responses endpoint from Globant Enterprise AI (GEAI). It includes plain text inputs, function calling, and file inputs (PDFs and images). Check the [[20]] for generic variables needed to use the API. **Note:** This endpoint currently supports only OpenAI models. ## Endpoint\n| Method | Path |\n| --- | --- |\n| **POST** | `/responses` | </br>\nThis endpoint has the same interface (Request/Response) as the [https://platform.openai.com/docs/api-reference/responses|OpenAI Responses API]. ## 1. Basic text completion\n#### Request\n```\ncurl --request POST \\ --url $BASE_URL/responses \\ -H \"Authorization: Bearer $YOUR_SAIA_API_TOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"openai/o1-pro\", \"input\": \"Tell me a three\u2011sentence bedtime story about a unicorn.\", \"stream\": false, \"max_tokens\": 2000\n}'\n```\n</br>\n#### Expected Response ```\n{ \"instructions\": null, \"metadata\": {}, \"reasoning\": { \"effort\": \"medium\" }, \"usage\": { \"completion_tokens\": 272, \"prompt_tokens\": 18, \"total_cost\": 0.1659, \"completion_tokens_details\": { \"reasoning_tokens\": 192 }, \"prompt_tokens_details\": { \"cached_tokens\": 0 }, \"total_tokens\": 290, \"currency\": \"USD\", \"completion_cost\": 0.1632, \"prompt_cost\": 0.0027 }, \"created_at\": 1746473821, \"error\": null, \"tools\": [], \"output\": [ { \"summary\": [], \"id\": \"rs_6819...\", \"type\": \"reasoning\", \"status\": null }, { \"role\": \"assistant\", \"id\": \"msg_68191...\", \"type\": \"message\", \"content\": [ { \"annotations\": [], \"text\": \"Once upon a time, in a hidden meadow where flowers swayed like colored ribbons, a unicorn named Moonlight pranced gracefully beneath the twinkling stars. By day, she roamed among the sun-kissed hills, her soft mane shimmering with every gentle breeze. At night, Moonlight\u2019s silvery horn glowed softly, granting sweet, soothing dreams to all who believed in magic.\", \"type\": \"output_text\" } ], \"status\": \"completed\" } ], \"top_p\": 1, \"previous_response_id\": null, \"parallel_tool_calls\": true, \"temperature\": 1, \"tool_choice\": \"auto\", \"model\": \"o1-pro-2025-03-19\", \"id\": \"resp_bGl...\", \"text\": { \"format\": { \"type\": \"text\" } }, \"incomplete_details\": null, \"truncation\": \"disabled\", \"user\": null, \"object\": \"response\", \"max_output_tokens\": null, \"status\": \"completed\"\n}\n``` ## 2. Function calling\n#### Request\n```\ncurl --request POST \\ --url $BASE_URL/responses \\ -H \"Authorization: Bearer $YOUR_SAIA_API_TOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"openai/o1-pro\", \"input\": \"What is the weather like in Paris today?\", \"tools\": [ { \"type\": \"function\", \"name\": \"get_weather\", \"description\": \"Get current temperature for a given location.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"City and country e.g. Bogot\u00e1, Colombia\" } }, \"required\": [ \"location\" ], \"additionalProperties\": false } } ]\n}'\n```\n</br>\n#### Expected Response\n```\n{ \"instructions\": null, \"metadata\": {}, \"reasoning\": { \"effort\": \"medium\" }, \"usage\": { \"total_tokens\": 0, \"currency\": \"USD\" }, \"created_at\": 1745243167, \"error\": null, \"tools\": [ { \"name\": \"get_weather\", \"description\": \"Get current temperature for a given location.\", \"strict\": true, \"type\": \"function\", \"parameters\": { \"additionalProperties\": false, \"type\": \"object\", \"properties\": { \"location\": { \"description\": \"City and country e.g. Bogot\u00e1, Colombia\", \"type\": \"string\" } }, \"required\": [ \"location\" ] } } ], \"output\": [ { \"summary\": [], \"id\": \"rs_6806...\", \"type\": \"reasoning\", \"status\": null }, { \"name\": \"get_weather\", \"arguments\": \"{\\\"location\\\":\\\"Paris, France\\\"}\", \"id\": \"fc_6806...\", \"type\": \"function_call\", \"call_id\": \"call_ruYPRIF9qcOxft5XrfjCBQg8\", \"status\": \"completed\" } ], \"top_p\": 1, \"previous_response_id\": null, \"parallel_tool_calls\": true, \"temperature\": 1, \"tool_choice\": \"auto\", \"model\": \"o1-pro-2025-03-19\", \"id\": \"resp_6806...\", \"text\": { \"format\": { \"type\": \"text\" } }, \"incomplete_details\": null, \"truncation\": \"disabled\", \"user\": null, \"object\": \"response\", \"max_output_tokens\": null, \"status\": \"completed\"\n}\n``` ## 3. Sending files\n### 3.1 Image input There are two ways to attach an image: * URL \u2013 pass a publicly reachable image_url. Recommended when the file is already hosted and you want a minimal payload.\n* Base\u201164 inline \u2013 embed the binary data in a data:image/...;base64, URI. Recommended for local files, private content, or CI pipelines with no public storage. </br>\n**Option A \u2014 Public image URL (cURL)**\n```\ncurl --request POST \\ --url $BASE_URL/responses \\ -H \"Authorization: Bearer $YOUR_SAIA_API_TOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"openai/gpt-4.1\", \"input\": [ { \"role\": \"user\", \"content\": [ {\"type\": \"input_text\", \"text\": \"What is in this image?\"}, { \"type\": \"input_image\", \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" } ] } ] }'\n```\n</br>\n**Option B \u2014 Inline base\u201164 (Python)**\n```\nimport base64, json, requests token = \"YOUR_SAIA_API_TOKEN\"\nurl = \"$BASE_URL/responses\"\nheaders = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"} def encode_file(file_path): with open(file_path, 'rb') as file: return base64.b64encode(file.read()).decode('utf-8') file_path = \"/path/to/your/image.png\"\nencoded_file = encode_file(file_path) payload = { \"model\": \"openai/gpt-4.1\", \"input\": [ { \"role\": \"user\", \"content\": [{ \"type\": \"input_text\", \"text\": \"What's this image about?\" }, { \"type\": \"input_image\", \"image_url\": f\"data:image/png;base64,{encoded_file}\" } ] } ]\n}\nr = requests.post(url, headers=headers, json=payload)\nprint(json.dumps(r.json(), indent=2))\n```\n</br>\n**Supported MIME types for images** | Extension(s) | MIME type | Base64 prefix |\n| --- | --- | --- |\n| .png | image/png | data:image/png;base64, |\n| .jpg, .jpeg | image/jpeg | data:image/jpeg;base64, |\n| .gif | image/gif | data:image/gif;base64, |\n| .webp | image/webp | data:image/webp;base64, ### 3.2 PDF input (Python example)\n```\nimport base64, json, requests token = \"YOUR_SAIA_API_TOKEN\"\nurl = \"$BASE_URL/responses\"\nheaders = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"} def encode_file(file_path): with open(file_path, 'rb') as file: return base64.b64encode(file.read()).decode('utf-8') file_path = \"/path/to/your/sample.pdf\"\nencoded_file = encode_file(file_path) payload = { \"model\": \"openai/o4-mini\", \"input\": [ { \"role\": \"user\", \"content\": [ { \"type\": \"input_text\", \"text\": \"What's this file about?\" }, { \"type\": \"input_file\", \"filename\": \"sample.pdf\", \"file_data\": f\"data:application/pdf;base64,{encoded_file}\" } ] } ]\n}\nr = requests.post(url, headers=headers, json=payload)\nprint(json.dumps(r.json(), indent=2))\n``` **Note:** Only models that support both text and image inputs, such as gpt-4o, gpt-4.1, or o1, can accept PDF files as input. ## Availability Since May 2025 release. {{{12}}} {{{17}}}\n##MARKDOWN##",
          "evaluationResultRowContextDocId": "60900ab1-258b-445f-9a0c-ec190a5d4cc8",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "b213b4eb-b749-4c2d-9884-75e3b7013aaf",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?1180",
          "evaluationResultRowContextName": "1180",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataSetRowId": "5b791458-a1a6-457c-b357-ed946cd68ca8",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0040635600",
      "evaluationResultRowEndDate": "2025-05-13T11:14:18",
      "evaluationResultRowOutput": "Para subir un archivo a un proyecto en Globant Enterprise AI, debes usar la API de archivos mediante el endpoint `POST /files`. Aqu\u00ed tienes los pasos resumidos para hacerlo:\n\n1. **Obtener tus identificadores**: Consigue tu `$PROJECT_ID` y `$ORGANIZATION_ID` utilizando el endpoint `GET /accessControl/apitoken/validate`.\n\n2. **Realizar la petici\u00f3n para subir el archivo**:\n   - Usa el endpoint `POST /files` con autorizaci\u00f3n por token.\n   - Puedes indicar el nombre con el que quieres referenciar el archivo en el par\u00e1metro `fileName`. Si no lo indicas, se utiliza el nombre original del archivo.\n   - Incluye en la petici\u00f3n el archivo a subir en formato `multipart/form-data`.\n\n3. **Ejemplo de comando cURL para subir un archivo**:\n   ```bash\n   curl -X POST \"$BASE_URL/v1/files\" \\\n     -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n     -H \"Content-Type: multipart/form-data\" \\\n     -H \"fileName: $FILE_NAME\" \\\n     -H \"folder: testFolder\" \\\n     -H \"organizationId: $ORGANIZATION_ID\" \\\n     -H \"projectId: $PROJECT_ID\" \\\n     -F \"file=@/path/to/your_file.mp4\"\n   ```\n\n4. **Verificar la respuesta**: Al subir el archivo correctamente, recibir\u00e1s una respuesta JSON indicando el \u00e9xito, como:\n   ```json\n   {\n     \"dataFileId\": \"string\",\n     \"dataFileUrl\": \"string\",\n     \"success\": true\n   }\n   ```\n\nCon esto, el archivo estar\u00e1 disponible para ser referenciado y usado en tu proyecto.\n\nToda esta informaci\u00f3n y detalle la puedes revisar en la documentaci\u00f3n oficial sobre c\u00f3mo subir y gestionar archivos v\u00eda API en Globant Enterprise AI.\n\nReferencia: [https://wiki.genexus.com/enterprise-ai/wiki?454](https://wiki.genexus.com/enterprise-ai/wiki?454)",
      "evaluationResultRowStartDate": "2025-05-13T11:14:09",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "9050"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#Usage Limits API This API allows managing usage limits for both projects and organizations, providing functionality to define, retrieve, update, and delete usage limits. To use the API, check the [[20##Generic Variables|generic variables]] needed. In addition, read the explanation about usage limits in [[666]] and [[71]]. ## Endpoints | **Method** | **Path** | **Description** |\n|---|---|---|\n| `POST` | `/usageLimits/organizations/{organization}/limits` | Defines a new usage limit for an organization |\n| `GET` | `/usageLimits/organizations/{organization}/limits/latest` | Retrieves the latest usage limit defined for the organization |\n| `GET` | `/usageLimits/organizations/{organization}/limits` | Retrieves all usage limits defined for the organization |\n| `DELETE` | `/usageLimits/organizations/{organization}/limits/{id}` | Deletes a usage limit for the organization |\n| `PUT` | `/usageLimits/organizations/{organization}/limits/{id}/hardLimit` | Modifies the `hardLimit` of an active usage limit for the organization |\n| `PUT` | `/usageLimits/organizations/{organization}/limits/{id}/softLimit` | Modifies the `softLimit` of an active usage limit for the organization |\n| `PUT` | `/usageLimits/organizations/{organization}/limits/{id}/renewalStatus` | Modifies the `renewalStatus` of an active usage limit for the organization |\n| `POST` | `/usageLimits/organizations/{organization}/projects/{project}/limits` | Defines a new usage limit for a project |\n| `GET` | `/usageLimits/organizations/{organization}/projects/{project}/limits` | Retrieves all usage limits for a project |\n| `GET` | `/usageLimits/organizations/{organization}/projects/{project}/limits/latest` | Retrieves the latest usage limit defined for a project |\n| `GET` | `/usageLimits/organizations/{organization}/projects/{project}/limits/active` | Retrieves the active usage limit for a project |\n| `DELETE` | `/usageLimits/organizations/{organization}/projects/{project}/limits/{id}` | Deletes an active usage limit for a project |\n| `PUT` | `/usageLimits/organizations/{organization}/projects/{project}/limits/{id}/hardLimit` | Modifies the `hardLimit` for an active usage limit for a project |\n| `PUT` | `/usageLimits/organizations/{organization}/projects/{project}/limits/{id}/softLimit` | Modifies the `softLimit` for an active usage limit for a project |\n| `PUT` | `/usageLimits/organizations/{organization}/projects/{project}/limits/{id}/renewalStatus` | Modifies the `renewalStatus` for an active usage limit for a project | ## POST/usageLimits/organizations/{organization}/limits Defines a new usage limit for an organization. <blockquote>This endpoint requires a Globant Enterprise AI API token related to <strong>organization</strong> scope.</blockquote> ### Parameters | Name | Type | Description |\n|---|---|---|\n| `subscriptionType` | `string` | Type of subscription. Options: `Freemium`, `Daily`, `Weekly`, `Monthly` |\n| `usageUnit` | `string` | Unit for the usage limit. Options: `Cost` |\n| `softLimit` | `number` | Soft usage limit. |\n| `hardLimit` | `number` | Hard usage limit. |\n| `renewalStatus` | `string` | Whether the limit is renewable. Options: `Renewable`, `NonRenewable` | ### Request Body ```json\n{ \"subscriptionType\": \"string\", // Options: Freemium, Daily, Weekly, Monthly \"usageUnit\": \"string\", // Options: Cost (Only Cost is allowed for organization limits) \"softLimit\": number, // Recommended usage limit \"hardLimit\": number, // Maximum allowed usage \"renewalStatus\": \"string\" // Options: Renewable, NonRenewable\n}\n``` ### Response If the endpoint is successful, it will return a `201 Created` status with the usage limit data: ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` Error responses will return appropriate HTTP status codes (400 Bad Request, 401 Unauthorized, 403 Forbidden, 409 Conflict) with a JSON body containing an errors array: ```json\n{ \"errors\": [ { \"id\": integer, \"description\": string } ]\n}\n``` ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/v2/usageLimits/organizations/${organization}/limits\" \\ -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" \\ -H \"Accept: application/json\" \\ -d '{ \"subscriptionType\": \"Monthly\", \"usageUnit\": \"Cost\", \"softLimit\": 10.00, \"hardLimit\": 20.00, \"renewalStatus\": \"Renewable\" }'\n``` ## GET/usageLimits/organizations/{organization}/limits/latest Retrieves the latest usage limit defined for the organization. <blockquote>This endpoint requires a Globant Enterprise AI API token related to <strong>organization</strong> scope.</blockquote> ### Response If the endpoint is successful, it will return a `200 OK` status with the latest usage limit data: ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v2/usageLimits/organizations/${organization}/limits/latest\" \\ -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" \\ -H \"Accept: application/json\" ``` ## GET/usageLimits/organizations/{organization}/limits Retrieves all usage limits defined for a specified organization. <blockquote>This endpoint requires a Globant Enterprise AI API token related to <strong>organization</strong> scope.</blockquote> ### Response Returns a JSON object containing an array of usageLimits for the specified organization. If no usage limits exist for the organization, an empty array will be returned. ```json\n{ \"usageLimits\": [ { \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Organization\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime }, // ... more usageLimits (if any) ]\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v2/usageLimits/organizations/${organization}/limits\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" ``` ## DELETE/usageLimits/organizations/{organization}/limits/{id} Deletes an active usage limit for the organization. <blockquote>This endpoint requires a Globant Enterprise AI API token related to <strong>organization</strong> scope.</blockquote> ### Response If the endpoint is successful, it will return a `200 OK` status with the limit status set to \"Cancelled\": ```json\n\n{ \"usageLimits\": [ { \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Project\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime }, // ... more usageLimits (if any) ]\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v2/usageLimits/organizations/${organization}/projects/${project}/limits\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" ``` ## GET/usageLimits/organizations/{organization}/projects/{project}/limits/latest Retrieves the latest usage limit defined for a specified project within a given organization. <blockquote>This endpoint can be used whether the Globant Enterprise AI API token is related to the organization scope or to the project scope.</blockquote> ### Response Returns a JSON object representing the latest usage limit for the project. If no usage limit exists, a 404 Not Found error is returned. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Project\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v2/usageLimits/organizations/${organization}/projects/${project}/limits/latest\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" ``` ## GET/usageLimits/organizations/{organization}/projects/{project}/limits/active Retrieves the currently active usage limit for a specified project within a given organization. <blockquote>This endpoint can be used whether the Globant Enterprise AI API token is related to the organization scope or to the project scope.</blockquote> ### Response Returns a JSON object representing the active usage limit for the project. If no active usage limit exists, a 404 Not Found error is returned. The response structure is the same as for other successful limit retrieval endpoints. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Project\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, // Will be 1 (Active) \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v2/usageLimits/organizations/${organization}/projects/${project}/limits/active\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" ``` ## DELETE/usageLimits/organizations/{organization}/projects/{project}/limits/{id} Deletes a specified usage limit for a project. <blockquote>This endpoint can be used whether the Globant Enterprise AI API token is related to the organization scope or to the project scope.</blockquote> ### Response Returns the deleted usage limit object. The status field will be updated to 4 (Cancelled). The response structure is consistent with other successful limit retrieval endpoints. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Project\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, // Will be 4 (Cancelled) \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` Error responses will return appropriate HTTP status codes (401 Unauthorized, 403 Forbidden, 404 Not Found) with a JSON body containing an errors array (see example in previous responses). ### cURL Sample ```bash\ncurl -X DELETE \"$BASE_URL/v2/usageLimits/organizations/${organization}/projects/${project}/limits/${id}\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\"\n``` ## PUT/usageLimits/organizations/{organization}/projects/{project}/limits/{id}/hardLimit Modifies the hardLimit property of an active usage limit for a specified project. <blockquote>This endpoint can be used whether the Globant Enterprise AI API token is related to the organization scope or to the project scope.</blockquote> ### Request Body ```json\n{ \"hardLimit\": number\n}\n``` ### Response Returns the updated usage limit object. The hardLimit field will reflect the new value. The response structure is the same as for other successful limit retrieval endpoints. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Project\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` Error responses will return appropriate HTTP status codes (400 Bad Request, 401 Unauthorized, 403 Forbidden, 404 Not Found) with a JSON body containing an errors array (see example in previous responses). ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v2/usageLimits/organizations/${organization}/projects/${project}/limits/${id}/hardLimit\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" -d '{ \"hardLimit\": 3000 }'\n``` ## PUT/usageLimits/organizations/{organization}/projects/{project}/limits/{id}/softLimit Modifies the softLimit property of an active usage limit for a specified project. <blockquote>This endpoint can be used whether the Globant Enterprise AI API token is related to the organization scope or to the project scope.</blockquote> ### Request Body ```json\n{ \"softLimit\": number\n}\n``` ### Response Returns the updated usage limit object. The softLimit field will reflect the new value. The response structure is the same as for other successful limit retrieval endpoints. ```json\n{ \"hardLimit\": number, \"id\": string, \"relatedEntityName\": string, // Will be \"Pia.Data.Project\" \"remainingUsage\": number, \"renewalStatus\": string, \"softLimit\": number, \"status\": integer, \"subscriptionType\": string, \"usageUnit\": string, \"usedAmount\": number, \"validFrom\": string, // Datetime \"validUntil\": string // Datetime\n}\n``` Error responses will return appropriate HTTP status codes (400 Bad Request, 401 Unauthorized, 403 Forbidden, 404 Not Found) with a JSON body containing an errors array (see example in previous responses). ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v2/usageLimits/organizations/${organization}/projects/${project}/limits/${id}/softLimit\" -H \"Authorization: Bearer $SAIA_ORGANIZATION_APITOKEN\" -H \"Accept: application/json\" -d '{ \"softLimit\": 1500 }'",
          "evaluationResultRowContextDocId": "2e975d25-0835-4716-9e66-a0cff109e37d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "3979598e-1e51-4fdf-b21f-89314c80c77c",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?802",
          "evaluationResultRowContextName": "802",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "an appropriate name could be: ConvertArticleToMarkdown * **Task description:** Provide a brief description of what the task does. For example: Convert a documentation article to Markdown format, applying correct structure, headings, and style conventions. * **Task context:** Include relevant information that gives context to the task, such as details of the overall process or considerations that the Agent must take into account. For instance: This task is part of a documentation workflow in which generated articles must be transformed into Markdown to ensure consistency across publications. The Agent should format the content according to standard Markdown rules, maintaining clarity and proper use of headings, lists, and emphasis. * **Task instructions:** Specify clear instructions that guide the Agent in the execution of the task. These instructions are part of the prompt that will be sent to the Agent. For example: Take the provided article and convert it into a well-structured Markdown document. Use appropriate Markdown syntax for headings, bold, italics, bullet points, and code blocks where needed. Make sure the content remains clear, professional, and aligned with documentation standards. If you click on the expand icon, a larger editing window will open to make typing easier. ### Examples Examples allow you to define input/output pairs that help the Agent understand what kind of result is expected. This is especially useful in tasks involving content transformation, such as converting text to Markdown. [[997]] To add an example, click on the **\\+ Add** option under the Examples section. Two fields will appear: * **Enter the example input**: where you provide a sample of the original content. * **Enter the example output**: where you show how that content should look after processing. Although this section is optional, it is recommended to include at least one example when the output must follow a specific structure or formatting style. In this case, since the task consists of converting an article to Markdown, a useful example would be: * **Example input**: `This is the introduction. It explains how the system works and what the key components are. Features include: easy setup, automatic updates, and compatibility with all major platforms.` * **Example output**: `# Introduction` `This is the introduction. It explains how the system works and what the key components are.` `## Features` `- Easy setup` `- Automatic updates` `- Compatibility with all major platforms` ### Inputs Although it is not mandatory in this case, you can indicate an artifact that works as input if this task must work with a previously generated content. Insert **Text** as the input artifact. This artifact represents the content provided by the user when triggering the Agentic Process. Since this is the initial material to be processed, it must be configured as the input of the task. ### Outputs This field is mandatory to ensure that the task generates an output that can be used by the following tasks in the process. In this case, since the task involves validating a documentation article, a suitable output could be an artifact of type **MarkdownArticle**\u2014or similar\u2014containing the corrected and validated content. Once you have configured all the necessary fields, the task node will be displayed as follows: [[998]] ## Step 7: Add a Second Task To continue building the process, add a second task node following the same procedure described earlier. This task uses the **Expert Translator** agent to translate the article\u2014 already converted to Markdown\u2014into the target language specified by the user. #### **Configure the task:** * **Agent**: Click on the **Select** link in the Properties panel and choose the **ExpertTranslator** agent from the list. * **Task name**: `TranslateMarkdownArticle`\n\n* **Task description**: Translate a Markdown article into the target language provided by the user. * **Task context**: This task is executed after the article has been reformatted to Markdown. Its purpose is to make the content accessible in the language selected by the end user, while maintaining structure and readability. * **Task instructions**: Translate the provided Markdown content into the target language `{{language}}`. Make sure to preserve the original formatting, structure, and headings used in the Markdown document. The output should remain clear, professional, and easy to read. **Note**: The `{{language}}` syntax refers to a process variable. You must define this variable by clicking on the process name (in this case, Documentation Assistant) at the top of the screen, selecting \"Edit process variables\", and adding a new variable named `language` of String type. [[999]] [[1001]] * **Input**: Select `MarkdownArticle`, which is the output of the previous task. * **Output**: Define an artifact named `TranslatedMarkdownArticle`. ## Step 8: Add an event Once you have configured the tasks, you can continue building the flow by adding a new task or event. To add a new task, follow the same procedure explained in the previous step: go to one of the connection points of the current node (for example, the one at the bottom), click on the \\+ icon and select Task. If you want to add an event instead, click on the same \\+ icon and select the Event option from the context menu. This action will insert an event type node that can be used to handle intermediate situations within the process. [[1002]] After selecting Event, a new node is inserted and automatically connected to the task. The right panel (Properties) opens automatically, allowing you to define the event that will trigger the next action. In this panel, complete the following fields: [[1003]] * **Signal name**: Enter a descriptive name for the event signal (e.g., `MarkdownReady`). * **Type**: In this field, a dropdown menu will appear with two options: `User` and `Artifact.` In this case, you must select `User` because the event will be triggered when a specific artifact is generated, acting as a continuation signal within the process. * **Inputs**: When you select `Artifact`, the *Inputs* section is enabled automatically. Here, you must define which artifact should be available for the event to be triggered. This behavior is similar to how the *Start* node works when configured with the *Artifact* type. The event node acts as an intermediate pause in the process and can be triggered either by the generation of a specific artifact or by a user action. In this case, the **Type** is set to `User`, which means the process will wait until the end user confirms that they want to proceed with the next step. ## Step 9: Add the Finish Node To complete the flow, add a **Finish** node after the event by clicking on the **\\+** icon and selecting **Finish**, just as you did for the previous elements. [[1004]] ## Step 10: Test the Agentic Process Once the flow is complete, you can test how the process behaves from an end-user perspective. Click on the **Run Test** button at the bottom right of the screen. This opens the testing panel, where you can simulate the interaction with the Agentic Process. [[1005]] In the testing panel: * Fill in the **Subject** field with a brief title or description.\n* Upload a document in the **Artifact (Text)** field. This document will be used as the input for applying the Globant writing style and converting it to Markdown.\n* Enter the **Language** you want the final output translated into (e.g., English, Portuguese, etc.). [[1006]] Once all fields are completed, click on **Run** to execute the process. After clicking on **Run**, the Agentic Process begins execution. It processes the input using the first task and pauses when it reaches the **event node**. At this point, the process waits for user confirmation to proceed. As shown in the image, a **MarkdownReady** section appears with a **Send** button next to it. [[1007]] To continue and complete the process, the end user must **click on the Send button**. This confirms that the Markdown version is ready and triggers the next step in the workflow, enabling the translation task to run and allowing the process to reach the **Finish** node. ## Step 11: Publish the Process Once the entire Agentic Process has been configured and tested, the final step is to publish it. To do this, click on the **Publish** button located in the bottom-right corner of the screen. This action makes the process available in the Playground, where it can be accessed and used like any other assistant. ### Delete a node To delete a node you must click on it. You will know that the node is selected when you see the circles around it. Then press the Delete key on your keyboard to remove it from the flow. ## Availability Since April 2025 release. {{{12}}}{{{17}}}##MARKDOWN##\n\n#How to create an Agentic Process An Agentic Processes represents a sequence of tasks that must be executed to achieve a specific result. These tasks may involve interactions with users, AI Agents and external tools. Agentic Processes are organized as a logical flow that can be initiated by an end user action or by the availability of a previously generated artifact. This is a step-by-step guide to create, test, and publish an Agentic Process that connects two AI Agents: the first one applies the Globant writing style to a user-provided text and converts it into Markdown format; the second one takes that Markdown content and translates it into the target language selected by the user. ## Step 1: Access The Lab First, log in to the [[42]]. In the Project Dynamic combo box, select the project you want to work with; in this case, Default(Docum Team) is used. Next, on the left side of the screen, you will find the Backoffice menu. In this menu, click on **The Lab.** [[975]] By clicking on The Lab in the Globant Enterprise AI Backoffice, a new window opens in the browser with [[972]]. ## Step 2: Go to the Agentic Processes Dashboard Once inside Globant Enterprise AI Lab, you will see the Side Navigation Menu on the left edge of the screen. If the menu is collapsed, click on the arrow icon to expand it. [[986]] In this menu, select the Agentic Processes option to access the Agentic Processes Dashboard. If no Agentic Processes have been created yet, a message saying \"No Agentic Processes Created\" will appear. Below that message, click on the New Processes button to start creating a new process. ## Step 3: Fill in the Process Details After clicking on the New Process button, a pop-up window titled Process Details will appear on the screen. Here you must complete the following fields: [[987]] * **Process Name:** Enter a unique and descriptive name for the process. * **Process Description:** Optionally, include a brief explanation of what the process is intended to do. Once the information is complete, click on **Save** to continue. ## Step 4: Set the Initial Signal After saving, a new tab with the name of your process opens automatically in the Agentic Processes dashboard. This tab shows a grid where you can design the flow of your process. By default, it includes a **Start** node at the center of the screen. This node represents how the interaction begins. [[988]] Click on **Start** to open the **Properties** panel on the right side of the screen. [[989]] In the **Signal name** field, you can define a label for the trigger, and in the Type dropdown, you must select one of the following options: * **User:** The process starts based on a user-initiated action. * **Artifact:** The process is triggered when a specific artifact is created. If you select **Artifact**, an additional **Inputs** section appears, where you must define a variable representing the artifact that should trigger the process. [[990]] An artifact is the (intermediate or final) result produced by a task within the process. It can be, for example, a generated document, a response produced by an Agent, or any other type of structured content that represents a deliverable within the process flow. Click on **Add+** and select the appropriate artifact from the dropdown menu. This instructs the process to begin only after that specific artifact is available. In this case, the **Signal name** is set to `UserInput`, and the **Type** selected is **Artifact**. In the **Inputs** section, `Text` is added. This configuration is used because the *Subject* field that end users can fill when interacting with the Agentic Process is very limited in length. Therefore, if the user wants to apply the Globant writing style to a longer piece of content, that content must be uploaded as a file artifact. ## Step 5: Add a Task Node To add a task node, move your cursor over the **Start** block. When you do this, you can see four circular anchor points around it (top, right, bottom, left). Hovering near any of these points displays the \\+ icon. [[991]] Click on the \\+ icon that appears when hovering over the anchor point of your choice. When you do this, a context menu appears with three options: **Task**, **Event**, and **Finish**. Click on **Task** to add a new task node to the process. This task represents the first action to be executed once the process is triggered. Once the task is added, the node appears and automatically opens the **Properties** panel on the right side of the screen, which allows you to configure the details of the task. [[992]] In addition to creating new tasks by clicking on the **\\+** icon next to a node, you can also add a task by dragging it directly from the **Toolbox**. To do this, go to the **Toolbox** (located in the bottom left corner) and click on **Task**. Then, drag the new task node to the desired position on the canvas. Once placed, connect the task to the rest of the flow by clicking on the **\\+** icon from the node you want it to connect to. A dotted line will appear, indicating the connection. Once confirmed, the flow will integrate this task into the process. ## Step 6: Configure the Task A task in an Agentic Process represents a unit of action that is executed as part of the overall flow. It may involve different activities, such as invoking an AI Agent, requesting information from a user, executing an external tool, processing data, or generating an artifact as an output. Each task can have inputs (for example, artifacts generated by previous tasks or user data) and outputs (new artifacts or actions that trigger subsequent tasks). To configure the task, fill in the available fields in the Properties panel: [[993]] * Agent * Task name * Task description * Task context * Task instructions * Examples * Inputs * Outputs ### Select an Agent At the top of the Properties panel, within the Agent block, click on the Select link. This action opens a pop-up window listing all the Agents previously defined in AI Lab for the current project. [[994]] In this window, you can use the search field to filter Agents by name or description. When you select one of them, its detailed information is displayed on the right side, including: [[995]] * Agent name and description * AI model used * Reasoning strategy configured * Tools Once you've confirmed that this is the Agent you want to use, click on the Select button. The Agent is now assigned to the task, and its name will appear in the Properties panel instead of \u201cSelect\u201d. This task uses the **Documentation Assistant** agent to apply Globant\u2019s writing rules and convert the text into Markdown format. ### Complete the task fields In the Task section, you must configure the following fields. They allow you to define precisely what the task should do and are combined with the Agent's capabilities to execute the corresponding action. [[996]] * **Task name:** Specify a clear name that identifies the task. In this case, since the goal is to review the article content,",
          "evaluationResultRowContextDocId": "05b50d0d-54f6-40f4-869d-d16f7160191e",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "57822db4-5e00-4a83-8841-3ba2534d0bbc",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?985",
          "evaluationResultRowContextName": "985",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Start Page The Globant Enterprise AI Start Page allows offering end users a clear guide to the assistant's capabilities and examples of use. To achieve this, the Start Page allows you to define the following elements that guide end users in their interaction with the assistant: - Define a welcome message that clearly describes the purpose of the assistant. - Describe the assistant's key functions, explaining the problems it is designed to solve, the actions it can perform correctly and its known limitations, ensuring that the end user has realistic expectations about its performance.\n- Add sample prompts that guide the user on how to interact with the assistant. These prompts can be specific requests such as \"Show me the composition of the company\u2019s profit during the last quarter\" or \"Create a bar chart with the sales data for the first 6 months of the year\", which provide insight into the interactions that the assistant can effectively handle. These guidelines are not only fundamental to improving the end user experience, but can also be leveraged in technical scenarios such as batch evaluations or synthetic data generation, providing a clear framework for understanding and analyzing the assistant's behavior in more advanced contexts. ## Start Page Presentation Depending on the type of assistant you are configuring, the layout of the Start Page may vary slightly. There are two presentations: one for the Assistants ([Data Analyst Assistant](886.html), [API Assistant](110.html), [Chat with Data Assistant](159.html)) and one for the [RAG Assistants](44.html). Despite these visual differences, both versions share the same essential functionalities. To configure the Start Page for both Assistants and RAG Assistants, first go to the corresponding page in the [Backoffice](42.html) to work with your data and click on 'UPDATE'. After that, depending on the type of assistant, the following tabs will be displayed: - Assistants: 'General Information' and 'Start Page'.\n- RAG Assistants: 'General Information', 'Prompt', 'Retrieval' and 'Start Page'. ## Assistants In the Assistants Start Page option, first you will find the option to configure the Welcome Message, where you define a title and a description to explain how the assistant will help the end user. Clicking on the 'CONFIRM' button will save this data and the message will appear in the Playground when accessing the assistant, helping the user to get started. In addition, you can add information about specific features of the assistant using the CREATE FEATURE button. Clicking on it opens a window for you to enter a title and a short description about the feature you are describing. After entering the title and description, click on CONFIRM to save the defined feature. Once saved, the feature will be displayed in the Playground along with its description. You can also set up sample prompts by clicking on CREATE EXAMPLE PROMPT. These predefined prompts will appear in the Playground and, when selected, will be completed automatically, facilitating the end user's interaction with the assistant and showing practical suggestions to perform certain tasks. Clicking on the CREATE EXAMPLE PROMPT button opens a window to enter the following information: - Name: Specify a name for the prompt.\n- Description: This field is optional; you can add a brief explanation describing the functionality of the prompt and how it should be used.\n- Text: This field is mandatory and here you define the content that will be used as an example in the prompt. The assistant will take this text to perform the configured task. Once you have entered the information, click on CONFIRM to save the prompt. ## RAG Assistants The RAG Assistants Start Page has a different layout compared to that of Assistants. Instead of the Welcome Message section used in Assistants, for RAG Assistants this section is called Basic Information. Here you define a title and description that clearly explain how the assistant will help end users in their tasks. This information is essential to provide users with information about the assistant's capabilities. Once you have completed the title and description, click on CONFIRM to save the changes. In RAG Assistants, instead of two buttons to create functionalities and examples of prompts, there is only one button: CREATE INFORMATION ITEM. When you click on CREATE INFORMATION ITEM, a window will open where you can enter a title, a description and the text that will make up the body of the information item. In addition, in this window you must select the type of item you want to create: Prompt or Feature. If you choose Prompt, you will define specific examples that will guide the end user when interacting with the assistant. If you select Feature, you will be describing a specific property of the assistant, clearly explaining its usefulness and how it can be used. Once you have completed the required fields, click on the SAVE button, and then click on CONFIRM (in Start Page) to save your changes.",
          "evaluationResultRowContextDocId": "0e63b8cf-b481-4530-8320-a108aaca5167",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "58bd5788-9e67-4535-9830-22e99bf74511",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?408",
          "evaluationResultRowContextName": "408",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# How to connect a Flow to B2Chat Once you have created your first Flow in Globant Enterprise AI following the steps in [How to create a Flow](322.html), you might want to integrate it with different communication platforms, including [B2Chat](https://www.b2chat.io/en/features/console/), to manage agent overflow scenarios (Human-in-the-Loop). Here is a step-by-step guide on how to connect a Flow through B2Chat. ## Step 1: Get a username and password from B2Chat Go to the URL [https://app.b2chat.io](https://app.b2chat.io) and log in. In the menu on the left, click on General Setting and then select Integrations. ![Image](./images/675.png) After accessing the Integrations section, select BotApi. There you will see the values corresponding to the username and password you will need. ![Image](./images/676.png) ## Step 2: Create an application key 1. Access the Flow you wish to expose. In the Side Navigation Menu, select [Configuration](371.html) and then Application Keys.\n2. Click on the Generate Key button. ![Image](./images/474.png)\n3. In the Type field, select API and in Key Name, enter a name such as \"B2ChatIntegration\". Next, confirm the operation by clicking on Save. ![Image](./images/825.png)\n4. Copy the generated key using the copy icon next to the key. ![Image](./images/826.png) ## Step 3: Get the Bot id Go to the Side Navigation Menu, and click on the [Overview](371.html) option in the Overview section. There you will see a summary of the Flow including the Bot id, along with other details such as the Flow's name and purpose. The Bot id is the alphanumeric identifier that appears below the name of your Flow. ![Image](./images/395.png) Copy the Bot id. ## Step 4: Configure the flow Access the Side Navigation Menu > [Configuration](371.html) > Integrations > B2Chat and fill in the required data: - **Endpoint:** It is the B2Chat URL https://api.b2chat.io\n- **B2Chat webhook:** B2Chat uses this URL to answer messages and reach the bot. The URL format is: `https://botbuilder.fluentlab.ai/fluentlab/b2chat/<bot-id>?x-api-key=<api>` - `<bot-id>`: This is the identifier you obtained in Step 3: Get the Bot id - `<api>`: This is the application key you generated in Step 2: Create an application key.\n- B2Chat username\n- B2Chat password ## Step 5: Configure interactions Once you have configured B2Chat, click on Flow. When doing so, you will notice that now there is a new space available called **B2Chat** in the Interactions Section: ![Image](./images/677.png) To create a basic agent escalation flow, you must drag the B2Chat Resolver component into the flow. This component is key because escalation to a human agent will take place at the point where it is inserted. ![Image](./images/678.png) ## Step 6: Testing To test the connection to B2Chat, use the **Playground** or one of the methods available in Flow (such as Run Flow, Open Testing Assistant, or Go to Demo Page). ![Image](./images/679.png) Once the flow reaches the B2Chat Resolver, escalation to the human agent will take place. Then, in the B2Chat console, the conversation will appear in the \"Chats\" tab, and the agent will be able to respond from there. ### B2Chat console ![Image](./images/680.png) ### Run Flow ![Image](./images/681.png) For the conversation to end in the B2Chat console, the agent must click on \"Close\". By doing so, the flow will continue to run and activate the next nodes according to the selected configuration.",
          "evaluationResultRowContextDocId": "05d7b228-0dc7-4063-94b7-fa30f602d80f",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "c6212661-0fdc-4365-a2d6-b5d133d68504",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?674",
          "evaluationResultRowContextName": "674",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.2,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "5b9225e2-ff17-4b7e-91b0-c220145f06d1",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0040412400",
      "evaluationResultRowEndDate": "2025-05-13T11:14:24",
      "evaluationResultRowOutput": "To use the Usage Limits API endpoints described in the documentation, you need the following:\n\n- A **Globant Enterprise AI API token </B>with the appropriate scope.\n  - For organization-related endpoints, the API token must be related to the **organization scope**.\n  - For project-related endpoints, the API token can be related to either the **organization scope </B>or the **project scope**.\n\nThis means that all calls to the API must be authorized using an API token valid for the corresponding organization or project, depending on which endpoint is being used.\n\nFor example:\n\n- To define a new usage limit for an organization (POST `/usageLimits/organizations/{organization}/limits`), the API token must have **organization scope**.\n- To retrieve usage limits for a project, the token can have **organization scope or project scope**.\n\nThis requirement is explicitly stated in the documentation sections describing each endpoint, including the necessity of the API token in the `Authorization` header.\n\nAdditionally, the endpoints require standard HTTP headers including at least:\n\n- `Authorization: Bearer <API_TOKEN>`\n- `Accept: application/json`\n\nYou can find detailed information about the API token requirements and scopes under each endpoint description in the Usage Limits API documentation.\n\nReference: [Usage Limits API documentation](https://wiki.genexus.com/enterprise-ai/wiki?802) [5](https://wiki.genexus.com/enterprise-ai/wiki?802)",
      "evaluationResultRowStartDate": "2025-05-13T11:14:18",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "9379"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "- New Average Request Time metric added in the Project Dashboard. ### January 8th - The option formerly known as 'Search Documents' has been improved and renamed to RAG Assistant (Retrieval Augmented Generation) to provide an optimized experience when searching and generating information. #### Frontend - Feedback is provided during conversations with RAG Assistants, indicating where you are in the process.\n- 'Response streaming' support for RAG Assistants.\n- Settings are hidden when selecting an assistant, except when 'Chat with LLMs' is selected. ## 2023 ### December 19th - Fixed: Too Many Redirects when accessing Playground using a browser in Spanish language. ### December 6th - New backoffice design.\n- Access to the Playground from the backoffice to chat with the assistants defined in the project.\n- Upload images for analysis with GPT-4 Vision.\n- Google Analytics support at the frontend.\n- Keep a conversation thread when chatting with documents.\n- An email notification is sent automatically when a new member is invited to join the organization or project. ### November 28th - First version officially released!! ### November 6th - The following OpenAI models are supported: GTP-4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), and GPT-4 Vision (gpt-4-vision-preview). ### October 18th - [Manage quotas per project](71.html): It is possible to set quota limits to control project expenses or usage. ### October 11th - [AI-Driven Load Balancing](32.html): The platform automatically manages the Load Balancing process when you work with generative AI providers, efficiently addressing the limits imposed by LLM platforms.\n\n- In the [Organization API](22.html), the ability to set and manage usage limits on projects through the [POST /project](22.html) and [GET /project/{id}](22.html) endpoints has been added.\n- [Quota Limit](71.html) now includes improvements such as highlighting the active quota in green, offering options to cancel active quotas, among others. ### October 17th - Rebranding to Globant Enteprise AI\n- Improvements in RAG - New option not to [compact the conversation history](297.html) - Support for Redis, MongoDB Atlas and PG Vector [vectorstores](267.html) - New endpoint to get an embedding. Check details here: [Embeddings API](422.html)\n- Playground improvements\n- File management improvements\n- New [LLMs supported](200.html) - NVIDIA provider with new models supported - nvidia.nemotron-mini-4b-instruct - meta.llama-3.1-8b-instruct - meta.llama-3.1-70b-instruct - meta.llama-3.1-405b-instruct - meta.llama-3.2-3b-instruct - Groq provider supported - groq/llama-3.1-70b-versatile - groq/llama-3.2-11b-vision-preview - groq/llama-3.2-3b-previewgroq/llama-3.2-1b-preview\n- New [embeddings models](447.html) added - Vertex AI: - vertex_ai/textembedding-gecko - vertex_ai/text-embedding-004 - vertex_ai/textembedding-gecko-multilingual - Nvidia: - nvidia/nvclip - nvidia/nv-embed-v1 - nvidia/baai.bge-m3 - nvidia/snowflake.arctic-embed-l - nvidia/nv-embedqa-mistral-7b-v2 - nvidia/embed-qa-4 - nvidia/nv-embedqa-e5-v5 ### September 25th - [Support for file](346.html) processing with prompt-based assistants. This will enable many scenarios, such as uploading documents and summarizing, extracting, and checking information, etc. Also, depending on the model used by the assistant, it will be able to process audio, video, or images.\n- [Support for multi-modal](345.html) LLMs allow processing docs, audio, video, and images in models like GPT-4o or Gemini Pro.\n- Chat with data assistants - The model used to build the queries was updated with GPT-4o, which improves the quality of the generated query. - Configure the query builder server by organization and project. This means you can connect with different DBMS from each project when building Chat with data assistants. - Show an explanation of how the query was built.\n- New [Playground Interface](349.html) design - New design - Upload documents from the front end to chat with them.\n- [Flows builder](321.html) - There will be two types of Flows, one more oriented to build a conversational UI and the other to build assistant flows. Access to these flows will only be available through [Chat API](34.html) or through the [channels](384.html) offered by Flows.\n- New models hosted in AWS Bedrock added: - Amazon Titan Express v1 - Amazon Titan Lite v1 - Anthropic Claude 3 Haiku - Anthropic Claude 3 Sonnet - Anthropic Claude 3.5 Sonnet - Cohere Command - Meta Llama 3 8B - Meta LLama 3 70B\n- It is now possible to provide clear guidance on the assistant's capabilities, allowing you to add information such as descriptions, features, and example prompts. This configuration can be done from the Backoffice, [Start Page](408.html), or WelcomeData section of the [Assistant API](35.html) and [RAG Assistants API](29.html) endpoints.\n- RAG Assistants - New endpoint in [RAG Assistants API](29.html): using [DELETE/profile/{name}/documents](29.html), all documents associated with the specified RAG Assistant are deleted. ### August 9th - Support of new models - GPT-4o mini\n- RAG Assistants - New option called [CLEANUP](60.html) allows to delete the documents associated to a RAG Assistant. - When creating a new assistant, the following defaults are updated: - [Chunk Count](69.html) updated from 2 to 5. - [Model Name](68.html) from gpt-3.5-turbo to gpt-4o-mini.\n- Data Analyst Assistant - New [Conversational Module](291.html) for Data Analyst Assistant.\n- Considerations - **Important**: [max_tokens integer below minimum value](290.html) error may occur if you are using the _max_tokens_ value as -1; those Assistants must be updated to use a correct positive value. ### July 4th - Permissions by assistants in role-based access control. A new [role and permissions management system](242.html) has been implemented to provide greater flexibility and control. Please read [Migration to the new Roles and Permissions Management System](264.html).\n- Dashboard performance optimizations. Please read [Using the Request Summary Table for Improved Dashboard Performance](273.html).\n- RAG Improvements - New option to customize the [context](205.html). - Support for [Max Marginal Relevance](206.html) - Support for _similarity_hybrid_ and _semantic_hybrid_ for [Azure AISearch](207.html) - Advanced options for SelfQuery retriever prompt case. See [Self Query Use Case](150.html) - Support for Parent Document retrieval option. See [Parent Document Retrieval Strategy](204.html) ### June 10th - [Enterprise AI Proxy](19.html) is deprecated. Use [Chat API](34.html) instead.\n- Support for new LLMs - OpenAI new model GPT-4o - Models in Google Vertex - Gemini 1.0 Pro - Gemini 1.5 Flash preview-0514 - Gemini 1.5 Pro preview-0514 - Claude 3 Haiku - Claude 3 Opus - Claude 3 Sonnet\n- RAG Improvements - New option to initialize RAG Assistant based on another when creating a new RAG Assistant. - New option to export document list in View Documents over a RAG Assistant. - Added filter options when browsing Documents. - SelfQuery RAG retriever partial support for a [customized Prompt](150.html). - Support for text-embedding-004 in Google models to generate the embeddings.\n- Deprecated [Assistant API](35.html) endpoints. - /assistant/text/begin - /assistant/text\n- Support to deploy in Google Cloud Platform. ### May 8th - New [Chat with Data Assistant](159.html).\n- New [Ingestion SDK](172.html) to automate document ingestion in RAG assistants.\n- New models hosted in NVIDIA platform supported. See [Supported Chat Models](200.html) for more details.\n- New option to export information about projects and members available for the organization administrator.\n- New API to extend dataset for [Data Analyst Assistant 1.0](89.html).\n- New filter by user email in Requests.\n- Update default to use _text-embedding-3-small_ OpenAI Embeddings for new RAG assistants.\n- Support for gemini-1.5-pro-preview-0409 model added. ### April 3rd - New [API Assistant](110.html).\n- New [Data Analyst Assistant 1.0](89.html).\n- The user Dashboard displays the email to identify the most active user or the one who generates the most costs. ### March 11th - GeneXus Identity Provider is implemented, expanding the login options in the [Backoffice](42.html) of the production environment. This allows for login not only with Google but also with Apple or GeneXus Account.\n- It is possible to customize the icon for each assistant. ### February 29th - [Frontend](82.html) improvements in UI/UX.\n- Option to get feedback from end users when interacting with [RAG Assistant](44.html).\n- Gemini Pro LLM support.\n- New [Dashboard](562.html) with user metrics.\n\n# Updates Log This log shows the most important fixes or features added to the platform. ## Log ## 2025 ### April 10th - New [Globant Enterprise AI Lab](972.html) - The Lab is designed for defining, managing, and orchestrating autonomous AI agents. It provides a standardized model for representing agents, their capabilities, and their interactions within complex workflows. The core components of the Lab include: - Agents & Tools: This module allows for the definition and management of individual agents and their resources, such as skills and tools. It serves as a central hub for cataloging and managing the agent workforce. - Agentic Processes: This component enables the definition of processes based on tasks executed by Agents. These workflows facilitate collaboration among agents to achieve larger objectives. More details at [How to create an Agentic Process](985.html). - Agent RunTime: This module provides the execution environment for agentic workflows, where agents perform tasks based on their skills and interact with artifacts, driven by events and the flow of knowledge. - The Lab aims to meet the growing demand for intelligent, self-sufficient AI agents capable of collaborating and solving complex problems with minimal human intervention. It offers a flexible and adaptable model, allowing for the creation and management of a diverse range of agents, from co-pilots working alongside humans to fully automated agents executing complex tasks. Implemented as a module of Globant Enterprise AI, the Lab supports the development of intelligent agents that can work autonomously or in collaboration with humans and other agents.\n- New features in Flows - Agent Integration Component: You can now directly integrate agents created with the AI Lab into a Flow. These agents can be exposed through platforms like WhatsApp, Teams, or Slack. - [File Upload Support](1053.html) from WhatsApp: Flows now support receiving file attachments such as documents, images, audio, and video directly from WhatsApp interactions. - Audio and Video Attachment in Web Chat: The Web Chat component now allows users to attach audio and video files, enhancing the interaction experience.\n- New Features for the Data Analyst Agent - Reduced Configuration Requirements: The setup needed to enable the assistant to respond to a wide range of questions has been minimized. - Enhanced Analysis Module: An additional analysis module has been incorporated to complement the responses with relevant business conclusions and interpretations of the obtained data.\n- New metrics to track processed tokens. - A new view has been added to each project dashboard to monitor token-based consumption. More details at [Tokens tab in Dashboard](965.html). - To enable this feature, you must run the initialization process described in [Reprocessing Requests and Loading the Summary Table](1015.html).\n- LLMs: - New Gemini 2.5 Pro (via providers Vertex AI and Gemini): Gemini 2.5 is Google\u2019s latest reasoning model, engineered to tackle increasingly complex challenges. This model is designed for tasks that demand advanced analytical thinking and robust problem-solving capabilities. More details at [LLM API](663.html). - Migration to Gemini 2.0 series (Vertex AI): Based on recommendations from Vertex AI, we have migrated from the legacy Gemini 1.0 and 1.5 models to the more advanced Gemini 2.0 series, offering improved performance, scalability, and integration capabilities. For comprehensive information, please refer to the [Deprecated Models](Deprecated Models). - New Azure OpenAI models (o1, o1-mini and o3-mini): We have expanded our model availability by introducing these models via Azure, providing the same high-quality capabilities as those offered through the OpenAI provider. - DeepSeek-R1 via AWS Bedrock: Recently added through a Serverless API, DeepSeek-R1 offers reliable inference with a substantial 128K token context window and up to 32K maximum output tokens. - OpenAI's new models with built-in web search tool: These specialized models integrate web search capabilities directly into the Chat Completions API, enabling them to both interpret and execute search queries in real time. - openai/gpt-4o-search-preview - openai/gpt-4o-mini-search-preview - New models - Beta only: - gemini/gemma-3-27b-it: Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 has a large, 128K context window, and multilingual support in over 140 languages. - SambaNova: - DeepSeek-R1: This Provider offers the fastest performance for running DeepSeek, processing up to 198 tokens per second per user, with a 16K token context window. The model is hosted in US data centers; privately and securely. - DeepSeek-V3-0324: This model significantly outperforms its predecessor with enhanced reasoning benchmarks, improved code executability and refined web aesthetics, and superior Chinese writing aligned with the R1 style. It also offers better multi-turn interactive rewriting, translation quality, detailed report analysis, and more accurate function calling. - QwQ-32B: SambaNova provides access to QwQ-32B-Preview, the best open source test-time compute model released by Alibaba. - Llama 4 collection by Meta: - Llama 4 Scout: A 17B-parameter multimodal MoE model with 16 experts that excels in text and image understanding. The model is currently in beta and is available via providers Nvidia, Groq and SambaNova. Via Groq, it supports a 128k tokens context window with fast inference at 460 tokens/sec, while SambaNova Cloud runs at 697 tokens/second/user. - Llama 4 Maverick: Available via Nvidia, this 17-billion-parameter model featuring 128 experts supports a 32k-token context window. ### March 10th - New LLMs: - GPT-4.5 - Claude 3.7 Sonnet (Providers Anthropic, Vertex AI and AWS Bedrock) - Updates in Gemini 2.0 series: - vertex_ai/gemini-2.0-flash-lite-preview-02-05 - vertex_ai/gemini-2.0-flash-thinking-exp-01-21\n- RAG Revision #6 - Support for o3-mini, gpt-4.5-preview, claude-3-7-sonnet-20250219, new DeepSeek, Gemini2* and sambanova LLM providers. - New pinecone provider available for [embeddings](447.html) and [rerankers](638.html). - The [CleanUp](60.html) action message has been corrected to clearly specify that it will permanently delete the RAG Assistant files and update the information in the RDS. - Added [usage](712.html) element on every response. - Improvements when changing the [LLM](69.html)/[Embeddings](68.html) settings; all models and providers are normalized to be selected from standard combo-box items; use the [override mechanism](209.html) if you need other options. - Support for [guardrails](840.html). - New [documentAggregation](298.html) property to decide how sources are grouped and returned.\n- It is possible to [provide feedback](170.html) on the response of the Chat with Data Assistant in the Frontend.\n\n- The new [Evaluation APIs](893.html) introduce key functionalities through three interconnected APIs: [DataSet API](839.html), [Evaluation Plan API](849.html), and [Evaluation Result API](856.html). This version is primarily designed for users with a data science profile and is mainly accessed via APIs, complemented by a series of Jupyter notebooks that demonstrate their use. For a comprehensive guide on how to use these APIs, you can refer to [How to evaluate an AI Assistant](881.html) and the [EvaluationAPITutorial.ipynb](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?887,,) notebook, which provide practical examples and code for working through the evaluation process.\n- File attachment support in Flows (version 0.9).\n- Support for Full Story integration in the Workspace/Playground to generate user access statistics in Full Story.\n- In the [LLM API](663.html), for models that have descriptions in the specified languages, the `descriptions` property is included in the Response, which contains the descriptions in the available languages, such as `Spanish`, `English` and `Japanese`.\n- [Data Analyst Assistant 2.0](886.html) version presents important improvements, simplifying the interaction with the data by reducing the main components to just two: **Dispatcher** and **Thinker**. In addition, the metadata structure is automatically generated when loading the datasets, streamlining the setup process. For more information, see [How to create a Data Analyst Assistant](105.html).\n- The option to consult version-specific documentation is now available. Articles with versions show the option \u201cOther document versions\u201d in the header. Clicking on \u201cOther document versions\u201d brings up a menu that allows you to choose between the most recent version (\u201cLatest\u201d) or earlier versions (e.g. \u201c2025-02 or prior\u201d). If you select a version other than \u201cLatest\u201d, a message appears: \u201cThis is not the latest version of this document; to access the latest version, click here\u201d. This message provides a direct link to the most up-to-date documentation. [Components Version Update](822.html) ### February 10th - New documentation with details about [Supported Chart Types](731.html).\n- New [Usage Limits API](802.html).\n- Flows - Logs associated with conversations are accessible from the [conversation history](378.html).\n- RAG Revision #5 - Ingestion supports new [file formats](137.html). For on-premises installations, update the _FILE_TYPES_ parameter. - New optional _cleanProcessing_ parameter when using [Parent Document Retrieval Strategy](204.html). - [Embeddings API](422.html) cache support; disabled by default from a [RAG assistant](68.html). - New [mode](68.html) property for using multimodal embeddings and creating a [Multimodal RAG](783.html).\n- New endpoint [GET /accessControl/apitoken/validate](22.html) returns information about the organization and project associated with the provided apitoken.\n- New LLMs: - Already in production - **OpenAI**: o3-mini - Already in Beta - **DeepSeek**: - deepseek/deepseek-reasoner - deepseek/deepseek-chat - azure/deepseek-r1 - nvidia/deepseek-ai-deepseek-r1 - groq/deepseek-r1-distill-llama-70b - sambanova/DeepSeek-R1-Distill-Llama-70B - Updates in Gemini 2.0 series: - gemini-2.0-flash-thinking-exp-01-21 (Via Providers Gemini and Vertex AI) - gemini/gemini-2.0-flash-lite-preview - gemini/gemini-2.0-pro-exp - vertex_ai/gemini-2.0-flash-001 - sambanova/Llama-3.1-Tulu-3-405B [Components Version Update](822.html) ### January 13th - Internationalization, [Backoffice](42.html), and frontend support for Japanese.\n- Invitations now include information about the organization and project in the subject.\n- New LLMs - Already in Production - **OpenAI**: o1 (2024-12-17 version) - Already in Beta - New Providers: **Cohere** - **Cohere**: Cohere-r\n- [Guardrails](682.html) configured by assistant.\n- [Rerank API](625.html) to semantically order a list of document chunks given a query.\n- New optional RAG [Retrieve and Rerank](620.html) adds an extra layer of precision to ensure that only the most relevant information reaches the model used in the generation step. ## 2024 ### December 12th - [Automatic Creation of Default Assistant](640.html)\n- [Organization Usage Limits](666.html): It is possible to set quota limits to control organization expenses or usage.\n- Chat with Data Assistant - [Show details](170.html) about the generated query in the Playground. - Support in [Chat API](34.html) to interact with Chat with Data Assistant.\n- Flows - Support for markdown when showing the response on the different channels supported by Flows (web, Slack, WhatsApp, and Teams). - New component for connecting flows to the agent overflow console (Human-in-the-loop) via B2Chat. Please read [How to connect a Flow to B2Chat](674.html).\n- RAG - [Chroma](537.html), [Elasticsearch](579.html), [OpenSearch](580.html) vectorstore support. - [Improvements](581.html) when processing PDF documents (update to RAG module revision #4 or higher). - Integration with [AWS Knowledge Bases](577.html) at query time.\n- Data Analyst Assistant - Option to update metadata options. - New version by default in new Data Analyst assistants.\n- New LLMs - OpenAI: gpt-4o-2024-11-20 - AWS Bedrock: Anthropic Claude 3.5 Haiku - Amazon Nova models (Micro, Lite, and Pro) - Llama 3.1 405B on Vertex AI - Beta: - Support for providers Cerebras, SambaNova and xAI (Grok models). - All new Gemini Experimental models.\n- Security - It is possible to define a user 'all-members@domain.com' to give access to the Frontend to all users of the domain '@domain.com'. Permissions are enforced based on the roles assigned to this user. Please read [Allowing Frontend Access for Users from Specific Domains](657.html) ### November 12th - Flows execution integrated into the Playground\n- New [LLMs support](200.html) - **OpenAI**: o1-preview and o1-mini - **Claude Sonnet 3.5 v2 - Providers**: Anthropic, Vertex AI, and AWS Bedrock - **Llama 3.2 models - Providers**: Vertex AI and AWS Bedrock\n- Chat with data assistants - Possibility to edit metadata, [entities](170.html), and attribute descriptions. - The Properties tab has been renamed to [Settings](170.html) along with the options that can be configured in it.\n- RAG - New [returnSourceDocuments](298.html) option to disable returning the documents section used to answer the question. - New [step](298.html) option to use the assistant as a retrieval tool. - Support for [custom history](85.html) in conversations using the _chat_history_ variable.\n- Stand-alone Frontend based on the new Playground UI - Options to customize the Frontend to use the client logo, color palette, welcome message, and descriptions. - Feature to collect feedback (thumbs up/down) in each response. - Google Analytics support.\n- Data Analyst Assistant - Support to upload large CSV files.",
          "evaluationResultRowContextDocId": "68ace6a6-8b1b-4736-a466-9ed0a67b97a9",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "0a1bf151-f752-47ff-8003-e15a4bb6075b",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?31",
          "evaluationResultRowContextName": "31",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Supported Embeddings Models **List of supported embeddings models** | **Provider** | **Model** | **Multimodal** | **Description** | **Environment Support** |\n|---|---|---|---|---|\n| **openai** | text-embedding-ada-002<br>text-embedding-3-small<br>text-embedding-3-large|No<br>No<br>No<br>No|Must set dimensions to 1536, other values are not enabled by default<sup>(1)</sup> | Beta, QA, Production|\n| **azureopenai** | * | | Same models as openai, configure the associated URL endpoints | Beta, QA, Production |\n| **google/googlevertexai/vertex_ai**<sup>(3)</sup>|multimodalembedding@001<sup>(2)</sup><br>text-embedding-004<br>text-embedding-005<br>text-multilingual-embedding-002<br>|Yes<br>No<br>No<br>No|The dimension parameter must be configured to 768<sup>(1)</sup> or 1408<sup>(2)</sup> | Beta, QA, Production |\n| **awsbedrock** | amazon.titan-embed-image-v1<br>cohere.embed-english-v3<br>amazon.titan-embed-v2<br>cohere.embed-english<br>cohere.embed-multilingual|Yes<br>Yes<br>No<br>Yes<br>No|Check the model dimensions configuration<sup>(1)</sup>| Beta, QA, Production |\n| **nvidia** | baai.bge-m3<br>nv-embed-v1<br>nvclip<br>embed-qa-4<br>nv-embedqa-e5-v5<br>snowflake.arctic-embed-l<br>nv-embedqa-mistral-7b-v2|No<br>No<br>Yes<br>No<br>No<br>No<br>No|Check the model dimensions configuration<sup>(1)</sup> | Beta, QA, Production |\n| **cohere** | embed-english-v3.0<br>embed-multilingual-v3.0<br>embed-english-light-v3.0<br>embed-multilingual-light-v3.0|Yes<br>No<br>No<br>No| 1024 dimensions except for the \"*light*\" models using 384. | Beta, QA, Production |\n| **pinecone**<sup>(2)</sup> | multilingual-e5-large<br>llama-text-embed-v2|No<br>No| 1024 dimensions. | Beta, QA | <sup>(1)</sup> - For [[441]], the associated module must be installed and configured. <sup>(2)</sup> - The configuration must be done in the [[70|Retrieval tab]] by configuring the [[298|Profile Metadata]] parameter. For more information, refer to [[298##Not registered Providers and Models|Not registered Providers and Models]]. Please note that the associated `Embeddings costs` are not recorded, and no related `Requests` are logged as they are processed by the provider. <sup>(3)</sup> - Check the [https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#supported-models|Provider recommendations]. {{{12}}}{{{17}}}\n##MARKDOWN##",
          "evaluationResultRowContextDocId": "9d067949-05dd-404f-8c5a-55e97750d270",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "21c7a6f3-c405-4366-9a54-f5084c6ae492",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?447",
          "evaluationResultRowContextName": "447",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Configuration - General Information [[65]] ## Name\nIdentifying name of the assistant that can be customized according to your preferences. ## Description\nDetailed description of the purpose and capabilities of your assistant. This is a place to provide information about how the assistant enhances the end user experience. ## Status\nIndicates whether the assistant is enabled or disabled for use. ## Embeddings Settings\nAn embedding model is a type of machine learning model designed to convert data\u2014like words, phrases, or sentences\u2014into dense numerical vectors, known as embeddings. These embeddings capture the semantic meaning of the input. Specific parameters related to embeddings and related model characteristics: ### Provider Name\nDetermines the embeddings model service provider used by your RAG assistant. This is a mandatory parameter that can take any of the following values: - **azure**: To use Azure OpenAI. - **cohere**: To use cohere models. - **vertex_ai**: To use Google Vertex AI, check valid models [https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding|here], you can set for example *embedding-001*, *text-embedding-004*, *text-embedding-005* embeddings models<sup>(1)</sup> of 768 Dimensions. - **awsbedrock**: To use embeddings models from AWS Bedrock. - **openai**: To use OpenAI. ### Model Name\nSpecific name of the model being used; [[447|supported values]] depend on the Provider Name selected. For example, if the Provider Name parameter has the \"openai\" value, Model Name takes the value 'text-embedding-ada-002' for assistants created up to 4/11/2024 and 'text-embedding-3-small' onwards. {{{216|If you need to use a model not detailed on the list, use the [[298##Not registered Providers and Models|overide mechanism]]|||}}} ### apiKey\nAPI authentication key provided to access the language model service. For example, if the Provider parameter is \"azureopenai\", you must specify \"apiKey\" with the authentication key. ### Type\nIt allows configuring the information processing approach for the embeddings model. It only applies for the following providers: - CoHere (using awsprovider): `search_document` (default), `search_query`, `classification`, `clustering`. - Google: `RETRIEVAL_DOCUMENT` (default for embeddings), `RETRIEVAL_QUERY` (default for querying), `SEMANTIC_SIMILARITY`, `CLASSIFICATION`, `CLUSTERING`, `TASK_TYPE_UNSPECIFIED` - nvidia: `passage`, `query`; check [https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/task-types|here] for more detail. ### useProxy\nUse Globant Enterprise AI proxy to get embeddings, all requests will be logged (enabled by default). ### cache\nCache the embeddings for the current Organization/Project (disabled by default), needs to enable the `useProxy` property too. ### BatchSize\nDetermines how many items are grouped in each batch when calling the embeddings model during the ingestion process. By default, `batchSize` is set to 100 items. However, you may need to adjust this parameter based on the specific embeddings model you are using. ### Mode\nDefined the mode options are: - `empty` (default) will use text embeddings. - `multimodal` support for image embeddings (notice the model must support it). ### endpoint\nThe URL pointing to the language model service (advanced). ### Dimensions\nIt allows configuring the index dimensionality in the vectorStore. Each model has a specific dimension, and the default value is 1536, associated with the OpenAI provider. <sup>(1)</sup>: Depending on the **Provider Name** and **Model Name** selected for your embeddings, you will need to check the **Dimensions** value. By default, it assumes 1536, which is valid for OpenAI *text-embedding-ada-002*, *text-embedding-3-small* and *text-embedding-3-large* (3072) shrunk to that value. Check the [[140|troubleshooting]] section for more detail. **Notes:** - Please [[274|contact the help desk ]] if you want to use a **Model Name** with a **Dimensions** value other than 1536, as it must match the **Vector Store Provider** configuration in the Index section.\n- If you change the Provider Name, Model Name, Dimensions, it is recommended to reingest all your data. {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "b496e5f2-2bce-4300-aad8-66eb65337440",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "55e66f05-0e13-4b1a-97bd-93a108fbdbf7",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?68",
          "evaluationResultRowContextName": "68",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#LLMs with Reasoning Capabilities This article explains how to enable and control the reasoning features available in three leading Large Language Model (LLM) providers\u2014OpenAI, Anthropic, and Google Vertex AI\u2014when calling the `chat/completions` endpoint. For each provider, you will find an overview of the relevant parameters and a cURL example that follows the standard request format used in GEAI projects. ## Providers\n### 1. OpenAI\nOpenAI\u2019s o-series reasoning models expose an optional [https://platform.openai.com/docs/api-reference/chat/create#chat-create-reasoning_effort|`reasoning_effort`] parameter that lets you choose how many tokens the model may devote to internal reasoning. This gives you explicit control over latency and cost. | Allowed values | Effect |\n| --- | --- |\n| `low` | Minimal extra reasoning (fastest, cheapest) |\n| `medium` | Balanced reasoning vs. speed/cost |\n| `high` | Maximum reasoning effort (slowest, highest cost) | #### **Sample cURL** ```\ncurl -X POST \"$BASE_URL/chat/completions\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"openai/o4-mini\", \"messages\": [ { \"role\": \"user\", \"content\": \"Could you explain and compare in detail the relationships between category theory and homotopy theory in modern algebraic topology, describe concrete applications to the classification of fibrations, then outline how these ideas extend to theoretical physics\u2014especially quantum field theory and quantum gravity\u2014considering AdS/CFT correspondence and renormalisation methods in non\u2011commutative geometry, and finally discuss the philosophical consequences of these advances on the unity of physical laws?\" } ], \"stream\": false, \"temperature\": 1, \"max_completion_tokens\": 100000, \"reasoning_effort\": \"high\"\n}' ``` ### 2. Anthropic Claude 3.7 Sonnet offers an [https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking|Extended Thinking] mode. Enable it by adding a top\u2011level `thinking` object with a `budget_tokens` field that sets the token budget for reasoning. * Range: 0 to \u2248\u202f63000 (Claude\u2019s max output tokens = 64\u202f000).\n* Note: Above 32000 tokens, the model may not consume the entire budget. #### **Sample cURL**\n```\ncurl -X POST \"$BASE_URL/chat/completions\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"anthropic/claude-3-7-sonnet-latest\", \"max_tokens\": 20000, \"thinking\": { \"type\": \"enabled\", \"budget_tokens\": 16000 }, \"messages\": [ { \"role\": \"user\", \"content\": \"Are there infinitely many prime numbers that leave a remainder of 2 when divided by 3?\" } ]\n}' ``` ### 3. Google Vertex AI Google\u2019s latest model, [https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash|Gemini 2.5 Flash Preview], also supports a reasoning budget via a `thinking` object. * Range: 1 to 24\u202f000 tokens. #### **Sample cURL** ```\ncurl -X POST \"$BASE_URL/chat/completions\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"vertex_ai/gemini-2.5-flash-preview-04-17\", \"messages\": [ { \"role\": \"user\", \"content\": \"A farmer buys 30 animals consisting only of chickens and cows, and when he counts their legs he gets 74\u2014how many of the animals are cows?\" } ], \"thinking\": { \"type\": \"enabled\", \"budget_tokens\": 10000 }, \"stream\": false, \"temperature\": 0.1\n}'\n``` {{{216| Although [https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro|Gemini 2.5 Pro Preview] is also a reasoning\u2011capable model, its reasoning feature is currently fixed and cannot be enabled, disabled, or tuned|||}}} ##Availability Since April 2025 release. {{{12}}}{{{17}}} ##MARKDOWN##",
          "evaluationResultRowContextDocId": "2d0746ec-0c30-4a2f-882c-733bfb8b8b46",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "717d3a3d-b29f-4135-98a5-04d0ab298340",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?1168",
          "evaluationResultRowContextName": "1168",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.5,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "5be6e907-e52c-4450-9393-a4505ddc225b",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0035554000",
      "evaluationResultRowEndDate": "2025-05-13T11:14:28",
      "evaluationResultRowOutput": "Los modelos de OpenAI que est\u00e1n soportados solo en beta seg\u00fan la documentaci\u00f3n son:\n\n- o1 (2024-12-17 version)  \n- o1-preview  \n- o1-mini  \n\nEsta informaci\u00f3n se encuentra en la secci\u00f3n de actualizaciones donde se listan los nuevos LLMs en producci\u00f3n y en beta. Tambi\u00e9n se menciona la disponibilidad de modelos o3-mini en producci\u00f3n y otros modelos nuevos.\n\nPuedes consultar m\u00e1s detalles en el registro de actualizaciones del 2024/2025.\n\nReferencia: [3](https://wiki.genexus.com/enterprise-ai/wiki?31)",
      "evaluationResultRowStartDate": "2025-05-13T11:14:24",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "8678"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# File Support in Assistants File Support in Assistants allows you to work with various file types, including documents, images, videos, and audio, to perform tasks like summarizing documents, extracting information, comparing files, and generating text based on file content. This opens up a world of possibilities for using assistants in new and innovative ways. ## Scenarios The end user should be able to interact with assistants using files in the following ways: - Summarize a document: Provide a concise summary of the document's content.\n- Extract specific information: Extract specific data points from a document, such as the amount, client name, and deadline from a contract.\n- Compare documents: Compare the information in two documents, identifying matching and differing clauses.\n- Generate text based on document content: Create new text based on the content of a document.\n- Ask questions based on uploaded documents: Similar to [RAG Assistants](44.html), but with the ability to upload new documents from the [Playground Interface](349.html) to add to the assistant's context. ## Functionality File Support in Assistants provides a robust set of features for managing and using files: - File Management: You can manage files associated with each project using CRUD operations (create, read, update, delete). A project-level API is available for file management, using the project identifier as the key. You can also configure a storage limit per project to manage storage space effectively.\n- File Referencing: You can reference files within assistant prompts using variables like `{file:filename}` or `{file:*}`. For example, you can use the prompt: \"Compare `{file:id1}` with `{file:id2}`\". The system seamlessly handles file uploads, context window size, and summarization, regardless of whether the model is multimodal or not.\n- File Types: File support extends beyond documents to include images and videos, using a similar syntax: `{file:image1}` or `{file:video1}`. For example, you can use the prompt: \"Extract the text from image `{file:id1}` and summarize its content\".\n- Frontend Upload: End users can easily select existing files or upload new ones when interacting with file-handling assistants. ## How to Use File Support in Assistants 1. Choose your assistant: Select the assistant that best suits your needs.\n2. Upload your file: Click on the \u201cUpload File\u201d button and select the file you want to use. The assistant will recognize the file type and process it accordingly.\n3. Write your prompt: Enter your instructions for the assistant in the prompt box. Be clear and specific about what you want the assistant to do.\n4. Use file references: To reference the uploaded file in your prompt, use the following syntax: - Documents: `{file:filename}` (for example, {file:contract}) - Images: `{file:image1}` - Videos: `{file:video1}` - Audio: `{file:audio1}`\n5. Submit your prompt: Click on the arrow icon (or press Enter on your keyboard) to send your prompt to the assistant.\n6. Review the results: The assistant will process your request and display the results. ## Samples ### Document: - Prompt: \u201cSummarize the key points of the document `{file:contract}`\u201d.\n- Response: \u201cThe contract outlines the terms of service for a software subscription, including pricing, payment terms, and support options\u201d. ### Image: - Prompt: \u201cDescribe the image `{file:product_image}`\u201d.\n- Response: \u201cThe image shows a red sports car with black leather interior\u201d. ### Video: - Prompt: \u201cSummarize the main points of the video `{file:tutorial}`\u201d.\n- Response: \u201cThe video demonstrates how to use the new software feature for creating custom reports\u201d. ### Audio: - Prompt: \u201cTranscribe the audio file `{file:meeting_recording}`\u201d.\n- Response: The assistant will provide a text transcript of the audio recording.",
          "evaluationResultRowContextDocId": "97680bd3-a82d-4d62-87ac-fba252965c5b",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "374b9b40-5f44-464b-b0bc-0c3d4d7a272d",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?346",
          "evaluationResultRowContextName": "346",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# File support for multimodal models Multimodal models allow interactions that go beyond traditional text-based prompts. These models can understand and process information from a variety of sources, including text, file documents, images, audio, and video. This means they can analyze, process, and generate content based on a richer and more complete set of inputs. Globant Enterprise AI implements multimodal models, allowing you to interact with models such as Gemini and OpenAI in a completely new way. By combining text with files, a world of possibilities opens up for analyzing, processing, and generating content. These models can directly receive files alongside your prompts, eliminating the need to manually extract and inject file content. This direct file integration offers several advantages. - Simplifies file handling. You no longer need to extract and inject file content into the prompt.\n- Files are sent directly to the model, streamlining the process and enabling direct model interaction.\n- Empowers you to interact directly with files within your prompts, making it easier to analyze, process, and generate content based on their contents. This eliminates the need for manual extraction and injection, simplifying your workflow.\n- Leverages the information contained in files to provide more comprehensive and insightful responses, enhancing their capabilities. ## File References in Prompts You can reference files in your prompts using a specific format: `{file:filename}`. For example, to reference a file named \"report.pdf\" you would write: `{file:report}`. > **Note**: This reference format only works for files that have already been uploaded to your project. If you're uploading a file for the first time, you'll need to use the standard file upload mechanism within the platform. ## How Files are Managed in Your Project 1. **File Storage**: All files uploaded to your project are stored securely and can be accessed by your prompts.\n2. **File Access**: When you reference a file in your prompt, the system automatically retrieves it and sends it to the multimodal model.\n3. **File Types**: The system supports a wide range of file types, including text documents, images, audio files, and more. ## Sample Suppose you have a document called \"product_specs.txt\" loaded in your project. You can use it in your prompt as follows: ```plaintext\n\"Please summarize the key features of the product described in {file:product_specs}\".\n``` The multimodal model will then access the \"product_specs.txt\" file and use its contents to generate a summary of the product features. ## See Also [Supported models and file formats](406.html)",
          "evaluationResultRowContextDocId": "381b4a5d-cb99-40b8-a58e-e67f82082418",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "37a1beaa-aca1-4c33-9bf6-111820a7dc1f",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?345",
          "evaluationResultRowContextName": "345",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Enterprise AI For GeneXus Documentation, please visit [wiki.genexus.com](https://wiki.genexus.com/) <div style=\"background:#f8f8f8; padding:100px 128px\"> <span style=\"display: block; color: #004742; font-size: 46px; font-weight: 700; margin-bottom: 16px; line-height: 54px;\">Globant Enterprise AI Documentation</span> Delve into all the documentation of Globant Enterprise AI and stay ahead with insights into upcoming features. [Start reading](https://wiki.genexus.com/enterprise-ai/wiki?15,GeneXus+Enterprise+AI+Overview) [Learn with training videos](https://training.genexus.com/es/aprendiendo/cursos/inteligencia-artificial/genexus-enterprise-ai/introduccion-al-curso-6105673) --- - [Overview](15.html)\n- [API Reference](20.html) - [Chat API](34.html) - [Organization API](22.html) - [RAG Assistant API](29.html) - [Chat with Documents API](33.html) - [Embeddings API](422.html) - [Assistant API](35.html) - [File API](450.html) - [Feedback API](407.html) - [Error Codes](23.html)\n- [Using variables in Prompts](85.html)\n- [Tokens](293.html)\n- [Backoffice](42.html)\n- [Frontend](82.html)\n- [Playground](349.html)\n- [Start Page](408.html)\n- [Data Analyst Assistant](89.html) - [How to use Data Analyst Assistant](105.html) - [How to set the language in Data Analyst Assistant](464.html) - [Conversational Module](291.html)\n- [API Assistant](110.html) - [How to use API Assistant](123.html)\n- [Chat with Data Assistant](159.html) - [How to use Chat with Data Assistant](170.html) - [How to generate metadata for the Chat with Data Assistant](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?178,,) - [Chat with Data Assistant Use Guidelines](177.html) - [How to change the output format and filter results in Chat with Data Assistant](181.html)\n- [RAG Assistant](44.html) - [How to create and configure a RAG Assistant](60.html) - [File Formats](137.html) - [Use Guidelines](61.html) - [Update Configuration](64.html) - [Configuration - General Information](68.html) - [Configuration - Prompt](69.html) - [Configuration - Retrieval](70.html) - [Index Configuration](145.html) - [Supported Vector Store Provider](267.html) - [Advanced configuration](209.html) - [Context Prompt Template](205.html) - [VectorStore Search Options](206.html) - [Self Query Use Case](150.html) - [Parent Document Retrieval](204.html) - [Azure AISearch Search Options](207.html) - [Profile Metadata configuration](298.html)\n- [Flows](321.html) - [How to create a Flow](322.html) - [How to create a Flow with Conditional component](363.html) - [How to expose a Flow through Teams](391.html) - [How to expose a Flow through Slack](477.html) - [How to expose a Flow in a web page](473.html) - [User's Guide for Globant Enterprise AI Flow Builder](371.html) - [Management of Variables in Flows](470.html) - [Flows Conversation History](378.html) - [User Metrics in Flows](380.html) - [Message Metrics in Flows](382.html) - [Flows Configuration](384.html) - [Flows Section](389.html) - [Interactions to Create a Flow](494.html)\n- [User Access Rights Management](242.html) - [Roles](245.html) - [Members](253.html)\n- [File support for multimodal models](345.html) - [How to upload and manage Files via API](454.html) - [Supported models and file formats](406.html)\n- [File Support in Assistants](346.html)\n- [Ingestion SDK](172.html)\n- [AI-Driven Load Balancing](32.html)\n- [Managing quotas per project](71.html)\n- [Supported LLMs](200.html)\n- [Updates Log](31.html)\n- [Coming Next](30.html)\n- Troubleshooting - [File Upload](199.html) - [LLM](290.html) - [Embeddings](140.html) - [Vectorstores](308.html) - [Flows with RAG](445.html)",
          "evaluationResultRowContextDocId": "8b185fd7-17c6-4f63-88f2-154a2b6b3824",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "4fa050ed-6c25-4773-9657-3c48eb18df68",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?8",
          "evaluationResultRowContextName": "8",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "Synchronization Gateway: Allows synchronization of multiple parallel paths. The process automatically waits at this point until all incoming paths are completed. - Enhanced Task Flexibility: Now, it supports multiple inputs and outputs per task, significantly expanding the complexity and richness of the processes you can model. - **Meta-Agent Iris** Improvements - Enhanced LLM Selection Experience: When creating or editing an agent with Iris, users now benefit from a refined LLM selection flow, improving usability and model configuration accuracy. - **Flows** - **File support for Teams & Slack**: You can now easily send documents, images, audio, and video files through Teams and Slack when you integrate a Flow into these conversational channels. - **Evaluation Module** Enhancements - New Metrics Introduced: - **Faithfulness**: Assesses how factually consistent a response is with the retrieved context. - **Hallucination**: Calculated as 1 - Faithfulness, indicating the level of fabricated information. - **Context Precision**: Measures the proportion of relevant information within the retrieved contexts, compared against a reference answer for a given user input. (Note: Current calculation does not yet consider the position of retrieved chunks.) - **Noise Sensitivity**: This would involve analyzing the relationship between **Assistant Accuracy** and **Context Precision** across successive runs of an evaluation plan, varying the number of chunks retrieved. It examines how much and in what way the quality of the generated response changes when irrelevant content is added to the retrieved context. ### [Work in progress](#Work+in+progress) This section lists features and improvements still in development, with no confirmed release date. - [Prompt Files](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?946,,) option: It allows you to upload files at the organization and project level so that the Chat Assistant you define can use them to answer questions.\n\n- New assistant configuration to generate images.\n- Options to export/import agents, processes and flows.\n- Evaluation module backoffice.\n- Workspace/Playground - Support for real-time audio. - Share chat option.\n\n# Coming Next Explore the key innovation points in this roadmap, which reveal the improvements and breakthroughs on the way. ### [Features and updates for the upcoming release](#Features+and+updates+for+the+upcoming+release) - Support for **Model Context Protocol (MCP)** to integrate external tools. - The **GEAI proxy** is a Python-based component that enables dynamic integration of external tools into **Globant Enterprise AI (GEAI)** via **MCP**. It acts as a bridge between GEAI and one or more MCP-compliant tool servers. - Once the MCP servers are properly configured and connected through the GEAI proxy, the tools they expose become automatically available in the **Lab \u2192 Tools** section of GEAI, ready for use by any Agent without additional setup. - See more information about this protocol [https://modelcontextprotocol.io/introduction](https://modelcontextprotocol.io/introduction) - See how to [import tools into GEAI using MCP tool servers](1179.html) - New [/responses](1180.html) Endpoint for AI Interactions - We\u2019ve introduced a new /responses endpoint in Globant Enterprise AI (GEAI), which is fully compatible with the OpenAI Responses API. This addition allows developers to submit prompts as plain text, invoke functions, or pass files such as PDFs and images. The endpoint simplifies AI integration by supporting a familiar request/response structure, enabling a smoother transition for teams already using OpenAI-based workflows. - New [Images API](1183.html) - A new API is available that lets you generate images from text prompts. Supported providers: OpenAI, Vertex AI and xAI. - LLMs: - New Gemini models: - **Gemini 2.5 Pro Preview 'I/O edition'**: Built on its predecessor with significantly enhanced coding abilities and improved reasoning for complex tasks. Designed for developers and advanced users, this edition refines performance across benchmarks and expands its problem-solving reach. Release date: May 6th, 2025. - **Gemini 2.5 Flash**: Google's latest model built for complex problem-solving. It allows users to activate thinking and set a thinking budget (1\u201324k tokens). Designed to balance reasoning and speed, it delivers better performance and accuracy by reasoning before responding. - Updates in OpenAI's \"o\" series: - **o3**: The most powerful reasoning model in the \"o\" family; it pushes the frontier across coding, math, science, visual perception, and more. - **o4-mini**: A smaller model optimized for fast, cost-efficient reasoning; it achieves remarkable performance for its size and cost, particularly in math, coding, and visual tasks. - **o1-pro**: Available through our Responses API, offering a faster, more flexible, and easier way to create agentic experiences. - Over the next few weeks, the o1\u2011preview model will be migrated to the new o3 model, while o1\u2011mini will move to o4\u2011mini. More info in [Deprecated Models](843.html). - Refer to the [LLMs with Reasoning Capabilities](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?1168,,) article for step-by-step guidance on how to use reasoning-enabled models through the API. - The new **GPT-4.1** model series by OpenAI is now available in the production environment, featuring significant improvements in coding, instruction following, and long-context handling\u2014along with their first-ever nano model. - **Grok 3** Model Family added, including two pairs of models: - Lightweight Variants: - *grok-3-mini-beta* and *grok-3-mini-fast-beta* support function calling and enhanced reasoning (with configurable effort levels) for tasks like meeting scheduling and basic customer support. Both variants deliver identical response quality; the difference lies in response latency, with the \"fast\" version optimized for quicker responses. - Flagship Variants: - *grok-3-beta* and *grok-3-fast-beta* are designed for enterprise use cases such as data extraction, coding, and text summarization. They bring deep domain expertise in fields like finance, healthcare, law, and science. Similar to the mini variants, these models have identical capabilities, with the \"fast\" version offering reduced response times at a higher cost. - **Llama 4 collection** by Meta: We continue to expand our coverage of this model family. Recently added Llama 4 Scout and Maverick through Vertex AI's serverless API. Also available in Beta: Llama 4 Maverick via Groq and SambaNova, and Llama 4 Scout through the Cerebras provider, which offers this model with an inference speed of up to 2,600 tokens per second. - **Llama Nemotron Collection**: The Llama Nemotron Ultra and Super models are now available in Beta as Nvidia NIM microservices. These are advanced reasoning models, post-trained to optimize performance on tasks such as RAG, tool calling, and alignment with human chat preferences. Both models support a context window of up to 128K tokens. - Introducing the **OpenRouter** Provider (Beta): - OpenRouter joins the GEAI model suite with its [Auto Router](https://openrouter.ai/openrouter/auto) meta-model, which analyzes each user query and dynamically routes it to the most suitable LLM. This workflow maximizes response quality while minimizing cost and latency, delivering the most efficient output possible. - **Qwen3 Family** recently added: The latest generation in the Qwen large language model series, features both dense and mixture-of-experts (MoE) architectures to excel in reasoning, multilingual support, and advanced agent tasks. Its unique ability to switch seamlessly between a thinking mode for complex reasoning and a non-thinking mode for efficient dialogue ensures versatile, high-quality performance. Significantly outperforming prior models like QwQ and Qwen2.5, Qwen3 delivers superior mathematics, coding, commonsense reasoning, creative writing, and interactive dialogue capabilities. - Better processing of error messages, for example on those cases where the LLMs return specific errors. - New [Python SDK for Globant Enterprise AI (PyGEAI)](1148.html). It's composed of libraries, tools, code samples, and other documentation that allows developers to interact with the platform more easily with a 16K token context window. - New [omni-parser API](1165.html) to get the content of different file types. - RAG - Support for new audio and video [formats](137.html). - New endpoints to [reindex documents](29.html). - New parameters available when [ingesting documents](1142.html). - startPage and endPage to selectively process what is needed. - media parameters such as mediaPrompt, dialogue, frameSamplingRate and so on. - fix parameter *truncate* is not supported when calling the [cohere-rerank-3.5](625.html) model. - **The Lab** Enhancements - **Flows Integration**: The definition and management of [Flows](389.html) are now fully integrated into the Lab. - **Agentic Processes**: - New Conditional Gateway: Introduces the ability to define branching paths based on natural language prompts, enabling dynamic decision-making within processes. - New",
          "evaluationResultRowContextDocId": "ca90e485-3ba1-425a-a26d-70eb84540e78",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "705f6841-00ac-40f9-bad7-ecc77964f833",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?30",
          "evaluationResultRowContextName": "30",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# How to create a Chat with Data Assistant Here is a step-by-step guide on how to use [Chat with Data Assistant](159.html) and start chatting with the assistant from the [Playground Interface](349.html). ## Step 1: Generate the metadata The first step in configuring the Chat with Data Assistant is to generate the metadata required for its operation. This metadata contains information about the database schema. That is, the list of all tables with their fields, the relationships between them, table and field descriptions, data types, etc. You must ensure that the table and field names are descriptive and representative so that the assistant can generate efficient queries using the metadata information. For example, a table named Table001 with fields named Field001, Field002, etc. will not provide a clear identification that it contains country information, even if this information is present in the table. If your table or field names are not descriptive enough, you can include additional table and field descriptions in the assistant prompt. However, these descriptions must be equally clear and representative to ensure proper understanding of the information. You will have two options available to generate the necessary metadata, depending on whether the database was created with GeneXus or is external. Read more at [How to generate metadata for the Chat with Data Assistant](178.html). ### Database created with GeneXus If the database was created with [GeneXus](https://wiki.genexus.com/commwiki/wiki?1756,Category:GeneXus), this process is carried out using the [GXquery](https://wiki.genexus.com/commwiki/wiki?21971,Table+of+contents:GXquery+4.0,) extension, which generates the corresponding [Knowledge Base](https://wiki.genexus.com/commwiki/wiki?1836,Knowledge+Base) metadata. This extension is obtained directly from the GXquery installation directory. More information at [HowTo: Install the GXquery extension](https://wiki.genexus.com/commwiki/wiki?24482,HowTo:+Installing+the+GXquery+extension). For more information on how to create the metadata, please visit the following links: [HowTo: Create GXquery metadata in GeneXus](https://wiki.genexus.com/commwiki/wiki?17905,HowTo:+Create+a+GXquery+metadata+in+GeneXus), [HowTo: Export metadata in GXquery](https://wiki.genexus.com/commwiki/wiki?24012,HowTo:+Exporting+a+metadata+in+GXquery). ### External database In the case of an external database, you can use a command line tool to generate the necessary metadata. This tool uses reverse engineering to connect to the database and generate the corresponding metadata. When running this tool without parameters, a list of the options that can be configured is displayed. Once the metadata is generated, it is saved as a file with .export extension. ## Step 2: Create Chat with Data Assistant Enter the [Globant Enterprise AI Backoffice](42.html). On the left side of the screen, you can find the Backoffice menu. In this menu, click on Assistants, and then click on CREATE CHAT WITH DATA ASSISTANT. Next, in the Project dynamic combo box, select the project you want to work with (in this case, Default(DocumTeam) is used). ![Image](./images/922.png) In the window that is displayed, select the Add files option by clicking on the + Add files... button and load the **.export** file obtained in the previous step. Then click on SAVE to complete the process. ![Image](./images/161.png) This will open a popup window to enter the Name and Description of the assistant. You can also add a file for Icon by clicking on +Add files... After entering the required information, click on CONFIRM to save all changes. ![Image](./images/162.png) ## Step 3: Update Chat with Data Assistant Once you have created the Assistant, go back to the Assistants page where you can select UPDATE. ![Image](./images/923.png) This option allows you to view the version identifier with which it was saved. In addition, you can change the name and description, configure it as enabled or disabled, or add an icon if you wish. ![Image](./images/164.png) ## Step 4: Edit Chat with Data Assistant To edit the Assistants, go to the Assistants home page and click on CONFIG. ![Image](./images/924.png) A window similar to the one below will open, where you will have the option to make the following four edits: - Add metadata files, start or stop the assistant, and get the Log. This functionality is available in the General tab.\n- Enable or disable entities and attributes, set descriptions, and define the possible values of an attribute in the Entities tab.\n- Configure the database connection for the assistant in the DataStore tab.\n- Configure the metadata to be included in the queries and select the versions of the system assistants in the Settings tab. ![Image](./images/166.png) ### General In the General window, you will find the following information about the assistant: ![Image](./images/167.png) - **Assistant Name:** Displays the name of the assistant, which in this case is \u201cShowCase\u201d.\n- **Metadata ID:** Displays the unique ID for the metadata associated with the assistant.\n- **Metadata status:** Indicates whether the assistant's metadata is \u201cStopped\u201d or \u201cStarted\u201d. To interact with the assistant's metadata, you can use the following buttons: - **+ Add files...:** Allows adding the .export file containing the assistant's metadata.\n- **STOP:** Changes the assistant's metadata status from \u201cStarted\u201d to \u201cStopped\u201d, disabling the assistant. This can be useful when you need to prevent users from continuing to make queries using the assistant while you perform a task that will cause these queries to fail; for example, if you need to take the database offline. When you have finished, you can restart the assistant using the START button.\n- **IMPORT LOG:** Allows you to download a log file (.log) containing detailed information about various actions in the system when importing metadata. This file records events such as inserting metadata, creating Knowledge Bases, generating the design system, and importing objects, among others. ### Entities In the Entities tab, you can view all the entities that are part of the assistant. In this case, the visible entities are City, Continent, Country, CountryCovidData, PivotTableMetadata, and State. Each entity is listed with its name, a description, and whether or not it is enabled for queries. ![Image](./images/466.png) For each entity, you can: - **Edit the description**: this makes it easier for the assistant to better understand what the entity is about.\n\n- **Enable or disable** the entity to control whether the entity will be used in queries. By clicking on the **+** symbol to the left of each entity, the attributes of the entity are displayed and the sign changes to **-**. For example, for the City entity, the visible attributes include CityId, CityLocation, CityName, and CityPopulation, among others. For each attribute, you can perform the following actions: ![Image](./images/467.png) - Edit the attribute description.\n- Enable or disable the attribute for queries.\n- Define the possible values that the attribute can take, if applicable. To do this, click on **EDIT VALUES.** This opens a window where you can define a list of valid values and their descriptions for the corresponding attribute. This list must be entered as a comma-separated sequence, using the following format: `value1=description1, value2=description2, ....` Once you have made the necessary changes to the entities and attributes, you must click on the **CONFIRM** button to save and apply them. ### Datastore In the DataStore tab, the first thing to do is to select the type of database (DBMS) you will use. The values that the DBMS can take are SQL Server, MySQL, PostgreSQL, Oracle, and SAP Hana. It is important to note that the connection type is made through **ADO.NET**, which means that the corresponding libraries must be available to establish the connection. #### Prerequisites 1. **Database Accessibility:** The database must be accessible from Globant Enterprise AI, which implies that both must be on the same network or that the database is accessible via the Internet. 2. **ADO.NET Drivers Installation**: If they are not installed, you will need to install the ADO.NET drivers corresponding to the type of database you are using. The drivers allow GeneXus to connect efficiently with the selected DBMS. #### Configuration 1. **Connection Parameters**: Configure the necessary connection parameters, including: - **Server**: Name or IP of the server. - **Database**: Name of the database you wish to connect to. - **Port**: Database port number (depending on the DBMS, such as 1433 for SQL Server). - **Authentication**: Depending on the DBMS, you can require a username and password or use a built-in authentication. You can set additional options depending on the type of database selected. 2. **Test Connection button**: At the end of the configuration of each DBMS option, you will find a button called \u201cTest Connection\u201d that will allow you to verify the connection before saving the changes. Once you have configured the connection correctly, you can use the assistant to interact with the data stored in the selected database. > **Note**: This process does not require additional installations for Globant Enterprise AI outside of the ADO.NET drivers. The interface is designed to guide you through a simple connection setup. ### Settings In the **Settings** tab, you will find two sections: ![Image](./images/468.png) - Metadata to include in prompt\n- System assistants In the **Metadata to include in prompt** section, you can select which metadata will be included in the assistant prompts when generating queries. This option allows you to check the boxes for **Descriptions**, **Data types**, and **Valid values**. These options are useful if the field names are not descriptive or clear enough, so adding more metadata will help the assistant generate more understandable and accurate queries. By checking these boxes, descriptions, data types, and valid attribute values will be included in the assistant's queries, making it easier to interpret and use the information. In addition, in the **System assistants** section, you can select the versions of the system assistants to be used by the assistant by setting the **Query** parameter. Once you have configured these parameters, remember to click on **CONFIRM** to save the changes. ## Step 5: Test your Chat with Data Assistant Finally, you can test your Chat with Data Assistant via [Chat API](34.html) or by selecting Playground in the left side menu of the Backoffice window: ![Image](./images/916.png) If you have multiple assistants created, select the one you want to interact with. Once there, type the following message in the chat box: *Display the number of patients by continent and country in a pie chart, showing the most representative ones* and press Send. The result of the query will look as follows: ![Image](./images/629.png) The pie chart displays the number of hospitalized patients by continent and country, sorted in descending order by the number of patients. For additional details on how to customize the output format of these queries refer to [How to change the output format and filter results in Chat with Data Assistant](181.html). ### Icons to interact with the query At the bottom right of the screen, there are six icons that allow you to interact with the query and access additional information about the data being viewed. 1. **View query detail**: This icon, in the shape of an eye, allows you to analyze the query that the assistant has just returned. Clicking on this icon displays a pop-up window showing the following two tabs: - **Description:** This tab shows an explanation in natural language that describes the purpose and operation of the query in a more accessible way, helping you to understand where the data displayed in the results comes from. ![Image](./images/630.png) - **Expression:** This tab shows the structure of the executed query which shows how the query was constructed, what elements were used, and how the data was processed. ![Image](./images/632.png) 2. **Copy assistant response**: The second icon allows you to copy the current query in JSON format. This format includes details such as the name of the query, the expression used, and other parameters related to the execution. 3. **Save query**: The third icon opens a window to save the current query as a preset, allowing you to save the configuration and refer to it again in the future. 4. **Regenerate answer**: The fourth icon allows you to update the query by running it again. 5. **Good answer**: This icon lets you mark the query's response as satisfactory. 6. **Bad answer**: This icon allows you to mark the query response as inaccurate or unsatisfactory.",
          "evaluationResultRowContextDocId": "c09d2f5b-11eb-4ed9-934b-8ab39f7fc327",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "af0db8b7-9f17-4e54-87a7-a084bc8e87d7",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?170",
          "evaluationResultRowContextName": "170",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.6,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "5dde8f38-6857-47c5-988b-e571bb8e9b89",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0038276200",
      "evaluationResultRowEndDate": "2025-05-13T11:14:37",
      "evaluationResultRowOutput": "Puedes utilizar el soporte de archivos por asistentes en Globant Enterprise AI siguiendo estos pasos generales, que te permiten interactuar con documentos, im\u00e1genes, videos y archivos de audio para tareas como resumen, extracci\u00f3n de informaci\u00f3n, comparaci\u00f3n y generaci\u00f3n de texto basado en el contenido del archivo:\n\n1. **Elegir el asistente adecuado**: Selecciona el asistente que mejor se adapte a tus necesidades para trabajar con archivos.\n\n2. **Subir tu archivo**: Haz clic en el bot\u00f3n \u201cUpload File\u201d dentro del asistente y selecciona el archivo que quieres usar. El asistente reconoce el tipo de archivo y lo procesa adecuadamente.\n\n3. **Escribir el prompt**: En el cuadro de texto para el prompt, ingresa las instrucciones o la consulta que deseas hacer, siendo claro y espec\u00edfico.\n\n4. **Referenciar archivos en el prompt**: Usa la sintaxis especial para hacer referencia a los archivos subidos y que el asistente pueda utilizarlos dentro del prompt:\n\n   - Documentos: `{file:filename}` (por ejemplo, `{file:contract}`)\n   - Im\u00e1genes: `{file:image1}`\n   - Videos: `{file:video1}`\n   - Audio: `{file:audio1}`\n\n5. **Enviar el prompt**: Env\u00eda tu solicitud al asistente para que la procese.\n\n6. **Revisar resultados**: El asistente procesar\u00e1 la informaci\u00f3n del archivo mencionado y te devolver\u00e1 la respuesta o resultado correspondiente.\n\n### Ejemplos de uso:\n- Para resumir un documento:  \n  _Prompt_: \u201cResume los puntos clave del documento `{file:contract}`\u201d.  \n  _Respuesta_: \u201cEl contrato establece los t\u00e9rminos de servicio para una suscripci\u00f3n de software, incluyendo precios, plazos de pago y opciones de soporte\u201d.\n\n- Para describir una imagen:  \n  _Prompt_: \u201cDescribe la imagen `{file:product_image}`\u201d.  \n  _Respuesta_: \u201cLa imagen muestra un coche deportivo rojo con interior de cuero negro\u201d.\n\n- Para transcribir un archivo de audio:  \n  _Prompt_: \u201cTranscribe el archivo de audio `{file:meeting_recording}`\u201d.  \n  _Respuesta_: El asistente proporciona la transcripci\u00f3n del audio.\n\nDe esta forma puedes aprovechar el soporte de archivos para mejorar la interacci\u00f3n y capacidades de los asistentes utilizando directamente el contenido de los archivos sin necesidad de procesarlos manualmente [3](https://wiki.genexus.com/enterprise-ai/wiki?346).",
      "evaluationResultRowStartDate": "2025-05-13T11:14:28",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "8235"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#RAG File Formats By default the following upload file formats are supported: | **Category** | **Extension** |\n|---|---|\n|Text|`.txt`|\n|Portable|`.pdf`<sup>2</sup>, `.md`, `.msg`<sup>2</sup>, `.org`<sup>2</sup>, `.rst`<sup>2</sup>, `.csv`<sup>1</sup>, `.tsv`|\n|General|`.eml`<sup>2</sup>, `.html`<sup>2</sup>, `.xml`|\n|Microsoft Office|`.doc`<sup>2</sup>, `.docx`<sup>2</sup>, `.ppt`<sup>2</sup>, `.pptx`<sup>2</sup>, `.xls`, `.xlsx`, `.rtf`|\n|Open Document|`.odt`<sup>2</sup>, `.odp`<sup>2</sup>, `.ods`<sup>2</sup>|\n|Ebook|`.epub`<sup>2</sup>|\n|JSON|`.json`, `.jsonl`|\n|Images|`.png`, `.jpeg/.jpg`, `.tiff`, `.bmp`, `.gif`|\n|Audio<sup>3</sup>|`.mp3`, `.ogg`, `.wav`, `.flac`, `.aac`, `.m4a`|\n|Video<sup>3</sup>|`.mp4`, `.avi`, `.mov`, `.mkv`|\n|Custom|`.custom`, `.web`| <sup>1</sup> - When using the [[581|legacy provider]], the separator must be a comma (`,`) and not a semicolon (`;`). <sup>2</sup> - supports the `pageNumber` element when using the [[581|GEAI provider]]. <sup>3</sup> - Supported since `2025-05` release. Take into account that simple files like `.csv`, `.txt` are expected to have **utf-8** encoding. For [[441]], you can extend the supported file extensions by changing the `FILE_TYPES` parameter. ## Custom Formats - [[201|.custom]]: use it to manually configure your desired chunks and metadata.\n- [[202|.web]]: use it to crawl a web site. ## Changelog ### Revision #8 * _New extensions_: `.mp4`, `.avi`, `.mov`, `.mkv`, `.mp3`, `.wav`, `.ogg`, `.aac`, `.flac`, `.m4a`, `.wma`. ### Revision #5 * _New extensions_: `doc`, `ppt`, `xls`, `msg`, `org`, `rtf`, `rst`, `tsv`, `eml`, `tiff`, `bmp`, `epub`.\n* _new options_ when processing `csv`, `xls*`. ## On-Premises Check the System parameters as detailed for the RAG module: | **Revision** | **Parameter** | **Value** |\n|---|---|---|\n| 2 | `FILE_TYPES` | `txt,pdf,docx,pptx,xlsx,odt,odp,ods,xlsx,epub,json,jsonl,csv,java,cs,py,js,ts,xml,html,web,custom,md` |\n| 5 | `FILE_TYPES` | `txt,pdf,doc,docx,ppt,pptx,xls,xlsx,odt,odp,ods,rtf,rst,epub,json,jsonl,csv,tsv,java,cs,py,js,ts,web,custom,md,png,gif,jpeg,jpg,mp3,mp4,msg,org,tiff,bmp,eml,xml,html` |\n| 8 | `FILE_TYPES` | `txt,pdf,doc,docx,ppt,pptx,xls,xlsx,odt,odp,ods,rtf,rst,epub,json,jsonl,csv,tsv,java,cs,py,js,ts,web,custom,md,png,gif,jpeg,jpg,mp3,m4a,mkv,aac,ogg,flac,wav,mp4,avi,mov,msg,org,tiff,bmp,eml,xml,html` | ## See Also [[581]] {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "84a8ad9b-723e-45dd-99c1-7383a327b4c3",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "0e7052db-35e5-49a6-ac09-5495baacf431",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?137",
          "evaluationResultRowContextName": "137",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "- New Average Request Time metric added in the Project Dashboard. ### January 8th - The option formerly known as 'Search Documents' has been improved and renamed to RAG Assistant (Retrieval Augmented Generation) to provide an optimized experience when searching and generating information. #### Frontend - Feedback is provided during conversations with RAG Assistants, indicating where you are in the process.\n- 'Response streaming' support for RAG Assistants.\n- Settings are hidden when selecting an assistant, except when 'Chat with LLMs' is selected. ## 2023 ### December 19th - Fixed: Too Many Redirects when accessing Playground using a browser in Spanish language. ### December 6th - New backoffice design.\n- Access to the Playground from the backoffice to chat with the assistants defined in the project.\n- Upload images for analysis with GPT-4 Vision.\n- Google Analytics support at the frontend.\n- Keep a conversation thread when chatting with documents.\n- An email notification is sent automatically when a new member is invited to join the organization or project. ### November 28th - First version officially released!! ### November 6th - The following OpenAI models are supported: GTP-4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), and GPT-4 Vision (gpt-4-vision-preview). ### October 18th - [Manage quotas per project](71.html): It is possible to set quota limits to control project expenses or usage. ### October 11th - [AI-Driven Load Balancing](32.html): The platform automatically manages the Load Balancing process when you work with generative AI providers, efficiently addressing the limits imposed by LLM platforms.\n\n- In the [Organization API](22.html), the ability to set and manage usage limits on projects through the [POST /project](22.html) and [GET /project/{id}](22.html) endpoints has been added.\n- [Quota Limit](71.html) now includes improvements such as highlighting the active quota in green, offering options to cancel active quotas, among others. ### October 17th - Rebranding to Globant Enteprise AI\n- Improvements in RAG - New option not to [compact the conversation history](297.html) - Support for Redis, MongoDB Atlas and PG Vector [vectorstores](267.html) - New endpoint to get an embedding. Check details here: [Embeddings API](422.html)\n- Playground improvements\n- File management improvements\n- New [LLMs supported](200.html) - NVIDIA provider with new models supported - nvidia.nemotron-mini-4b-instruct - meta.llama-3.1-8b-instruct - meta.llama-3.1-70b-instruct - meta.llama-3.1-405b-instruct - meta.llama-3.2-3b-instruct - Groq provider supported - groq/llama-3.1-70b-versatile - groq/llama-3.2-11b-vision-preview - groq/llama-3.2-3b-previewgroq/llama-3.2-1b-preview\n- New [embeddings models](447.html) added - Vertex AI: - vertex_ai/textembedding-gecko - vertex_ai/text-embedding-004 - vertex_ai/textembedding-gecko-multilingual - Nvidia: - nvidia/nvclip - nvidia/nv-embed-v1 - nvidia/baai.bge-m3 - nvidia/snowflake.arctic-embed-l - nvidia/nv-embedqa-mistral-7b-v2 - nvidia/embed-qa-4 - nvidia/nv-embedqa-e5-v5 ### September 25th - [Support for file](346.html) processing with prompt-based assistants. This will enable many scenarios, such as uploading documents and summarizing, extracting, and checking information, etc. Also, depending on the model used by the assistant, it will be able to process audio, video, or images.\n- [Support for multi-modal](345.html) LLMs allow processing docs, audio, video, and images in models like GPT-4o or Gemini Pro.\n- Chat with data assistants - The model used to build the queries was updated with GPT-4o, which improves the quality of the generated query. - Configure the query builder server by organization and project. This means you can connect with different DBMS from each project when building Chat with data assistants. - Show an explanation of how the query was built.\n- New [Playground Interface](349.html) design - New design - Upload documents from the front end to chat with them.\n- [Flows builder](321.html) - There will be two types of Flows, one more oriented to build a conversational UI and the other to build assistant flows. Access to these flows will only be available through [Chat API](34.html) or through the [channels](384.html) offered by Flows.\n- New models hosted in AWS Bedrock added: - Amazon Titan Express v1 - Amazon Titan Lite v1 - Anthropic Claude 3 Haiku - Anthropic Claude 3 Sonnet - Anthropic Claude 3.5 Sonnet - Cohere Command - Meta Llama 3 8B - Meta LLama 3 70B\n- It is now possible to provide clear guidance on the assistant's capabilities, allowing you to add information such as descriptions, features, and example prompts. This configuration can be done from the Backoffice, [Start Page](408.html), or WelcomeData section of the [Assistant API](35.html) and [RAG Assistants API](29.html) endpoints.\n- RAG Assistants - New endpoint in [RAG Assistants API](29.html): using [DELETE/profile/{name}/documents](29.html), all documents associated with the specified RAG Assistant are deleted. ### August 9th - Support of new models - GPT-4o mini\n- RAG Assistants - New option called [CLEANUP](60.html) allows to delete the documents associated to a RAG Assistant. - When creating a new assistant, the following defaults are updated: - [Chunk Count](69.html) updated from 2 to 5. - [Model Name](68.html) from gpt-3.5-turbo to gpt-4o-mini.\n- Data Analyst Assistant - New [Conversational Module](291.html) for Data Analyst Assistant.\n- Considerations - **Important**: [max_tokens integer below minimum value](290.html) error may occur if you are using the _max_tokens_ value as -1; those Assistants must be updated to use a correct positive value. ### July 4th - Permissions by assistants in role-based access control. A new [role and permissions management system](242.html) has been implemented to provide greater flexibility and control. Please read [Migration to the new Roles and Permissions Management System](264.html).\n- Dashboard performance optimizations. Please read [Using the Request Summary Table for Improved Dashboard Performance](273.html).\n- RAG Improvements - New option to customize the [context](205.html). - Support for [Max Marginal Relevance](206.html) - Support for _similarity_hybrid_ and _semantic_hybrid_ for [Azure AISearch](207.html) - Advanced options for SelfQuery retriever prompt case. See [Self Query Use Case](150.html) - Support for Parent Document retrieval option. See [Parent Document Retrieval Strategy](204.html) ### June 10th - [Enterprise AI Proxy](19.html) is deprecated. Use [Chat API](34.html) instead.\n- Support for new LLMs - OpenAI new model GPT-4o - Models in Google Vertex - Gemini 1.0 Pro - Gemini 1.5 Flash preview-0514 - Gemini 1.5 Pro preview-0514 - Claude 3 Haiku - Claude 3 Opus - Claude 3 Sonnet\n- RAG Improvements - New option to initialize RAG Assistant based on another when creating a new RAG Assistant. - New option to export document list in View Documents over a RAG Assistant. - Added filter options when browsing Documents. - SelfQuery RAG retriever partial support for a [customized Prompt](150.html). - Support for text-embedding-004 in Google models to generate the embeddings.\n- Deprecated [Assistant API](35.html) endpoints. - /assistant/text/begin - /assistant/text\n- Support to deploy in Google Cloud Platform. ### May 8th - New [Chat with Data Assistant](159.html).\n- New [Ingestion SDK](172.html) to automate document ingestion in RAG assistants.\n- New models hosted in NVIDIA platform supported. See [Supported Chat Models](200.html) for more details.\n- New option to export information about projects and members available for the organization administrator.\n- New API to extend dataset for [Data Analyst Assistant 1.0](89.html).\n- New filter by user email in Requests.\n- Update default to use _text-embedding-3-small_ OpenAI Embeddings for new RAG assistants.\n- Support for gemini-1.5-pro-preview-0409 model added. ### April 3rd - New [API Assistant](110.html).\n- New [Data Analyst Assistant 1.0](89.html).\n- The user Dashboard displays the email to identify the most active user or the one who generates the most costs. ### March 11th - GeneXus Identity Provider is implemented, expanding the login options in the [Backoffice](42.html) of the production environment. This allows for login not only with Google but also with Apple or GeneXus Account.\n- It is possible to customize the icon for each assistant. ### February 29th - [Frontend](82.html) improvements in UI/UX.\n- Option to get feedback from end users when interacting with [RAG Assistant](44.html).\n- Gemini Pro LLM support.\n- New [Dashboard](562.html) with user metrics.\n\n# Updates Log This log shows the most important fixes or features added to the platform. ## Log ## 2025 ### April 10th - New [Globant Enterprise AI Lab](972.html) - The Lab is designed for defining, managing, and orchestrating autonomous AI agents. It provides a standardized model for representing agents, their capabilities, and their interactions within complex workflows. The core components of the Lab include: - Agents & Tools: This module allows for the definition and management of individual agents and their resources, such as skills and tools. It serves as a central hub for cataloging and managing the agent workforce. - Agentic Processes: This component enables the definition of processes based on tasks executed by Agents. These workflows facilitate collaboration among agents to achieve larger objectives. More details at [How to create an Agentic Process](985.html). - Agent RunTime: This module provides the execution environment for agentic workflows, where agents perform tasks based on their skills and interact with artifacts, driven by events and the flow of knowledge. - The Lab aims to meet the growing demand for intelligent, self-sufficient AI agents capable of collaborating and solving complex problems with minimal human intervention. It offers a flexible and adaptable model, allowing for the creation and management of a diverse range of agents, from co-pilots working alongside humans to fully automated agents executing complex tasks. Implemented as a module of Globant Enterprise AI, the Lab supports the development of intelligent agents that can work autonomously or in collaboration with humans and other agents.\n- New features in Flows - Agent Integration Component: You can now directly integrate agents created with the AI Lab into a Flow. These agents can be exposed through platforms like WhatsApp, Teams, or Slack. - [File Upload Support](1053.html) from WhatsApp: Flows now support receiving file attachments such as documents, images, audio, and video directly from WhatsApp interactions. - Audio and Video Attachment in Web Chat: The Web Chat component now allows users to attach audio and video files, enhancing the interaction experience.\n- New Features for the Data Analyst Agent - Reduced Configuration Requirements: The setup needed to enable the assistant to respond to a wide range of questions has been minimized. - Enhanced Analysis Module: An additional analysis module has been incorporated to complement the responses with relevant business conclusions and interpretations of the obtained data.\n- New metrics to track processed tokens. - A new view has been added to each project dashboard to monitor token-based consumption. More details at [Tokens tab in Dashboard](965.html). - To enable this feature, you must run the initialization process described in [Reprocessing Requests and Loading the Summary Table](1015.html).\n- LLMs: - New Gemini 2.5 Pro (via providers Vertex AI and Gemini): Gemini 2.5 is Google\u2019s latest reasoning model, engineered to tackle increasingly complex challenges. This model is designed for tasks that demand advanced analytical thinking and robust problem-solving capabilities. More details at [LLM API](663.html). - Migration to Gemini 2.0 series (Vertex AI): Based on recommendations from Vertex AI, we have migrated from the legacy Gemini 1.0 and 1.5 models to the more advanced Gemini 2.0 series, offering improved performance, scalability, and integration capabilities. For comprehensive information, please refer to the [Deprecated Models](Deprecated Models). - New Azure OpenAI models (o1, o1-mini and o3-mini): We have expanded our model availability by introducing these models via Azure, providing the same high-quality capabilities as those offered through the OpenAI provider. - DeepSeek-R1 via AWS Bedrock: Recently added through a Serverless API, DeepSeek-R1 offers reliable inference with a substantial 128K token context window and up to 32K maximum output tokens. - OpenAI's new models with built-in web search tool: These specialized models integrate web search capabilities directly into the Chat Completions API, enabling them to both interpret and execute search queries in real time. - openai/gpt-4o-search-preview - openai/gpt-4o-mini-search-preview - New models - Beta only: - gemini/gemma-3-27b-it: Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 has a large, 128K context window, and multilingual support in over 140 languages. - SambaNova: - DeepSeek-R1: This Provider offers the fastest performance for running DeepSeek, processing up to 198 tokens per second per user, with a 16K token context window. The model is hosted in US data centers; privately and securely. - DeepSeek-V3-0324: This model significantly outperforms its predecessor with enhanced reasoning benchmarks, improved code executability and refined web aesthetics, and superior Chinese writing aligned with the R1 style. It also offers better multi-turn interactive rewriting, translation quality, detailed report analysis, and more accurate function calling. - QwQ-32B: SambaNova provides access to QwQ-32B-Preview, the best open source test-time compute model released by Alibaba. - Llama 4 collection by Meta: - Llama 4 Scout: A 17B-parameter multimodal MoE model with 16 experts that excels in text and image understanding. The model is currently in beta and is available via providers Nvidia, Groq and SambaNova. Via Groq, it supports a 128k tokens context window with fast inference at 460 tokens/sec, while SambaNova Cloud runs at 697 tokens/second/user. - Llama 4 Maverick: Available via Nvidia, this 17-billion-parameter model featuring 128 experts supports a 32k-token context window. ### March 10th - New LLMs: - GPT-4.5 - Claude 3.7 Sonnet (Providers Anthropic, Vertex AI and AWS Bedrock) - Updates in Gemini 2.0 series: - vertex_ai/gemini-2.0-flash-lite-preview-02-05 - vertex_ai/gemini-2.0-flash-thinking-exp-01-21\n- RAG Revision #6 - Support for o3-mini, gpt-4.5-preview, claude-3-7-sonnet-20250219, new DeepSeek, Gemini2* and sambanova LLM providers. - New pinecone provider available for [embeddings](447.html) and [rerankers](638.html). - The [CleanUp](60.html) action message has been corrected to clearly specify that it will permanently delete the RAG Assistant files and update the information in the RDS. - Added [usage](712.html) element on every response. - Improvements when changing the [LLM](69.html)/[Embeddings](68.html) settings; all models and providers are normalized to be selected from standard combo-box items; use the [override mechanism](209.html) if you need other options. - Support for [guardrails](840.html). - New [documentAggregation](298.html) property to decide how sources are grouped and returned.\n- It is possible to [provide feedback](170.html) on the response of the Chat with Data Assistant in the Frontend.\n\n- The new [Evaluation APIs](893.html) introduce key functionalities through three interconnected APIs: [DataSet API](839.html), [Evaluation Plan API](849.html), and [Evaluation Result API](856.html). This version is primarily designed for users with a data science profile and is mainly accessed via APIs, complemented by a series of Jupyter notebooks that demonstrate their use. For a comprehensive guide on how to use these APIs, you can refer to [How to evaluate an AI Assistant](881.html) and the [EvaluationAPITutorial.ipynb](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?887,,) notebook, which provide practical examples and code for working through the evaluation process.\n- File attachment support in Flows (version 0.9).\n- Support for Full Story integration in the Workspace/Playground to generate user access statistics in Full Story.\n- In the [LLM API](663.html), for models that have descriptions in the specified languages, the `descriptions` property is included in the Response, which contains the descriptions in the available languages, such as `Spanish`, `English` and `Japanese`.\n- [Data Analyst Assistant 2.0](886.html) version presents important improvements, simplifying the interaction with the data by reducing the main components to just two: **Dispatcher** and **Thinker**. In addition, the metadata structure is automatically generated when loading the datasets, streamlining the setup process. For more information, see [How to create a Data Analyst Assistant](105.html).\n- The option to consult version-specific documentation is now available. Articles with versions show the option \u201cOther document versions\u201d in the header. Clicking on \u201cOther document versions\u201d brings up a menu that allows you to choose between the most recent version (\u201cLatest\u201d) or earlier versions (e.g. \u201c2025-02 or prior\u201d). If you select a version other than \u201cLatest\u201d, a message appears: \u201cThis is not the latest version of this document; to access the latest version, click here\u201d. This message provides a direct link to the most up-to-date documentation. [Components Version Update](822.html) ### February 10th - New documentation with details about [Supported Chart Types](731.html).\n- New [Usage Limits API](802.html).\n- Flows - Logs associated with conversations are accessible from the [conversation history](378.html).\n- RAG Revision #5 - Ingestion supports new [file formats](137.html). For on-premises installations, update the _FILE_TYPES_ parameter. - New optional _cleanProcessing_ parameter when using [Parent Document Retrieval Strategy](204.html). - [Embeddings API](422.html) cache support; disabled by default from a [RAG assistant](68.html). - New [mode](68.html) property for using multimodal embeddings and creating a [Multimodal RAG](783.html).\n- New endpoint [GET /accessControl/apitoken/validate](22.html) returns information about the organization and project associated with the provided apitoken.\n- New LLMs: - Already in production - **OpenAI**: o3-mini - Already in Beta - **DeepSeek**: - deepseek/deepseek-reasoner - deepseek/deepseek-chat - azure/deepseek-r1 - nvidia/deepseek-ai-deepseek-r1 - groq/deepseek-r1-distill-llama-70b - sambanova/DeepSeek-R1-Distill-Llama-70B - Updates in Gemini 2.0 series: - gemini-2.0-flash-thinking-exp-01-21 (Via Providers Gemini and Vertex AI) - gemini/gemini-2.0-flash-lite-preview - gemini/gemini-2.0-pro-exp - vertex_ai/gemini-2.0-flash-001 - sambanova/Llama-3.1-Tulu-3-405B [Components Version Update](822.html) ### January 13th - Internationalization, [Backoffice](42.html), and frontend support for Japanese.\n- Invitations now include information about the organization and project in the subject.\n- New LLMs - Already in Production - **OpenAI**: o1 (2024-12-17 version) - Already in Beta - New Providers: **Cohere** - **Cohere**: Cohere-r\n- [Guardrails](682.html) configured by assistant.\n- [Rerank API](625.html) to semantically order a list of document chunks given a query.\n- New optional RAG [Retrieve and Rerank](620.html) adds an extra layer of precision to ensure that only the most relevant information reaches the model used in the generation step. ## 2024 ### December 12th - [Automatic Creation of Default Assistant](640.html)\n- [Organization Usage Limits](666.html): It is possible to set quota limits to control organization expenses or usage.\n- Chat with Data Assistant - [Show details](170.html) about the generated query in the Playground. - Support in [Chat API](34.html) to interact with Chat with Data Assistant.\n- Flows - Support for markdown when showing the response on the different channels supported by Flows (web, Slack, WhatsApp, and Teams). - New component for connecting flows to the agent overflow console (Human-in-the-loop) via B2Chat. Please read [How to connect a Flow to B2Chat](674.html).\n- RAG - [Chroma](537.html), [Elasticsearch](579.html), [OpenSearch](580.html) vectorstore support. - [Improvements](581.html) when processing PDF documents (update to RAG module revision #4 or higher). - Integration with [AWS Knowledge Bases](577.html) at query time.\n- Data Analyst Assistant - Option to update metadata options. - New version by default in new Data Analyst assistants.\n- New LLMs - OpenAI: gpt-4o-2024-11-20 - AWS Bedrock: Anthropic Claude 3.5 Haiku - Amazon Nova models (Micro, Lite, and Pro) - Llama 3.1 405B on Vertex AI - Beta: - Support for providers Cerebras, SambaNova and xAI (Grok models). - All new Gemini Experimental models.\n- Security - It is possible to define a user 'all-members@domain.com' to give access to the Frontend to all users of the domain '@domain.com'. Permissions are enforced based on the roles assigned to this user. Please read [Allowing Frontend Access for Users from Specific Domains](657.html) ### November 12th - Flows execution integrated into the Playground\n- New [LLMs support](200.html) - **OpenAI**: o1-preview and o1-mini - **Claude Sonnet 3.5 v2 - Providers**: Anthropic, Vertex AI, and AWS Bedrock - **Llama 3.2 models - Providers**: Vertex AI and AWS Bedrock\n- Chat with data assistants - Possibility to edit metadata, [entities](170.html), and attribute descriptions. - The Properties tab has been renamed to [Settings](170.html) along with the options that can be configured in it.\n- RAG - New [returnSourceDocuments](298.html) option to disable returning the documents section used to answer the question. - New [step](298.html) option to use the assistant as a retrieval tool. - Support for [custom history](85.html) in conversations using the _chat_history_ variable.\n- Stand-alone Frontend based on the new Playground UI - Options to customize the Frontend to use the client logo, color palette, welcome message, and descriptions. - Feature to collect feedback (thumbs up/down) in each response. - Google Analytics support.\n- Data Analyst Assistant - Support to upload large CSV files.",
          "evaluationResultRowContextDocId": "68ace6a6-8b1b-4736-a466-9ed0a67b97a9",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "d1ef2eae-be0e-43a9-b3c7-b37592cc2e2f",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?31",
          "evaluationResultRowContextName": "31",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Supported models and file formats This table provides a detailed overview of the multimodal models supported by Globant Enterprise AI and their compatibility with different file formats such as `pdf`<sup>(1)</sup>, `txt`, `png`, `jpg`, `jpeg`, `gif`, `mp3` and `mp4`. The size of the supported files will depend on the context window of each model. <div style=\"background-color: rgb(242,242,242);border-left: 6.0px solid rgb(128,128,128);margin-bottom: 15.0px;padding: 4.0px 12.0px;\"><b>Note</b>:All non-multimodal models (not included in the table) accept text (`txt`) and PDF (`pdf`) files.</div> | **Provider** | **Model** | **Supported formats** | **Context window (tokens)**|\n|---|---|---|---|\n| anthropic | claude-3-5-sonnet-20241022 | txt, pdf, jpeg, jpg, png, gif | 200000 |\n| anthropic | claude-3-haiku-20240307 | txt, pdf, png, gif | 200000 |\n| anthropic | claude-3-opus-20240229 | txt, pdf, png, gif | 200000 |\n| anthropic | claude-3-5-sonnet-20240620 | txt, pdf, png, gif | 200000 |\n| anthropic | claude-3-7-sonnet-latest | txt, pdf, png, gif, jpeg | 200000 |\n| awsbedrock | anthropic.claude-3-opus | txt, pdf | 200000 |\n| awsbedrock | meta.llama3-2-11b | txt, pdf | 128000 |\n| awsbedrock | meta.llama3-2-90b | txt, pdf | 128000 |\n| awsbedrock | anthropic.claude-3-7-sonnet | txt, pdf, png, gif, jpeg | 200000 |\n| awsbedrock | anthropic.claude-3.5-sonnet-v2 | txt, pdf, jpeg, jpg, png, gif | 200000 |\n| awsbedrock | anthropic.claude-3-haiku | txt, pdf, png, gif | 200000 |\n| awsbedrock | anthropic.claude-3.5-sonnet | txt, pdf, png, gif | 200000 |\n| awsbedrock | amazon.nova-lite-v1:0 | txt, pdf, png, gif | 300000 |\n| awsbedrock | amazon.nova-pro-v1:0 | txt, pdf, png, gif | 300000 |\n| azure | gpt-4o | txt, pdf, gif, jpg, png | 128000 |\n| azure | gpt-4o-mini | txt, pdf, gif, jpg, png | 128000 |\n| azure | o1 | txt, pdf, jpg, png, gif | 200000 |\n| gemini | gemini-2.0-flash-thinking-exp-01-21 | txt, pdf, png, mp3 | 1048576 |\n| gemini | gemini-1.5-pro-latest | txt, pdf, jpg | 2097152 |\n| gemini | gemini-exp-1206 | txt, pdf, jpg | 2097152 |\n| gemini | gemini-2.0-pro-exp | txt, pdf, jpg | 2097152 |\n| gemini | gemini-1.5-flash-8b-exp-0827 | txt, pdf, jpg, png, gif, mp3 | 1048576 |\n| gemini | gemini-1.5-flash-latest | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| gemini | gemini-2.0-flash-exp | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| gemini | gemini-2.0-flash-lite-preview | txt, pdf, jpg, png, mp3 | 1048576 |\n| gemini | gemma-3-27b-it | txt, pdf | 131072 |\n| gemini | gemini-2.5-pro-exp-03-25 | txt, pdf, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| groq | meta-llama-4-scout-17b-16e-instruct | txt, pdf, jpg, jpeg, png | 131072 |\n| groq | meta-llama-4-maverick-17b-128e-instruct | txt, pdf, jpg, jpeg, png | 131072 |\n| groq | llama-3.2-11b-vision-preview | txt, pdf | 128000 |\n| groq | llama-3.2-90b-vision-preview | txt, pdf | 128000 |\n| nvidia | meta-llama-4-scout-17b-16e-instruct | txt, pdf, png, jpeg, jpg, gif | 131072 |\n| nvidia | meta-llama-4-maverick-17b-128e-instruct | txt, pdf, png, jpeg, jpg, gif | 32768 |\n| openai | chatgpt-4o-latest | txt, pdf, jpg, png, gif | 128000 |\n| openai | o1 | txt, pdf, jpg, png, gif | 200000 |\n| openai | o1-pro | pdf, jpg, jpeg, png, gif, webp | 200000 |\n| openai | o3 | txt, pdf, jpg, jpeg, png, gif | 200000 |\n| openai | o4-mini | txt, pdf, jpg, jpeg, png, gif | 200000 |\n| openai | gpt-4.1 | txt, pdf, gif, jpg, png, jpeg | 1047576 |\n| openai | gpt-4.1-mini | txt, pdf, gif, jpg, png, jpeg | 1047576 |\n| openai | gpt-4.1-nano | txt, pdf, gif, jpg, png, jpeg | 1047576 |\n| openai | gpt-4.5-preview | txt, pdf, gif, jpg, png, jpeg | 128000 |\n| openai | gpt-4o | txt, pdf, gif, jpg, png | 128000 |\n| openai | gpt-4o-mini | txt, pdf, gif, jpg, png | 128000 |\n| openai | gpt-4o-2024-11-20 | txt, pdf, gif, jpg, png | 128000 |\n| openai | gpt-4o-search-preview | txt, pdf | 128000 |\n| openai | gpt-4o-mini-search-preview | txt, pdf | 128000 |\n| openrouter | [https://openrouter.ai/openrouter/auto|auto] | txt, pdf, gif, jpg, jpeg, png | depends on the selected model |\n| sambanova | Llama-3.2-11B-Vision-Instruct | txt, pdf, png, jpg | 4096 |\n| sambanova | Llama-3.2-90B-Vision-Instruct | txt, pdf | 4096 |\n| vertex_ai | claude-3-7-sonnet-20250219 | txt, pdf, png, gif, jpeg | 200000 |\n| vertex_ai | claude-3-5-sonnet-20240620 | txt, pdf, pdf, png, gif | 200000 |\n| vertex_ai | claude-3-haiku-20240307 | txt, pdf, png, gif | 200000 |\n| vertex_ai | claude-3-opus-20240229 | txt, pdf, png, gif | 200000 |\n| vertex_ai | meta.llama-3.2-90b-vision-instruct-maas | txt, pdf | 128000 |\n| vertex_ai | meta-llama-4-scout-17b-16e-instruct-maas | txt, pdf, jpeg, jpg, png, gif | 10000000 |\n| vertex_ai | meta-llama-4-maverick-17b-128e-instruct-maas | txt, pdf, jpeg, jpg, png, gif | 1000000 |\n| vertex_ai | claude-3-5-sonnet-v2-20241022 | txt, pdf, jpeg, jpg, png, gif | 200000 |\n| vertex_ai | gemini-2.0-flash-thinking-exp-01-21 | txt, pdf, jpg, png, mp3, mp4 | 32000 |\n| vertex_ai | gemini-1.5-pro | txt, pdf, jpg, png, gif, mp3, mp4, ogg | 2097152 |\n| vertex_ai | gemini-1.5-pro-002 | txt, pdf, jpg, png, gif, mp3, mp4, ogg | 2097152 |\n| vertex_ai | gemini-2.0-flash-exp | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| vertex_ai | gemini-2.0-flash-001 | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| vertex_ai | gemini-2.0-flash-lite | txt, pdf, jpg, jpeg, png, mp3, mp4, ogg | 1048576 |\n| vertex_ai | gemini-2.0-flash | txt, pdf, jpg, jpeg, png, mp3, mp4, ogg | 1048576 |\n| vertex_ai | gemini-2.5-pro-exp-03-25 | txt, pdf, jpg, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| vertex_ai | gemini-2.5-flash-preview-04-17 | txt, pdf, jpg, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| vertex_ai | gemini-2.5-pro-preview-05-06 | txt, pdf, jpg, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| xai | grok-vision-beta | txt, pdf, png | 8192 |\n| xai | grok-2-vision-1212 | txt, pdf, png | 32768 | <sup>(1)</sup> - Encrypted files are not supported. ## See Also [[200]] [[843]] {{{12}}} {{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "2021f0ec-69d2-4e24-a755-4628261a3c84",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "d46afebf-ab07-4f48-85f1-fd16dc2ecbee",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?406",
          "evaluationResultRowContextName": "406",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# File support for multimodal models Multimodal models allow interactions that go beyond traditional text-based prompts. These models can understand and process information from a variety of sources, including text, file documents, images, audio, and video. This means they can analyze, process, and generate content based on a richer and more complete set of inputs. Globant Enterprise AI implements multimodal models, allowing you to interact with models such as Gemini and OpenAI in a completely new way. By combining text with files, a world of possibilities opens up for analyzing, processing, and generating content. These models can directly receive files alongside your prompts, eliminating the need to manually extract and inject file content. This direct file integration offers several advantages. - Simplifies file handling. You no longer need to extract and inject file content into the prompt.\n- Files are sent directly to the model, streamlining the process and enabling direct model interaction.\n- Empowers you to interact directly with files within your prompts, making it easier to analyze, process, and generate content based on their contents. This eliminates the need for manual extraction and injection, simplifying your workflow.\n- Leverages the information contained in files to provide more comprehensive and insightful responses, enhancing their capabilities. ## File References in Prompts You can reference files in your prompts using a specific format: `{file:filename}`. For example, to reference a file named \"report.pdf\" you would write: `{file:report}`. > **Note**: This reference format only works for files that have already been uploaded to your project. If you're uploading a file for the first time, you'll need to use the standard file upload mechanism within the platform. ## How Files are Managed in Your Project 1. **File Storage**: All files uploaded to your project are stored securely and can be accessed by your prompts.\n2. **File Access**: When you reference a file in your prompt, the system automatically retrieves it and sends it to the multimodal model.\n3. **File Types**: The system supports a wide range of file types, including text documents, images, audio files, and more. ## Sample Suppose you have a document called \"product_specs.txt\" loaded in your project. You can use it in your prompt as follows: ```plaintext\n\"Please summarize the key features of the product described in {file:product_specs}\".\n``` The multimodal model will then access the \"product_specs.txt\" file and use its contents to generate a summary of the product features. ## See Also [Supported models and file formats](406.html)",
          "evaluationResultRowContextDocId": "381b4a5d-cb99-40b8-a58e-e67f82082418",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "f2f7c69b-5812-4c22-904e-b0e839687b18",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?345",
          "evaluationResultRowContextName": "345",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.6,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "5ee56e2e-40df-43c9-a83c-24a285f86710",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0051849800",
      "evaluationResultRowEndDate": "2025-05-13T11:14:50",
      "evaluationResultRowOutput": "Los formatos de archivos soportados por los modelos multimodales de Globant Enterprise AI var\u00edan seg\u00fan el modelo y el proveedor, pero en general incluyen:\n\n- Texto: `txt`, `pdf` (archivos PDF no encriptados)\n- Im\u00e1genes: `png`, `jpeg`, `jpg`, `gif`\n- Audio: `mp3`, `ogg`, `wav`, `flac`, `aac`, `m4a` (en algunos modelos y desde la versi\u00f3n 2025-05)\n- Video: `mp4`, `avi`, `mov`, `mkv` (en algunos modelos y desde la versi\u00f3n 2025-05)\n\nPor ejemplo, modelos como los de la serie **anthropic Claude 3.5 Sonnet </B>soportan texto y formatos de imagen como `txt, pdf, jpeg, jpg, png, gif`. Modelos como **Gemini 2.0 </B>soportan adem\u00e1s audio y video (`mp3, mp4`). Otros modelos m\u00e1s especializados soportan variantes adicionales y diferentes ventanas de contexto.\n\nAdem\u00e1s, los modelos no multimodales aceptan texto (`txt`) y PDF (`pdf`).\n\nPara conocer el detalle exacto para cada modelo, te dejo una muestra de la tabla de los modelos y sus formatos compatibles:\n\n| Proveedor  | Modelo                       | Formatos soportados                        | Ventana de contexto (tokens) |\n|------------|------------------------------|-------------------------------------------|------------------------------|\n| anthropic  | claude-3-5-sonnet-20241022   | txt, pdf, jpeg, jpg, png, gif             | 200000                       |\n| gemini     | gemini-1.5-flash-latest      | txt, pdf, jpg, png, gif, mp3, mp4         | 1048576                      |\n| openai     | gpt-4o                      | txt, pdf, gif, jpg, png                    | 128000                       |\n| openai     | o1-pro                      | pdf, jpg, jpeg, png, gif, webp             | 200000                       |\n| awsbedrock | amazon.nova-pro-v1:0         | txt, pdf, png, gif                         | 300000                       |\n\nY muchos otros modelos con sus formatos de soporte detallados en la tabla del documento.\n\nAdem\u00e1s, para la ingesti\u00f3n de documentos en RAG Assistants, est\u00e1n soportados estos formatos en la plataforma, seg\u00fan \u00faltimas revisiones:\n\n| Categor\u00eda          | Extensiones soportadas                                                         |\n|--------------------|--------------------------------------------------------------------------------|\n| Texto              | `.txt`                                                                          |\n| Portable           | `.pdf`, `.md`, `.msg`, `.org`, `.rst`, `.csv`, `.tsv`                          |\n| General            | `.eml`, `.html`, `.xml`                                                        |\n| Microsoft Office    | `.doc`, `.docx`, `.ppt`, `.pptx`, `.xls`, `.xlsx`, `.rtf`                      |\n| Open Document      | `.odt`, `.odp`, `.ods`                                                           |\n| Ebook              | `.epub`                                                                         |\n| JSON               | `.json`, `.jsonl`                                                               |\n| Im\u00e1genes           | `.png`, `.jpeg/.jpg`, `.tiff`, `.bmp`, `.gif`                                  |\n| Audio              | `.mp3`, `.ogg`, `.wav`, `.flac`, `.aac`, `.m4a`                                |\n| Video              | `.mp4`, `.avi`, `.mov`, `.mkv`                                                 |\n| Custom             | `.custom`, `.web`                                                               |\n\n(Esta tabla puede extenderse y configurarse por par\u00e1metros en sistemas on-premises).\n\nPara detalles m\u00e1s espec\u00edficos, ver la documentaci\u00f3n oficial sobre [Supported models and file formats](https://wiki.genexus.com/enterprise-ai/wiki?406) y [RAG File Formats](https://wiki.genexus.com/enterprise-ai/wiki?137).\n\nReferencias:  \n[1](https://wiki.genexus.com/enterprise-ai/wiki?406)  \n[2](https://wiki.genexus.com/enterprise-ai/wiki?345)  \n[7](https://wiki.genexus.com/enterprise-ai/wiki?137)",
      "evaluationResultRowStartDate": "2025-05-13T11:14:37",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "10553"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# LLM Troubleshooting Below you can find known issues when working with [Supported Chat Models](200.html). ### [OpenAI's \"o\" models restrictions](#OpenAI%27s+%22o%22+models+restrictions) The parameters *temperature*, *top_p* and *n* must be set to 1, while *presence_penalty* and *frequency_penalty* are fixed at 0. Any other configuration change may generate errors such as the following: ```\nUnsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported\n``` Check your assistant configuration accordingly, related information [here](https://platform.openai.com/docs/guides/reasoning/quickstart). ### [Empty responses when using any of OpenAI\u2019s reasoning models](#Empty+responses+when+using+any+of+OpenAI%E2%80%99s+reasoning+models) When creating a [Chat Assistant](708.html) using any of OpenAI\u2019s reasoning models\u2014such as **o1**, **o1-mini**, or **o3-mini**\u2014you might encounter a scenario where the response status shows **\u201cSucceeded\u201d** but the actual response content is empty. One common cause is an insufficient **Max Output Tokens** setting. According to [OpenAI\u2019s documentation on reasoning models](https://platform.openai.com/docs/guides/reasoning/quickstart?api-mode=chat#how-reasoning-works), even though reasoning tokens are not visible to you, they still consume space in the model\u2019s context window. If **Max Output Tokens** is set too low, the model may not generate any user-visible output. To resolve this, try configuring the assistant with the maximum **Max Output Tokens** allowed by each model: - **o1** and **o3-mini**: up to 100k tokens\n- **o1-mini**: up to 65k tokens Increasing the **Max Output Tokens** to these values should prevent empty responses when the status indicates \u201cSucceeded.\u201d ### [Invalid 'max_tokens': integer below minimum value](#Invalid+%27max_tokens%27%3A+integer+below+minimum+value) The following error appears when executing an assistant where the *max_tokens* parameter is set to -1. ```\nError code: 400\nInvalid 'max_tokens': integer below minimum value. Expected a value >= 1, but got -1 instead.\ntype: invalid_request_error\nparam: max_tokens\ncode: integer_below_min_value\n``` The case was reproduced using OpenAI provider. Assign a maximum value according to the [selected model](https://platform.openai.com/docs/api-reference/chat#chat-create-max_tokens) as -1 is not detailed as supported. ### [max_tokens is too large](#max_tokens+is+too+large) The following error appears when executing an assistant ```\nError connecting to the SAIA service cause: 400\nmax_tokens is too large: 12000. This model supports at most 4096 completion tokens, whereas you provided 12000\n``` Check the *max_token* parameter supported for your assistant configured model; the selected *max_token* parameter is greater than the maximum supported. ### [The response was filtered due to the prompt triggering Azure OpenAI's content management policy](#The+response+was+filtered+due+to+the+prompt+triggering+Azure+OpenAI%27s+content+management+policy) The following error appears when executing an assistant with a complex query using Azure OpenAI endpoints ```\nThe response was filtered due to the prompt triggering Azure OpenAIs content management policy.\nPlease modify your prompt and retry.\nTo learn more about our content filtering policies please read our documentation\nhttps://go.microsoft.com/fwlink/?linkid=2198766\n``` Check the deployment made for the associated endpoint, make sure to set the *content filter* to the empty value (default); do not use the [Microsoft.Default.v2](https://learn.microsoft.com/en-us/azure/ai-services/openai/whats-new#new-responsible-ai-default-content-filtering-policy) configuration. Go to the [Azure AI Foundry portal](https://ai.azure.com/), locate the *deployments* section; and for each completion model (such as gpt-4o, gpt-4o-mini), use the *Update Deployment* option to set the Content Filter as \"Default\". ![Image](./images/883.png) ### [Service: BedrockRuntime, Status Code: 403, Request ID: GUID](#Service%3A+BedrockRuntime%2C+Status+Code%3A+403%2C+Request+ID%3A+GUID) The following error appears when using a model in AWS Bedrock ```\nYou don't have access to the model with the specified model ID\nReceived Model Group=awsbedrock/modelname\n``` Make sure you have access to the model *modelname*, follow these steps to enable it: [Add or remove access to Amazon Bedrock foundation models](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html) ### [Empty Prompt for Anthropic Models](#Empty+Prompt+for+Anthropic+Models) When creating a [Chat Assistant](708.html) and selecting an Anthropic model (for example, *anthropic/claude-3-7-sonnet-latest*), the **Prompt** field is mandatory and cannot be left empty. If you try to configure the assistant without any content in this field, you will encounter an error similar to the following: ```\n{\"error\":{\"message\":\"litellm.BadRequestError: AnthropicException - {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"invalid_request_error\\\",\\\"message\\\":\\\"system: text content blocks must be non-empty\\\"}}. Received Model Group=anthropic/claude-3-7-sonnet-latest\\nAvailable Model Group Fallbacks=None\",\"type\":null,\"param\":null,\"code\":\"400\"}}\n``` Make sure that the **Prompt** field contains at least one valid text content block. Review the assistant's configuration and fill in the prompt with appropriate information to avoid this error.",
          "evaluationResultRowContextDocId": "02b5107e-351f-42fd-91b5-715e37300386",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "093be937-b949-44ed-b448-e9e4abe07985",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?290",
          "evaluationResultRowContextName": "290",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Globant Enterprise AI Frontend **Warning**: Globant Enterprise AI Frontend will be discontinued as of February 10th, 2025. [Playground Interface](349.html) is on the way to replace this. The Globant Enterprise AI Frontend offers an experience similar to what you will find in ChatGPT or Bard, as well as an intuitive interface for interacting with Assistants and LLMs. Additionally, it allows you to create your own prompts and adjust settings in the User Menu to have full control over your customized experience. The frontend screen is divided into three sections: 1. Assistants Panel and User Menu\n2. Chat Panel\n3. Contextual Panel ![Image](./images/72.png) Below is a detailed description of the steps for establishing conversations with Assistants and LLMs, together with information on the operation of each Panel. ## 1. Assistants Panel and User Menu This Panel contains: - **In the upper part:** the Assistants section.\n- **In the lower part:** the User Menu. ### Assistants Section In the upper part of the Panel there is a list of Assistants, including those defined by default and those defined by your organization. The default Assistants include \"Search with Google\", \"Chat with LLMs\", and \"Create Images\". ![Image](./images/73.png) In this section, you may perform the following actions: - Generate a New Chat by selecting the Assistant with which you wish to interact, or click on \u201c+ New chat\u201d.\n- Create New Folders by clicking on the option available next to \u201c+ New Chat\u201d (whose image contains a folder and \u201c+\u201d).\n- Select, search for, and organize Chats. Once you have created folders and chats, you may select them, make searches, and even organize your chats. For example, you may drag your chats to the corresponding folder to keep everything organized and accessible.\n- Rename your conversations. To do so, click on the name of the chat, select the pencil image, and then edit based on your preferences. Lastly, confirm the change by pressing Enter or by clicking on the check mark.\n- Delete a specific chat when you no longer need it. To do this, you must select the chat to be deleted, click on the Trash icon, and end by confirming with a click on the check mark. Bear in mind that the chat history is stored in the local storage of the browser you are using; therefore, if you change your browser, you will not view the same data. ### User Menu The User Menu is in the lower part of the Panel, with additional options to manage and customize your experience. ![Image](./images/74.png) It includes the following options: - **Clear Conversations:** it deletes all conversations.\n- **Settings:** it adjusts the Panel's appearance by changing between light and dark modes.\n- **Log out:** to exit the system. ## 2. Chat Panel The Chat Panel is the main interface for interacting with Assistants and LLMs. Here, you will be able to view your queries and the responses generated. You may use the Chat Box to enter your queries and enjoy fluent communications. The upper part of the Chat Panel shows the name of the Assistant selected for chatting. The following image shows that the assistant selected is \"Search with Google\". ![Image](./images/75.png) You will find arrows on both sides of the selected Assistant name: - Left arrow: collapses the Assistants Panel.\n- Right arrow: collapses the Prompt Panel. Below is a detail of the steps you should follow to: - Chat with Assistants.\n- Chat with LLMs. ### Chat with Assistants 1. **Select Assistant:** select the Assistant you wish to interact with in the Assistants Panel.\n2. **Write Message:** write your message in the Chat Box of the Chat Panel in accordance with the Assistant\u2019s guidelines.\n3. **Send Message:** press Enter or click on the arrow that will send your message to the Assistant.\n4. **Receive Reply:** the Assistant's reply will appear in the Chat Panel. ### Chat with LLMs 1. **Select Assistant:** select the LLM Assistant with which you wish to interact in the Assistants Panel. ![Image](./images/76.png) 2. **Customize the LLM:** in the Chat Panel, you may select the desired LLM Assistant (the default value is GPT-3.5) and adjust the conversation style using the scroll bar.\n3. **Write Message:** write your message in the Chat Box of the Chat Panel.\n4. **Send Message:** press Enter or click on the arrow that will send your message.\n5. **Receive Reply:** the reply will appear in the Chat Panel. ## Contextual Panel The Contextual Panel allows you to simplify daily or repetitive tasks by creating prompts. The prompts are stored locally in your computer to facilitate the execution of actions with just one click. ![Image](./images/77.png) The Prompt Panel allows the following actions: - **Create a New Prompt:** click on \"+ New prompt\" and complete the data requested on this screen: ![Image](./images/79.png) - **Create New Folders:** as in the Chat Panel, you may organize your Prompts in customized folders. To create a new folder, click on the option appearing beside \u201c+ New prompt\u201d (with an image of a folder and \u201c+\u201d).\n- **Select, Search, and Organize Prompts:** once you have created folders and prompts you may select, do searches, and even organize your prompts. Simplify your experience by dragging your prompts to the corresponding folders. To use a defined prompt go to the Chat Panel, write a slash (/) in the Chat Box, and select the desired prompt: ![Image](./images/80.png)",
          "evaluationResultRowContextDocId": "23ee2522-aa47-4ddb-b9ff-fb9f7b9ed494",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "21197ce3-adfb-4eda-8e13-803fc7d6d122",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?82",
          "evaluationResultRowContextName": "82",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "``` ## Step 3: Creating the Flow Follow the steps described in [[322]] to create the Flow called Mall Chat Assistant with the following structure: [[364]] To add variables to the Flow, click on the Variable option in the LOGICAL section of the [[371##Flows menu|Flows Menu]]. Next, drag the component below the User input node into the flow. In this case, a variable has been added: today. **Today variable:** It is defined by clicking on the down arrow inside the variables node and selecting the option + Add new variable. Next, enter the name today and press Enter. Deactivate the Clear option. To set the value of this variable, click on the edit icon to the right of the node. In the left menu, click on the Open in new window icon, and in the window that opens (Value) add `$today`. ## Step 4: Implementing the Derivation Logic Every end user query must be evaluated to determine which assistant is the most appropriate to answer. This is implemented with IF conditions in the flow, where the end user's intentions are compared with the available assistants. To add the Conditional component, go to the Interactions section and, below LOGICAL in the Flow menu, drag the component and drop it between the Assistant and Go to nodes. [[365]] Dragging the Conditional node generates a branch with two paths: one labeled If and one labeled Else. This structure allows you to evaluate a condition and direct the flow according to the result. To add an additional condition, click on the Add condition option to the right of the first node of the branch: [[366]] After clicking on Add condition in the first node of the branch, an additional node of Expression type is added, just like the first one. These Expression nodes allow evaluating different conditional expressions. [[367]] To add the variable to be evaluated, click on the word Variable, located at the bottom of each node. In the first two nodes of Expression type, you can see how each condition evaluates the `orchestrationResponse` variable. Depending on the value of this variable, the flow is directed to different assistants: either to the Movie Ticket Assistant or to the Mall Shops Catalog. If neither of these conditions is met, the flow continues through the Else branch, where by default there is a Go to node. A Message node has been added before the Go to node so that the flow can display a specific message to the end user. Then, the corresponding nodes are added to handle the responses according to the evaluated condition. [[368]] A node of API Assistant type that is configured for the Movie Ticket Assistant and handles queries related to the movie theater in the shopping mall is placed below the first node of Expression type (which checks if `orchestrationResponse` is equal to \"Movie Ticket Assistant\"). A node of RAG Assistant type that is configured for the Mall Shop Catalog Assistant and provides information about the stores in the mall is added below the second node of Expression type (which checks if `orchestrationResponse` is equal to \"Mall Shops Catalog\"). Finally, below each node a Go to is added, which redirects back to the User Input node. Once the assistant has processed and answered the query, this allows the flow to return to the entry point to receive new user interactions, ensuring a continuous and fluid conversation. [[369]] ## Step 5: Testing and Adjusting the Mall Chat Assistant Once you have configured the flow, make sure to click on the 'Save' button located at the top left of the screen. This step is essential to save all changes made to the flow. To test the flow you have created, you have three options available from the same design window: 1. Click on the arrow icon (similar to the play button) to run the flow ('Run Flow').\n2. Click on the message icon to open the testing assistant ('Open Testing Assistant').\n3. Click on the open in new window icon (the box with an arrow) to access the demo page ('Go to Demo Page'). The first two options will open a window on the right side of the screen, while the third option will open a new window in the browser, allowing you to see how the flow runs in a demo environment. During testing, you can run various queries to make sure that the flow responds correctly. For example, you can ask \"Where can I purchase books?\" and the flow should correctly identify the query and direct it to the Mall Shop Catalog, where the end user will be informed about the availability of stores that sell books. Then, you can ask \"What movies can I watch?\" and verify that the flow correctly redirects to the Movie Ticket Assistant to get the list of available movies. If the answer is ambiguous, as in \"I would like to watch the last one\", the orchestrator should use the context of the conversation to identify the last movie mentioned and continue the interaction with the Movie Ticket Assistant. In situations where the query does not correspond to any assistant\u2014such as \"What is Linux?\"\u2014the orchestrator should handle the response using a fallback and provide the information directly from the model, so that the flow covers all possible end user interactions. These tests will allow you to adjust the flow as needed, ensuring that the Mall Chat Assistant runs efficiently and delivers a smooth and accurate experience for end users. {{{12}}}{{{17}}}##MARKDOWN##\n\n#How to create a Flow with Conditional component This is a step-by-step guide to create a Mall Chat Assistant using [[321|Flows]] in Globant Enterprise AI. This Flow manages multiple assistants, which are specialized in different areas such as querying store catalogs, booking movie tickets, and orchestrating conversations. The steps required to set up and test this assistant are described below. ## Step 1: Configuring the Work Environment To get started, first make sure you have access to the [[42]] and are familiar with the [[371|interface for creating Flows]]. This environment will allow you to create, manage, and orchestrate the various assistants that make up the Mall Chat Assistant. ## Step 2: Creating Assistants ### Mall Shop Catalog Assistant This [[44|RAG Assistant]] is designed to provide information about the stores in the mall. The objective is to allow end users to check the availability of products or services in the different stores of the mall. The assistant is configured to answer questions related to the location, products, and opening hours of the stores. It uses a simple JSON document containing the store name and a short description. ### Movie Ticket Assistant This [[110]] allows end users to search the movie listings available in the mall and book tickets. It is based on the integration with an API that simulates the availability of movies in a movie theater. This assistant must be able to handle questions about schedules and reservations. ### Orchestrator Mall Assistant This [[566##Chat Assistant creation|Chat Assistant]] manages and redirects conversations to the appropriate assistant. The orchestrator is responsible for understanding the end user's intent and redirecting the conversation accordingly. It uses a system of predefined prompts to redirect queries to the corresponding assistant. The orchestrator keeps the context of the conversation, so the interaction is fluid and coherent. It defines variables such as user name and date. Orchestrator Mall Assistant Prompt: ```\nAssistant is a large language model.\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions.\nAdditionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\nAssistant is a helpful Mall Customer Service agent; you are kind but you always follow the rules. CONVERSATION CONTEXT ---\nThe user name is: {name} Today is: {today} OTHER ASSISTANTS\n---\nAssistant can ask the user to use other Assistants to look up information that may be helpful in answering the user\u2019s original question. The other Assistants the human can ask are:\n> Mall Shops Catalog: This Assistant provides information about shops inside the mall.\n> Movie Ticket Assistant: This assistant provides information about the movie theater inside the mall. RESPONSE FORMAT\n---\nWhen responding to me, if you want the human to ask another Assistant JUST ANSWER WITH THE ASSISTANT NAME AND NOTHING ELSE.\nFor any other response, you can create an HTML fragment with RICH TEXT if it's needed.",
          "evaluationResultRowContextDocId": "57a2c916-1a55-4579-abb9-419226e4e2c1",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "26867a83-050e-453f-a942-0f26bee35a78",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?363",
          "evaluationResultRowContextName": "363",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#How to create a Flow with internet search This is a step-by-step guide to create a conversational assistant with internet search using [[321]]. This Flow manages two assistants: a Chat Assistant that answers questions using internet search results, and another Chat Assistant that rephrases follow-up questions (based on the conversation history) as standalone questions. In addition, it includes a Search API component. ## Step 1: Configuring the Work Environment To get started, first, make sure you have access to the [[42]] and are familiar with the [[371|interface for creating Flows]]. ## Step 2: Creating Chat Assistants In this step, two [[566##Chat Assistant creation|Chat Assistants]] are created. They are called 'Response from Search' and 'Follow-up Rephrase'. ### 'Response from Search' Chat Assistant This Chat Assistant generates answers for the end user based solely on the results of an internet search, without using prior knowledge or inventing information. It takes the provided search results and uses them to answer the end user's question in the best possible way. If necessary, the answer can include rich text in HTML format, but Markdown should not be used to format the answer. **LLM Settings:** - AI Provider: Google VertexAI\n- Model: gemini-1.5-pro\n- Temperature: 0.10\n- Max Output Tokens: 8192 **Prompt:** ```\nINSTRUCTIONS\n---\nYour task is to generate a response for the user based on the results of an internet search.\nDO NOT USE PRIOR KNOWLEDGE AND DO NOT MAKE UP INFORMATION USE THE SEARCH RESULTS.\nGiven the following results from an internet search and NOT PRIOR KNOWLEDGE answer the user question the best you can. SEARCH RESULTS\n---\n{searchResults} RESPONSE FORMAT\n---\nFor the response you can generate RICH TEXT USING HTML if it is needed.\nDO NOT USE MARKDOWN TO FORMAT YOUR RESPONSE.\nFOR RICH TEXT DO NOT USE MARKDOWN USE HTML INSTEAD.\n``` ### 'Follow-up Rephrase' Chat Assistant This Chat Assistant rephrases a follow-up question based on the conversation history, ensuring it becomes a standalone question if needed. If the follow-up question already works as a standalone question, it simply repeats the question without any changes. **LLM Settings:** - AI Provider: Google VertexAI\n- Model: gemini-1.5-pro\n- Temperature: 0.10\n- Max Output Tokens: 8192 **Prompt:** ```\nGiven the conversation history and not prior knowledge, and a follow-up question, rephrase the follow-up question to be a standalone question IF IT IS NEEDED. If the question works as a standalone question, just repeat the question. RESPONSE ONLY WITH THE STANDALONE QUESTION AND NOTHING MORE; DO NOT INCLUDE ANY OTHER TEXT.\n``` ## Step 3: Creating the Flow Follow the steps described in [[322]] to create the Flow called 'Search Assistant' with the following structure: [[689]] ## Step 4: Implementing the API component To add the API component, go to the [[494|Interactions section]] and, below INTEGRATIONS in the Flow menu, drag the component between the Assistants nodes. [[690]] ## Step 5: Setting Up the Search API Go to [https://developers.google.com/custom-search/v1/introduction?hl=es-419|Google Search API documentation] and click on Get API key button. [[691]] In the pop-up window that opens, select or create a project from the Select or create project menu. [[692]] Select the Yes option to agree that the use of the services and related APIs is subject to compliance with the applicable Terms of Service. Then click on NEXT to continue with the setup process. In the next window, click on the SHOW KEY button to reveal the API Key. Copy this API key, as it will be required to configure the API node in your flow. [[693]] ## Step 6: Creating a Programmable Search Engine Go to [https://programmablesearchengine.google.com/controlpanel/all|Programmable Search Engine] and click on the Add button. [[694]] In the pop-up window that opens, fill in the necessary fields to create the search engine: [[695]] - **Name of your search engine:** In the \"Search engine name\" field, enter a name for the search engine, such as Search Assistant.\n- **What to search for:** Select the \"Search the entire web\" option to have the engine search the entire web. If you prefer to search only specific sites, you can select the \"Search specific sites or pages\" option and add the necessary URLs.\n- **Search settings:** If you wish, you can adjust additional settings such as image search or SafeSearch filter.\n- **Captcha:** Select the \"I am not a robot\" checkbox.\n- **Create:** At the end of the window, click on the Create button to finalize the creation of the search engine. In the window that appears, you must click on the Customize button. [[696]] This will take you to the Overview section of the search engine you have just created. In this section, you should find and copy the Search engine ID, which is a unique value associated with your search engine. This ID is required to configure the API node in your flow. [[697]] ## Step 7: Configuring the API Node With the Search engine ID and API key, you can now configure the API node. First, you must click on the Edit button that appears when you position the mouse over the API node. [[698]] Once you have clicked on Edit, the State Configuration menu will be displayed on the left side. [[699]] Within this menu, in the URL section, you must configure the URL with the following value: ```\nhttps://www.googleapis.com/customsearch/v1?key=<API key>&cx=<Search engine ID>&q=<query>\n``` Be sure to replace `<API key>` and `<Search engine ID>` with the previously obtained values. In the API node, go to the **Parameters** section and click on **+ Add new** to add a new parameter. Then, configure the parameter with the following details: - **Parameter name:** q.\n- **Value:** lastResponse (the value obtained from the previous response). Next, in the State Configuration section, you must make sure to mark q as **Query params**. [[700]] In the **Responses** section, make sure to map the items property to the **searchResults** variable. If searchResults is not available, you can click on Add new variable to create it. [[701]] ## Step 8: Configuring the Assistant Node After you have configured the API node, go to the Assistant node, which is located just below the API node in your flow. In this node, find the Variable section and change the input variable to searchResults. [[702]] ## Step 9: Testing the Flow Finally, save the changes and test the flow. You can use the test window or the Playground to make sure everything works correctly, verifying that the Assistant responds properly using the search results. | [[704]] | [[705]] | {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "526b3969-e9ac-4188-a986-6b0013b11c4d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "37481100-f529-430c-80be-1a56b903368c",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?688",
          "evaluationResultRowContextName": "688",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Start Page The Globant Enterprise AI Start Page allows offering end users a clear guide to the assistant's capabilities and examples of use. To achieve this, the Start Page allows you to define the following elements that guide end users in their interaction with the assistant: - Define a welcome message that clearly describes the purpose of the assistant. - Describe the assistant's key functions, explaining the problems it is designed to solve, the actions it can perform correctly and its known limitations, ensuring that the end user has realistic expectations about its performance.\n- Add sample prompts that guide the user on how to interact with the assistant. These prompts can be specific requests such as \"Show me the composition of the company\u2019s profit during the last quarter\" or \"Create a bar chart with the sales data for the first 6 months of the year\", which provide insight into the interactions that the assistant can effectively handle. These guidelines are not only fundamental to improving the end user experience, but can also be leveraged in technical scenarios such as batch evaluations or synthetic data generation, providing a clear framework for understanding and analyzing the assistant's behavior in more advanced contexts. ## Start Page Presentation Depending on the type of assistant you are configuring, the layout of the Start Page may vary slightly. There are two presentations: one for the Assistants ([Data Analyst Assistant](886.html), [API Assistant](110.html), [Chat with Data Assistant](159.html)) and one for the [RAG Assistants](44.html). Despite these visual differences, both versions share the same essential functionalities. To configure the Start Page for both Assistants and RAG Assistants, first go to the corresponding page in the [Backoffice](42.html) to work with your data and click on 'UPDATE'. After that, depending on the type of assistant, the following tabs will be displayed: - Assistants: 'General Information' and 'Start Page'.\n- RAG Assistants: 'General Information', 'Prompt', 'Retrieval' and 'Start Page'. ## Assistants In the Assistants Start Page option, first you will find the option to configure the Welcome Message, where you define a title and a description to explain how the assistant will help the end user. Clicking on the 'CONFIRM' button will save this data and the message will appear in the Playground when accessing the assistant, helping the user to get started. In addition, you can add information about specific features of the assistant using the CREATE FEATURE button. Clicking on it opens a window for you to enter a title and a short description about the feature you are describing. After entering the title and description, click on CONFIRM to save the defined feature. Once saved, the feature will be displayed in the Playground along with its description. You can also set up sample prompts by clicking on CREATE EXAMPLE PROMPT. These predefined prompts will appear in the Playground and, when selected, will be completed automatically, facilitating the end user's interaction with the assistant and showing practical suggestions to perform certain tasks. Clicking on the CREATE EXAMPLE PROMPT button opens a window to enter the following information: - Name: Specify a name for the prompt.\n- Description: This field is optional; you can add a brief explanation describing the functionality of the prompt and how it should be used.\n- Text: This field is mandatory and here you define the content that will be used as an example in the prompt. The assistant will take this text to perform the configured task. Once you have entered the information, click on CONFIRM to save the prompt. ## RAG Assistants The RAG Assistants Start Page has a different layout compared to that of Assistants. Instead of the Welcome Message section used in Assistants, for RAG Assistants this section is called Basic Information. Here you define a title and description that clearly explain how the assistant will help end users in their tasks. This information is essential to provide users with information about the assistant's capabilities. Once you have completed the title and description, click on CONFIRM to save the changes. In RAG Assistants, instead of two buttons to create functionalities and examples of prompts, there is only one button: CREATE INFORMATION ITEM. When you click on CREATE INFORMATION ITEM, a window will open where you can enter a title, a description and the text that will make up the body of the information item. In addition, in this window you must select the type of item you want to create: Prompt or Feature. If you choose Prompt, you will define specific examples that will guide the end user when interacting with the assistant. If you select Feature, you will be describing a specific property of the assistant, clearly explaining its usefulness and how it can be used. Once you have completed the required fields, click on the SAVE button, and then click on CONFIRM (in Start Page) to save your changes.",
          "evaluationResultRowContextDocId": "0e63b8cf-b481-4530-8320-a108aaca5167",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "9ee48fc4-891c-4f85-a241-b4bf6fc9616a",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?408",
          "evaluationResultRowContextName": "408",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Tokens tab in Dashboard The Tokens tab in the Dashboard provides a detailed view of token consumption in the system, including key metrics and visualizations for analysis. To access this section, click on \"Tokens\" in the Dashboard interface. [[966]] ###Filters Start and end dates can be selected to define the analysis period. ###Key Token Metrics Main token-related metrics are displayed: * **Number of Tokens**: Total number of tokens used * **Input**: Tokens sent in requests. * **Output**: Tokens generated in responses. * **Total**: Sum of both values. * **Average Tokens by Request**: Average number of tokens used per request, divided into * **Input**: Average tokens in requests. * **Output**: Average tokens in responses. * **Total**: Sum of both values. ###Chart: Number of Tokens by Day This chart displays the daily token usage within the selected date range. It represents the number of Input, Output, and Total tokens, allowing users to visualize daily consumption patterns. ###Table: Tokens by Assistants This table breaks down token usage by assistant in the system. It includes the following columns: * **Assistant:** Name of the assistant that processed the requests.\n* **Model:** AI model used by the assistant.\n* **Input: **Tokens sent in requests to that assistant.\n* **Output: **Tokens generated in responses from that assistant.\n* **Total:** Sum of both values. This table helps analyze which assistants consume the most tokens and which AI models they are using. ###Availability Since April 2025 release. {{{12}}} {{{17}}} ##MARKDOWN##",
          "evaluationResultRowContextDocId": "03734457-63f8-41fc-9805-f7d3e32b5342",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "d02a535f-4bcf-40a9-9935-e0025fe1faf3",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?965",
          "evaluationResultRowContextName": "965",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.7,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        }
      ],
      "dataSetRowId": "6360496d-8a59-4ebf-bba6-7093e09c13d1",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0035669400",
      "evaluationResultRowEndDate": "2025-05-13T11:14:57",
      "evaluationResultRowOutput": "After submitting a prompt to the assistant in Globant Enterprise AI, the following happens:\n\n1. **Processing the Input: </B>The user writes a message in the Chat Box of the Chat Panel and sends it by pressing Enter or clicking the send arrow.\n\n2. **Assistant Receives the Query: </B>The selected Assistant processes the user input according to its configured logic and model.\n\n3. **Generating a Response: </B>The Assistant generates a reply based on the prompt, possibly leveraging configured AI models or retrieving data as per its design.\n\n4. **Displaying the Response: </B>The generated reply appears in the Chat Panel for the user to read and interact with.\n\nThis interaction flow allows a continuous conversation between the user and the Assistant, facilitating multiple back-and-forth messages as needed.\n\nThis general process is described in the **Globant Enterprise AI Frontend </B>documentation, particularly in the section on the **Chat Panel**, under \"Chat with Assistants\" or \"Chat with LLMs\" where it details selecting an assistant, typing a message, sending it, and receiving the reply:\n\n- Select an Assistant in the Assistants Panel.\n- Write and send a message in the Chat Panel.\n- The Assistant processes the message and sends back a response.\n- The response is displayed in the Chat Panel.\n\nReference: [Globant Enterprise AI Frontend](https://wiki.genexus.com/enterprise-ai/wiki?82) [4](https://wiki.genexus.com/enterprise-ai/wiki?82)",
      "evaluationResultRowStartDate": "2025-05-13T11:14:50",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "8172"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# History Prompt Usage Defines how historical interactions are handled in the conversation. It is configured in the [Prompt](69.html) of the [RAG Assistant](44.html). The default History Prompt value is as follows: ```\nGiven the following conversation and a follow-up question, rephrase the follow-up question to be a standalone question. Chat History:\n{chat_history}\nFollow-Up Input: {question}\nStandalone question:\n``` Depending on the use case, you may need to try different alternatives to compact the history and rephrase the query to keep the relevant information. This is another prompt that can be used. ```\n--- functionality ---\nreceive a chat history, along with a new question. And add a very brief and concise summary of the conversation history before the question --- input structure --- history: [conversation history]\nnew question: [question2] --- output structure --- [very concise summary of the previous conversation] [question2] --- input ---\nhistory: {chat_history}\nnew question: {question} --- question with summary of the previous conversation ---\n``` Anyway, you can change the prompt according to your use case. ### [Compacting the Conversation History with LLMs](#Compacting+the+Conversation+History+with+LLMs) If you set the [History Message Count](69.html) parameter to a value greater than 0, the RAG Assistant will use an LLM to compact the conversation history before querying the vector store. This is especially important when using models with small context windows (e.g., 4k, 16k). The RAG Assistant uses the History Prompt to summarize the conversation history and maintain the essential parts of the conversation and relevant context for the new question. You can control this behavior using the `compactHistory` option: - `compact` (default): The RAG Assistant uses the History Prompt to summarize the conversation history.\n- `none`: The RAG Assistant bypasses the History Prompt and uses only the last n interactions (as defined by the History Message Count parameter). The last messages from previous conversations will be included (question and answer). Use this value for more control. The compactHistory option is configured from the [Profile Metadata](70.html), in [Retrieval](70.html) below chat/search. Here's an example: ```json\n{ \"chat\": { \"search\": { \"compactHistory\": \"none\" } }\n}\n``` If you don't specify this value, it defaults to `compact`. > **Note**: You may need to adjust the **Chunk Count** parameter to balance the total number of input tokens sent to the prompt. ### [Sample](#Sample) Consider a RAG Assistant with the following settings in the [Prompt](69.html) section: - History Message Count: 6\n- [Chunk Count](69.html): 2 In a conversation where the following questions and answers are exchanged: ```\nquestion 1\nanswer 1\nquestion 2\nanswer 2\nquestion 3\nanswer 3\nquestion 4\n``` To answer **question 4**, the RAG Assistant will perform the following steps: #### [compact](#compact) 1. Call an LLM using the History Prompt to compact the history; in this case, it considers from \"question 1\" to \"answer 3\" (because History Message Count is 6).\n2. Use the result of the previous call to get the best 2 chunks from the vector store.\n3. Call an LLM using the Prompt with \"question 4\" and the context variable assigned. #### [none](#none) 1. Get the best 2 chunks from the vector store using \"question 4\".\n2. Call an LLM using the Prompt with question 4 and the context variable assigned, plus the latest messages exchanged (the complete messages from \"question 1\" to \"answer 3\"). The last LLM call will differ depending on the **compactHistory** setting: | compact | None |\n|---|---|\n| `{\"model\":\"...\",\"messages\":[{\"role\":\"user\",\"content\":\"You are an assistant...question: 4\"}]}` | `{\"model\":\"...\",\"messages\":[{\"role\":\"system\",\"content\":\"You are an assistant....question: question 4\"},{\"role\":\"user\",\"content\":\"question 1\"},{\"role\":\"assistant\",\"content\":\"answer 1\"},{\"role\":\"user\",\"content\":\"question 2\"},{\"role\":\"assistant\",\"content\":\"answer 2\"},{\"role\":\"user\",\"content\":\"question 3\"},{\"role\":\"assistant\",\"content\":\"answer 3\"},{\"role\":\"user\",\"content\":\"question 4\"}]}` | As you can see, the `compact` setting removes unnecessary context, while the none setting includes all the recent messages. Choose the setting that best suits your needs and the context window of your LLM. ### [See Also](#See+Also) [Manual History Management](85.html)",
          "evaluationResultRowContextDocId": "84baf845-aa2d-490a-a17f-ada7c331dee2",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "01d4d000-95c9-4e5a-ae3b-a2dcf8ac3112",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?297",
          "evaluationResultRowContextName": "297",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "{ \"model\": \"saia:flow:87507723-3e3b-47f6-a1d0-aa53370c71d2\", /* Where 87507723-3e3b-47f6-a1d0-aa53370c71d2 is the bot_id*/ \"messages\": { \"role\": \"user\", \"content\": \"Question for flow\" }\n}\n``` ### cURL Sample ```bash\ncurl --location \"$BASE_URL/chat\" \\\n-H \"Content-Type: application/json;charset=utf-8\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"saia-conversation-id: $CONVERSATION_ID\" \\\n--data '{ \"model\": \"saia:flow:87507723-3e3b-47f6-a1d0-aa53370c71d2\", \"messages\": { \"role\": \"user\", \"content\": \"Question for flow\" }\n}'\n``` ## How to integrate Globant Enterprise AI with third-party SDKs ### cURL ```bash\ncurl -X POST \"$BASE_URL/chat\" \\\n-H 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n-H 'X-Saia-Cache-Enabled: false' \\\n-H 'Content-Type: application/json' \\\n-data '{ \"model\": \"openai/gpt-4o\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a professional Translator. Translate the user text to English. Just output one word. \" }, { \"role\": \"user\", \"content\": \"Hola\" } ], \"stream\": false\n}'\n``` Note that in the `model` parameter, you must specify the model in the format `provider/nameModel`. ### OpenAI SDK for Python ```python\nfrom openai import OpenAI api_key = \"$(SAIA_API_KEY)\"\napi_base = \"https://api.saia.ai/chat\" openai = OpenAI(api_key=api_key, base_url=api_base) completion = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}]) print(completion.choices[0].message.content)\n``` ### OpenAI SDK for TypeScript ```typescript\nconst { Configuration, OpenAIApi } = require(\"openai\"); const configuration = new Configuration({ apiKey: SAIA_APITOKEN, basePath: \"https://api.saia.ai/chat\", });\nconst openai = new OpenAIApi(configuration); async function main() { const chatCompletion = await openai.createChatCompletion({ model: \"gpt-4o-mini\", messages: [{role: \"user\", content: \"Hello world\"}], }); console.log(chatCompletion.data.choices[0].message);\n} main();\n``` ## How to call Gemini 1.5 pro ### cURL ```bash\ncurl -X POST \"$BASE_URL/chat\" \\\n-H 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n-H 'X-Saia-Cache-Enabled: false' \\\n-H 'Content-Type: application/json' \\\n-data '{ \"model\": \"vertex_ai/gemini-1.5-pro-preview-0409\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a professional Translator. Translate the user text to English. Just output one word. \" }, { \"role\": \"user\", \"content\": \"Hola\" } ], \"stream\": false\n}'\n``` ## Creating and Using an Assistant with Variables First, you need to create an assistant with the [variables](85.html), and then pass the variables when you use it. ### cURL - Creating the Assistant ```bash\ncurl --location 'https://api.saia.ai/v1/assistant' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer $SAIA_APITOKEN' \\\n--data '{ \"type\": \"chat\", \"name\": \"Test-variables\", \"prompt\": \"You are a translator. Translate to {language}.\"\n}'\n``` ### cURL - Using the Assistant ```bash\ncurl --location 'https://api.saia.ai/chat' \\\n--header 'Saia-Auth: $SAIA_APITOKEN' \\\n--header 'X-Saia-Cache-Enabled: false' \\\n--header 'Content-Type: application/json' \\\n--data '{ \"model\": \"saia:assistant:Test-variables\", \"messages\": [ { \"role\": \"user\", \"content\": \"Hello\" } ], \"variables\": [ { \"key\": \"language\", \"value\": \"French\" } ], \"stream\": false\n}'\n```\n\n# Chat API The Chat API simplifies how businesses connect to various AI models across different providers. It acts as a single point of access, eliminating the need to manage multiple SDKs or APIs. This means you can interact with any supported LLM using just one SDK. In addition, the API centralizes the use of any `Assistant` in a single entry point. Check the [generic variables](20.html) needed to use the API. ## Endpoint The general endpoint is as follows: | Method | Path |\n|---|---|\n| POST | /chat or /chat/completions | The `/chat/completions` and `/chat` endpoints are supported and you can choose either endpoint. Both have the same interface (Request/Response) as the [OpenAI Chat Completion API](https://platform.openai.com/docs/api-reference/chat/create). These endpoints can be called using the OpenAI SDKs. ## POST/chat ### Request - Method: POST\n- Path: $BASE_URL/chat\n- Headers: - Content-Type: application/json - Authorization: Bearer $SAIA_PROJECT_APITOKEN - saia-conversation-id: $CONVERSATION_ID This header only applies to [Flows](321.html). It is necessary to maintain the context of the conversation. If omitted, each message will be treated as a new conversation without context. The value can be any alphanumeric string and changing it will start a new conversation. Conversations are valid for 10 minutes of inactivity, after which they are automatically deleted. Currently, if the response takes more than 60 seconds to be generated, it will not be received. Therefore, this endpoint is suitable only for simple Flows that do not require many interaction steps.\n- Request Body The payload will vary depending on the selected `Assistant`. The general pattern is as follows: ```json\n{ \"model\": \"string\", // mandatory \"messages\": [ ... ], // at least one message \"stream\": boolean // optional\n}\n``` The `model` needs to address the assistant `type` and `name` or `bot_id`, depending on the Type. Then, the parameters will vary depending on the `type`. Its format is as follows: ```json\n\"model\": \"saia:<assistant_type>:<assistant_name>|<bot_id>\"\n``` The `bot_id`, found in the [OVERVIEW](371.html), located in the Flow Builder **Side Navigation Menu**, is used if the `type` is `flow`. | Type | Description |\n|---|---|\n| `assistant` | Identifies an [Assistant API](35.html), [Data Analyst Assistant](886.html), [Chat with Data Assistant](159.html) and [API Assistant](110.html) |\n| `search` | Identifies a [RAG Assistant](64.html) |\n| `flow` | Identifies a [Flow](321.html) | The `messages` element defines the desired messages to be added. The minimal value needs to be the following, where the `content` details the user input. ```json\n{ \"role\": \"string\", /* user, system and may support others depending on the selected model */ \"content\": \"string\"\n}\n``` You can add additional parameters. Below are possible body samples. ## Assistant Sample ```json\n{ \"model\": \"saia:assistant:translate-to-spanish\", /* Using a Standard Assistant named 'translate-to-spanish' */ \"messages\": [ { \"role\": \"user\", \"content\": \"Hi, welcome to Globant Enterprise AI!!\" } ], \"stream\": true\n}\n``` The expected result is to `stream` the translated content depending on the Prompt defined. ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"saia:assistant:translate-to-spanish\", \"messages\": [ { \"role\": \"user\", \"content\": \"Hi, welcome to Globant Enterprise AI!!\" } ], \"stream\": true\n}'\n``` ## Data Analyst Sample ```json\n{ \"model\": \"saia:assistant:myDataAnalystAssistant\", /* Using a DataAnalyst Assistant named 'myDataAnalystAssistant'. It can also be referenced by its \"assistantId\" value (uuid) instead of the name. */ \"messages\": [ { \"role\": \"user\", \"content\": \"What was the month with the highest Ancillaries sales?\" } ], \"stream\": true\n}\n``` ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"saia:assistant:myDataAnalystAssistant\", \"messages\": [ { \"role\": \"user\", \"content\": \"What was the month with the highest Ancillaries sales?\" } ], \"stream\": true\n}'\n``` ## Chat with Data Assistant Sample ```json\n{ \"model\": \"saia:assistant:ChatWithData\", \"messages\": [ { \"role\": \"user\", \"content\": \"How many covid deaths were there in Singapore 2022?\" } ]\n}\n``` ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"saia:assistant:ChatWithData\", \"messages\": [ { \"role\": \"user\", \"content\": \"How many covid deaths were there in Singapore 2022?\" } ], \"stream\": false\n}'\n``` If the request is processed successfully, the response is returned in JSON format, which includes two key components: the query, represented by `key = \u201cquery\u201d`, and the result of the execution, accessible through `key = \u201cdataSet\u201d`. ```json\n{ \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"{\\\"query\\\":{\\\"id\\\":\\\"db7c1e09-c059-4b96-9847-f174b1af6974\\\",\\\"name\\\":\\\"CovidDeathsInSingapore2022\\\",\\\"description\\\":\\\"Covid Deaths In Singapore2022\\\",\\\"expression\\\":\\\"Query CovidDeathsInSingapore2022 [OutputType='Table', Paging='False']\\\\r\\\\n{\\\\r\\\\n\\\\tSum(NewDeaths) [Name='Element01']\\\\r\\\\n\\\\t#filters\\\\r\\\\n\\\\t\\\\tCountryISO = 'SG'\\\\r\\\\n\\\\t\\\\tYear(Date) = 2022\\\\r\\\\n\\\\t#end\\\\r\\\\n}\\\",\\\"removeDuplicates\\\":false,\\\"outputType\\\":\\\"Table\\\",\\\"paging\\\":false},\\\"dataSet\\\":{\\\"id\\\":\\\"3dc46b4b-700a-42d2-8cd0-34669979c590\\\",\\\"pageNumber\\\":1,\\\"totalPages\\\":1,\\\"totalRows\\\":1,\\\"metadata\\\":[{\\\"name\\\":\\\"Element01\\\",\\\"type\\\":\\\"datum\\\",\\\"caption\\\":\\\"Sum of New Deaths\\\",\\\"dataType\\\":\\\"number\\\"}],\\\"data\\\":[{\\\"Element01\\\":\\\"386\\\"}]}}\" }\n}\n``` ## API Assistant Sample ```json\n{ \"model\": \"saia:assistant:test-openapi-weather-assistant\", \"stream\": false, \"messages\": [ { \"role\": \"user\", \"content\": \"weather in madrid\" } ]\n}\n``` ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"saia:assistant:test-openapi-weather-assistant\", \"messages\": [ { \"role\": \"user\", \"content\": \"weather in madrid\" } ], \"stream\": false\n}'\n``` ## RAG Sample ```json\n{ \"threadId\": \"uuid_as_string\", /* conversation identifier (optional) */ \"model\": \"saia:search:Default\", /* Using the Default RAG Assistant */ \"messages\": [ { \"role\": \"user\", \"content\": \"Summarize the features of Globant Enterprise AI\" } ], \"stream\": true\n}\n``` The expected result is to query the Default RAG Assistant and stream a reply once the sources are obtained. ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"saia:search:Default\", \"messages\": [ { \"role\": \"user\", \"content\": \"Summarize the features of Globant Enterprise AI\" } ], \"stream\": true\n}'\n``` More information: [RAG samples with Chat API](712.html) ## Flow Sample ```json",
          "evaluationResultRowContextDocId": "07864e36-b44e-42b1-97d9-8d18a85be63b",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "358bd274-4400-474b-b9d1-748bc437561d",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?34",
          "evaluationResultRowContextName": "34",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Conversational Module **Deprecated**: Since March 10th. Replaced by [Data Analyst Assistant 2.0](886.html). The Conversational Module of the [Data Analyst Assistant 1.0](89.html) facilitates the process of extracting the correct information, allowing the end user to interact with the data in a fluid and natural conversational manner. Before making a final query, the user can check glossary terms, learn about the assistant's capabilities, or review the structure of the data according to the metadata. This preparation enables more precise questions to be formulated and then sent for analysis and final presentation. The module improves the user experience by simplifying data interaction and eliminating the need for technical query knowledge. It makes data access more inclusive and efficient by automating query generation, which saves time and reduces manual effort. This streamlined approach facilitates quicker, more accurate data analysis. ## [Functionality](#Functionality) Once activated, the Conversational Module enhances user interaction with the Data Analyst Assistant by providing a natural conversational interface for data analysis. Here's how it works: - **End user Input:** The module receives end user input in text format. It also considers previous conversation history and the last code generated in the coder (if applicable).\n- **Context Analysis:** The module analyzes the end user's input in the context of the metadata and glossary.\n- **Query Generation:** Based on the context and end user input, the module generates an appropriate query to retrieve the requested data.\n- **Direct Response:** If the query can be answered directly from the assistant's metadata or glossary, the module will provide a direct response.\n- **Code Generation:** If the query requires calculations based on the assistant's dataset, the module will generate the necessary code to retrieve and process the data, then return the calculated result.\n- **Conversation History:** The module maintains a history of the conversation, allowing it to understand the context of subsequent questions and reformulate queries accordingly. ## [Activation](#Activation) To enable the Conversational Module, you need to configure the conversational_mode node in your [Metadata (JSON)](105.html). This node is parallel to the dataset definition nodes. Here's an example of how to configure the conversational_mode node: ```json\n{ \"conversational_mode\": { \"enabled\": 1, \"version\": 1, \"metadata_interpretation\": 1, \"question_reformulation\": 1 }, \"dataset_name_1\": {}, \"dataset_name_2\": {}\n}\n``` - `enabled`: Set to 1 to activate the Conversational Module.\n- `version`: Specifies the version of the module. The possible values are: - `1 (Smart Version)`: Reformulates the question and sends it to the process automatically, without user confirmation. - `2 (Basic Version)`: Reformulates the question and requests confirmation from the user before sending it to process and retrieve the data.\n- `glossary_interpretation`: Enables the module to interpret terms from the glossary.\n- `metadata_interpretation`: Enables the module to interpret metadata definitions.\n- `question_reformulation`: Enables the module to reformulate queries based on conversation history. To enable the Conversational Module in a previously defined Data Analyst Assistant, you need to go to the [Edit section](105.html) and select Replace Datasets in Update mode. You can then edit the Metadata (JSON) to add the configuration node for the Conversational Module. ## [Samples](#Samples) **End user asks:** \"What are the sales figures for product X?\" **Module:** If the metadata requires a date range for sales data, the module will prompt the user to specify the desired date range. **End user asks:** \"Can you show me the top 5 selling products?\" **Module:** The module will generate a query to retrieve the top 5 selling products based on the available data. **End user asks:** \"What is the average price of product Y?\" **Module:** The module will generate a query to calculate the average price of product Y.",
          "evaluationResultRowContextDocId": "01b3981d-faa9-454f-924e-51e9c70a6c80",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "500daa15-62db-487f-b81a-db80660aba09",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?291",
          "evaluationResultRowContextName": "291",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "is defined; for example, \"lastUserInput\". This variable contains the user's most recent input and will be used as the basis for the query on the selected documents or tags. Similarly, in the Output section, there is also a Variable field that specifies the variable in which the output generated by that node will be stored. In this case, we are interested in saving the RAG Assistant\u2019s response in a new variable called lastResponse. Therefore, the Use as response option is disabled. Clicking on the pencil icon to the right of the node opens the \"State Configuration\" menu where you can modify the following items: - **Acceptable confidence level**: The default value is 0.2, which indicates the level of confidence in the match of the chunks that the RAG obtains after searching the vector database. Only responses that meet or exceed this confidence level will be considered valid.\n\n- **Include conversation history**: Enabled by default. It ensures that the conversation history is included in the context of the interaction, allowing the assistant to take previous messages into account when generating responses.\n- **Include context variables**: Enabled by default. It allows context variables stored during the conversation to be sent to the assistants so that they can be used within the prompts.\n- **Show sources**: Enabled by default. It allows the assistant to show the sources of the information used to generate the response, which helps to provide transparency and to validate the accuracy of the response.\n- **Handle empty response**: Disabled by default. It handles situations where the assistant is unable to generate a valid response. When enabled, it opens a new flow branch (as shown in the image), allowing you to configure specific actions to follow in case no response is available. In this new branch, you can add any of the components available in the Interactions menu on the left to continue the interaction as needed. ![Handle Empty Response](./images/337.png) In this case, if the RAG Assistant finds the answer in the documents, you want the flow to call an assistant to translate the response to Japanese and store it in the same input variable (lastResponse). If no answer is found, the flow goes to a second RAG Assistant configured with documents used in Globant Enterprise AI courses. The variable that goes into the RAG Assistant is lastUserInput, which contains the user's last input. The response generated by this RAG Assistant is stored in the lastResponse variable. ![RAG Assistant Configuration](./images/338.png) ### Go To Node Finally, the Go To node is used to connect different flows within the interaction. This node allows you to select the flow to which you want to redirect the end user based on their current interaction. ![Go To Node](./images/339.png) Clicking on the blank bar (corresponding to the drop-down menu) displays a list of available flows: ![Available Flows](./images/340.png) Here you can select and define the flow to which you want to redirect the user. To review or modify any of these flows, you can access the Flows menu on the left side of the screen. In addition, \"Pick from canvas\" allows you to select a node directly from the canvas, facilitating navigation and connection between different parts of the Flow. The 'Go' icon next to the node allows you to navigate directly to the node to which it is connected. In this case, there are two 'Go To' nodes: - The first 'Go To': Returns to the 'User Input' node. Since the previous node (which translates to Japanese) does not store the response in a new variable and simply redirects to the 'User Input', the response received by the end user is the answer to their question translated to Japanese.\n- The second 'Go To': Sends the response to the 'Assistant' node, which is responsible for translating the generated response to Japanese using the lastResponse variable. This variable contains the response obtained from either of the two RAG Assistants previously executed in the flow. In this way, the flow ensures that the end user receives the response in Japanese, regardless of whether the response was generated by the first RAG Assistant or the second. ![Flow Diagram](./images/341.png) ## Step 3: Test flow Once you have configured the flow, make sure to click on the 'Save' button located at the top left of the screen. To test the flow you have created, you have three options available from the same design window: ![Test Flow Options](./images/342.png) 1. Click on the arrow icon (similar to the play button) to run the flow ('Run Flow').\n2. Click on the message icon to open the testing assistant (\u201cOpen Testing Assistant\u201d). ![Testing Assistant](./images/343.png)\n3. Click on the icon to open in a new window (the box with an arrow) to access the demo page (\u201cGo to Demo Page\u201d). ![Demo Page](./images/344.png) The first two options will open a window on the right side of the screen, while the third option will open a new window in the browser.\n\n# How to create a Flow This is a step-by-step guide to create and test an Assistants [Flow](321.html). ## Step 1: Create New First, log in to the [Globant Enterprise AI Backoffice](42.html). In the Project Dynamic combo box, select the project you want to work with (in this case, Default is used). Next, on the left side of the screen, you will find the backoffice menu. In this menu, click on Flows. ![Flow Menu](./images/324.png) By clicking on Flows in the Globant Enterprise AI Backoffice, a new window opens in the browser with the Flow Builder, where the Flows associated with the selected Globant Enterprise AI project are created and managed. When accessing Globant Enterprise AI\u2019s Flow Builder for the first time, a welcome screen like the one shown in the image below is displayed. From this screen, you can start creating a new flow by clicking on the \"Create\" button. ![Welcome Screen](./images/325.png) When you click on \"Create\", a pop-up window opens in which you must fill in the following information: 1. **Name**: Descriptive name for the flow, allowing it to be easily identified.\n2. **Description**: This field is optional, but it is recommended to add a brief description of the flow to clarify its purpose or content.\n3. **Type**: Allows you to select the type of flow to create, with the options \"Chat Flow\" or \"Assistant Flow\". Selecting one of these types will determine specific configurations and characteristics of the flow. In this case, Assistant Flow is selected.\n\n4. **Select languages**: Sets the language in which the flow will be configured, and allows you to define the language of the hard-coded messages. Multiple languages can be selected, so the same message can be available in different languages. Once you have completed these fields, you can click on the \"Create\" button. ![Create Flow](./images/326.png) ## Step 2: Manage the Start flow Once you have clicked on \"Create,\" you will return to the welcome screen. There, click on the \"Manage\" button of the newly created flow to add the assistants you want and customize their behavior. ![Manage Flow](./images/327.png) Clicking on the \"Manage\" button displays the first flow, which is created automatically. This initial flow, called \"start,\" establishes a basic configuration that you can customize, adding or modifying interactions, messages and assistants according to your needs. ![Start Flow](./images/328.png) ### Start flow The initial flow, called \"start,\" is automatically generated with the ID input.welcome. ![Start Flow ID](./images/329.png) ### Start (Intent: input.welcome) Node The Start (Intent: input.welcome) node marks the beginning of the conversation flow. This node is activated every time the flow is triggered, and its main purpose is to start the flow and, from there, the other nodes that manage the interaction with the user are connected. To rename the start node, go to the left menu, find Flows > start, click on the three dots next to it, select \"Rename Flow\" and, after typing the new name, press Enter to save the change. ![Rename Flow](./images/330.png) ### Message Node The next node, Message, is in charge of sending the welcome message or any other initial message configured. In this case, the message is: \"Hello {name}, how can I help you?\". Here, {name} represents a dynamic variable that is filled with the user's name. The message displayed in this node is customizable, and the text can be formatted by selecting different styles, such as Normal, Heading 1, among others, from the drop-down menu that appears where it says \"Normal\". This allows you to adjust the text style according to your needs. ![Message Node](./images/331.png) On the top bar of the node, you can see several formatting buttons such as bold, italic, underline, among others, to customize the text. In addition, there are icons that allow you to add lists, links and emojis. The last button, which has the shape of a small drawer, allows you to insert dynamic variables in the message. Clicking on this button displays a list of the variables available in the system, and it is also possible to define new custom variables. This Message node can be deleted by clicking on the trash can icon to the right of the node. It is also possible to add a variant by clicking on the \"plus\" (+) button at the top right. ### User Input Node The flow then proceeds to capture the user input through the User Input node, storing it in the lastUserInput variable for later use. ### Assistant Node In the Assistant node, you must configure the assistant to perform specific actions. This node uses the information stored in the lastUserInput variable, which contains the text entered by the end user in the User Input node. ![Assistant Node](./images/332.png) To configure the assistant, click on the \"Select an assistant\" field. By default, \"No assistant selected\" will be displayed. From the drop-down menu, choose the assistant you wish to use. **Note**: The assistants available are those that have been defined in the associated project, which was configured in the first step. The assistants can be any of those defined in Globant Enterprise AI, including [Data Analyst Assistant](886.html), [API Assistant](110.html), [Chat with Data Assistant](159.html). Once selected, the Prompt field will display the message that defines the task of the assistant. This message is read-only and can\u2019t be edited. The assistant's response can be stored in the lastUserInput variable or in a new variable, depending on your needs. By default, the Use as response option is enabled, which causes the assistant's response to be used directly in the lastUserInput variable. If you prefer to store the response in a new variable, disable Use as response. This will enable the Output field, where you can select or create a new variable. ![Assistant Output](./images/333.png) In addition, you can enable the option to read the assistant's response in JSON format. By enabling this option, you will have the ability to map the fields of that JSON to variables previously defined in your configuration. This allows for more precise integration of the assistant's responses in more complex workflows or in systems that require a specific format for data handling. The pencil icon to the right of the node corresponds to the Edit button. By clicking on it, you can enable or disable the Include conversation history and Include context variables options. By default, Include conversation history is enabled. It allows the assistant to access the conversation history to generate more coherent and contextual responses. The Include context variables option is also enabled by default. This option sends to the assistant variables defined in the flow, such as previously captured data or configured values, so that they can be used within prompts. This node can be deleted by clicking on the trash can icon to the right of the node. In this case, the Assistant node is deleted: ![Delete Assistant Node](./images/334.png) ### Insert RAG Assistant or Assistant Node On the left menu, below the Interactions category, you will find the different components that can be added to the flow. In particular, in the GENERATIVE section, you can choose between adding an Assistant or a RAG Assistant. ![Insert RAG Assistant](./images/335.png) In this case, you need to add a RAG Assistant. To do so, click on the RAG Assistant option in the GENERATIVE section. Next, drag the component to the desired location in the flow. In this case, you can insert it between the User input and Go To nodes. When you add a new RAG node, you must click on the \"Select an assistant\" field. By default, the option \"No assistant selected\" will appear. From the drop-down menu, choose the RAG assistant you wish to use. Once you select the RAG assistant, the next field, \"Select a document\", allows you to choose a specific document that will act as the source for responding to the end user's request. If you select a document, the assistant will only use that document as a reference. ![Select Document](./images/336.png) Alternatively, you can use the \"Indicates the documents' tags\" field. Here you can add the tags defined in [Step 2: Upload RAG documents](60.html), which are used to filter the documents. This allows the assistant to search only in the documents that match the indicated tags to respond to the end user's request. In the Input section, a field called Variable is displayed. There, the variable to be used as input for that node",
          "evaluationResultRowContextDocId": "09ebe65a-fb9d-4f45-8146-084558fef786",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "d09442c0-f6e5-4859-b08a-2bfa34008a86",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?322",
          "evaluationResultRowContextName": "322",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.6,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        }
      ],
      "dataSetRowId": "6d9f36e6-2dbd-43e9-8275-da4420a909e8",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0040126000",
      "evaluationResultRowEndDate": "2025-05-13T11:15:07",
      "evaluationResultRowOutput": "To maintain the context of a conversation in the Chat API, you need to use the header **`saia-conversation-id` </B>in your API requests. This header acts as a conversation identifier, allowing the system to keep track of the conversation history within that session.\n\n- The value of `saia-conversation-id` can be any alphanumeric string.\n- If you omit this header, each message will be treated as a new conversation without context.\n- Conversations remain valid for 10 minutes of inactivity, after which they are deleted.\n- Changing the value of `saia-conversation-id` starts a new conversation with no history.\n- This is especially important for Flows, as it allows context to be maintained across messages.\n\nAdditionally, some assistant nodes in Flows have options to include conversation history and context variables, which allows assistants to generate more coherent and contextual responses by accessing previous messages and stored variables. These features are enabled by default in the Flow configuration for the Assistant Node.\n\nExample from the documentation on the header usage:\n\n```plaintext\n- Headers: \n  - saia-conversation-id: $CONVERSATION_ID\nThis header only applies to Flows. It is necessary to maintain the context of the conversation. If omitted, each message will be treated as a new conversation without context.\n```\n\nFurthermore, the RAG Assistants use prompts to compact or summarize the conversation history (configured via the History Prompt) to maintain relevant context efficiently when querying vector stores, which also helps manage conversation context.\n\nFor more detailed information on using the Chat API and maintaining context via the conversation ID header, see the Chat API documentation [here](https://wiki.genexus.com/enterprise-ai/wiki?34).\n\nAnd for maintaining context in Flows and assistants, including details on including conversation history and context variables, see the Flow creation and management docs [here](https://wiki.genexus.com/enterprise-ai/wiki?322).\n\n---\n\n**References: </B> \n- Chat API conversation context with `saia-conversation-id` header [2](https://wiki.genexus.com/enterprise-ai/wiki?34)  \n- Flow Assistant node options for conversation history and context variables [4](https://wiki.genexus.com/enterprise-ai/wiki?322)  \n- History prompt usage and compacting conversation history in RAG Assistant [5](https://wiki.genexus.com/enterprise-ai/wiki?297)",
      "evaluationResultRowStartDate": "2025-05-13T11:14:57",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "8740"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "``` ## Step 3: Creating the Flow Follow the steps described in [[322]] to create the Flow called Mall Chat Assistant with the following structure: [[364]] To add variables to the Flow, click on the Variable option in the LOGICAL section of the [[371##Flows menu|Flows Menu]]. Next, drag the component below the User input node into the flow. In this case, a variable has been added: today. **Today variable:** It is defined by clicking on the down arrow inside the variables node and selecting the option + Add new variable. Next, enter the name today and press Enter. Deactivate the Clear option. To set the value of this variable, click on the edit icon to the right of the node. In the left menu, click on the Open in new window icon, and in the window that opens (Value) add `$today`. ## Step 4: Implementing the Derivation Logic Every end user query must be evaluated to determine which assistant is the most appropriate to answer. This is implemented with IF conditions in the flow, where the end user's intentions are compared with the available assistants. To add the Conditional component, go to the Interactions section and, below LOGICAL in the Flow menu, drag the component and drop it between the Assistant and Go to nodes. [[365]] Dragging the Conditional node generates a branch with two paths: one labeled If and one labeled Else. This structure allows you to evaluate a condition and direct the flow according to the result. To add an additional condition, click on the Add condition option to the right of the first node of the branch: [[366]] After clicking on Add condition in the first node of the branch, an additional node of Expression type is added, just like the first one. These Expression nodes allow evaluating different conditional expressions. [[367]] To add the variable to be evaluated, click on the word Variable, located at the bottom of each node. In the first two nodes of Expression type, you can see how each condition evaluates the `orchestrationResponse` variable. Depending on the value of this variable, the flow is directed to different assistants: either to the Movie Ticket Assistant or to the Mall Shops Catalog. If neither of these conditions is met, the flow continues through the Else branch, where by default there is a Go to node. A Message node has been added before the Go to node so that the flow can display a specific message to the end user. Then, the corresponding nodes are added to handle the responses according to the evaluated condition. [[368]] A node of API Assistant type that is configured for the Movie Ticket Assistant and handles queries related to the movie theater in the shopping mall is placed below the first node of Expression type (which checks if `orchestrationResponse` is equal to \"Movie Ticket Assistant\"). A node of RAG Assistant type that is configured for the Mall Shop Catalog Assistant and provides information about the stores in the mall is added below the second node of Expression type (which checks if `orchestrationResponse` is equal to \"Mall Shops Catalog\"). Finally, below each node a Go to is added, which redirects back to the User Input node. Once the assistant has processed and answered the query, this allows the flow to return to the entry point to receive new user interactions, ensuring a continuous and fluid conversation. [[369]] ## Step 5: Testing and Adjusting the Mall Chat Assistant Once you have configured the flow, make sure to click on the 'Save' button located at the top left of the screen. This step is essential to save all changes made to the flow. To test the flow you have created, you have three options available from the same design window: 1. Click on the arrow icon (similar to the play button) to run the flow ('Run Flow').\n2. Click on the message icon to open the testing assistant ('Open Testing Assistant').\n3. Click on the open in new window icon (the box with an arrow) to access the demo page ('Go to Demo Page'). The first two options will open a window on the right side of the screen, while the third option will open a new window in the browser, allowing you to see how the flow runs in a demo environment. During testing, you can run various queries to make sure that the flow responds correctly. For example, you can ask \"Where can I purchase books?\" and the flow should correctly identify the query and direct it to the Mall Shop Catalog, where the end user will be informed about the availability of stores that sell books. Then, you can ask \"What movies can I watch?\" and verify that the flow correctly redirects to the Movie Ticket Assistant to get the list of available movies. If the answer is ambiguous, as in \"I would like to watch the last one\", the orchestrator should use the context of the conversation to identify the last movie mentioned and continue the interaction with the Movie Ticket Assistant. In situations where the query does not correspond to any assistant\u2014such as \"What is Linux?\"\u2014the orchestrator should handle the response using a fallback and provide the information directly from the model, so that the flow covers all possible end user interactions. These tests will allow you to adjust the flow as needed, ensuring that the Mall Chat Assistant runs efficiently and delivers a smooth and accurate experience for end users. {{{12}}}{{{17}}}##MARKDOWN##\n\n#How to create a Flow with Conditional component This is a step-by-step guide to create a Mall Chat Assistant using [[321|Flows]] in Globant Enterprise AI. This Flow manages multiple assistants, which are specialized in different areas such as querying store catalogs, booking movie tickets, and orchestrating conversations. The steps required to set up and test this assistant are described below. ## Step 1: Configuring the Work Environment To get started, first make sure you have access to the [[42]] and are familiar with the [[371|interface for creating Flows]]. This environment will allow you to create, manage, and orchestrate the various assistants that make up the Mall Chat Assistant. ## Step 2: Creating Assistants ### Mall Shop Catalog Assistant This [[44|RAG Assistant]] is designed to provide information about the stores in the mall. The objective is to allow end users to check the availability of products or services in the different stores of the mall. The assistant is configured to answer questions related to the location, products, and opening hours of the stores. It uses a simple JSON document containing the store name and a short description. ### Movie Ticket Assistant This [[110]] allows end users to search the movie listings available in the mall and book tickets. It is based on the integration with an API that simulates the availability of movies in a movie theater. This assistant must be able to handle questions about schedules and reservations. ### Orchestrator Mall Assistant This [[566##Chat Assistant creation|Chat Assistant]] manages and redirects conversations to the appropriate assistant. The orchestrator is responsible for understanding the end user's intent and redirecting the conversation accordingly. It uses a system of predefined prompts to redirect queries to the corresponding assistant. The orchestrator keeps the context of the conversation, so the interaction is fluid and coherent. It defines variables such as user name and date. Orchestrator Mall Assistant Prompt: ```\nAssistant is a large language model.\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions.\nAdditionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\nAssistant is a helpful Mall Customer Service agent; you are kind but you always follow the rules. CONVERSATION CONTEXT ---\nThe user name is: {name} Today is: {today} OTHER ASSISTANTS\n---\nAssistant can ask the user to use other Assistants to look up information that may be helpful in answering the user\u2019s original question. The other Assistants the human can ask are:\n> Mall Shops Catalog: This Assistant provides information about shops inside the mall.\n> Movie Ticket Assistant: This assistant provides information about the movie theater inside the mall. RESPONSE FORMAT\n---\nWhen responding to me, if you want the human to ask another Assistant JUST ANSWER WITH THE ASSISTANT NAME AND NOTHING ELSE.\nFor any other response, you can create an HTML fragment with RICH TEXT if it's needed.",
          "evaluationResultRowContextDocId": "57a2c916-1a55-4579-abb9-419226e4e2c1",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "6feb838b-badf-4af9-8756-7a14b4eedc45",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?363",
          "evaluationResultRowContextName": "363",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# User's Guide for Globant Enterprise AI Flow Builder Globant Enterprise AI Flow Builder provides a visual tool that allows building and configuring conversational flows. ![Image](./images/372.png) ## Top Toolbar The Top Toolbar in the Globant Enterprise AI Flow Builder offers several options that allow you to manage and navigate between flows, as well as access key functionalities when building conversational flows. ![Image](./images/373.png) ### Current Flow Name This drop-down menu displays the name of the flow you are working with. In this case, the current flow is Documentation Assistant. From this menu, you can: - Quickly access other flows created.\n- Return to the main Flows window by clicking on the \"See All\" option. ### Language Change It allows you to change the language of the Flow Builder. In this case, it appears as \"En\" to indicate English. This option is available only if several languages have been selected in the [Select languages](322.html) option when creating the flow. ### Profile It displays the name of the project you are working on and provides the option to log out. ### Go to Demo Page It opens a new window that allows you to test the flow in real time, making it easier to verify the assistant's behavior and responses before implementing it. ### Open Logs Console It opens the Open Logs Console option, where you can access real-time logs of the assistant execution. This is especially useful for debugging tasks and detailed analysis of flow behavior. ### Open Testing Assistant It opens an interactive chat to test the flow in real time, helping to verify how the assistant responds under different scenarios before final implementation. ## Side Navigation Menu On the left side of the window is the Side Navigation Menu. This menu is a collapsible section that is hidden by clicking on the 'Collapse' option at the bottom. It provides quick access to various options: ![Image](./images/374.png) ### OVERVIEW Provides key information about the flow you are managing. Here you can see: ![Image](./images/375.png) - The name of the flow. In this case, \u201cDocumentation Assistant\u201d.\n- The Bot id and Project id, which are unique identifiers of the related bot and project, useful for internal reference and management.\n- The flow description, which is the text entered at the time of creation to detail its purpose or function. In addition, the section includes options to edit the flow name and add members. ### BUILD This section groups the tools that allow you to build and manage the flows and variables used in the assistants: - Flows: From here you can view, create, edit or manage conversational flows.\n- Variables: Allows you to view and manage the global and local variables that will be used in the flows. ### LOGS/DIAGNOSTICS This section is dedicated to the analysis and diagnosis of the assistant's behavior, providing access to interaction logs: - [Conversations history](378.html): Displays the history of all conversations between users and the assistant. It is useful to review how the interactions went and to make adjustments if necessary. ### ANALYTICS This section provides tools to analyze the performance and effectiveness of the assistant using different metrics: - [User Metrics](380.html): Presents metrics related to users, such as number of interactions and frequency of use.\n- [Message Metrics](382.html): Displays metrics related to the messages exchanged, providing insight into the effectiveness of the assistant's responses. ### CONFIGURATION This section allows you to adjust the general settings of the assistant and its environment: - [Configuration](384.html): Accesses the global system settings, where you can find the options Application keys to manage authentication keys, Integrations to configure connections with other platforms, Bot versioning to manage bot versions, and Danger Zone to export, import or restore bot configurations. ## Flows menu This menu has two main sections: Flows and Interactions. ![Image](./images/376.png) 1. [Flows Section](389.html): This section lists all available flows and appears by default when selecting a flow from the welcome screen. However, it is only displayed when the Flows option is selected in the Side Navigation Menu (the leftmost menu on the screen). If another option is selected in the Side Navigation Menu, this section is hidden or other options relevant to the selection are displayed.\n2. Interactions section: This area provides a list of components that can be used to build the flow. Components include user input, conditions, variables, language changes, history restart, among others. ## Flow Creation Area This section shows the flow under construction, allowing you to design the interaction sequence of the different assistants. ![Image](./images/377.png) In the upper right part of the area, you will find several key tools: - Save: This button saves the progress made when building the flow.\n- Run Flow: It allows running the flow to test how it would work in a test environment.\n- Code View (icon {}): This icon allows you to switch to a code view, where you can view and edit the flow in code format. To navigate the flow, you can click and drag the diagram to the creation area. Zooming is controlled with the mouse scroll wheel or pinch gesture on the touchpad, adjusting the zoom level without affecting the vertical position within the flow.",
          "evaluationResultRowContextDocId": "2708af94-91ad-4cc5-8095-f02fe76415ba",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "96350250-3c8e-4cfe-a515-ca8a638e65ad",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?371",
          "evaluationResultRowContextName": "371",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "* **Body**: This section allows you to write the content of the email. You can format the text using basic editing options such as bold, italics, underline, lists and links. You can also insert emojis or variables. ### Spreadsheet Allows you to write data into a spreadsheet directly from a flow. You can use this node to store user responses or any other information in real time, organizing the data in specific columns of the spreadsheet. [[508]] To configure the Spreadsheet node, click on the Edit icon located on the right side of the node. Clicking on Edit will open a side menu where you can enter the Spreadsheet ID and the Sheet name. The Spreadsheet ID is obtained from the URL of the Google Spreadsheet, and the Sheet name corresponds to the specific tab within the file where you want to save the data, such as \"Sheet 1\". In addition, make sure to upload the JSON file with the service account credentials and that the service account has permissions on the spreadsheet. The node displays a drop-down field where you can select the variables to be written to the spreadsheet. You must arrange the variables in the order in which they appear in the spreadsheet. You can also add other context variables if necessary. ## RESPONSES The Responses section contains the nodes responsible for generating responses to the end user. [[509]] ### Message Sends custom messages within a flow, such as an initial greeting. For example, you can use: \"Hello {name}, how can I help you?\", where {name} is a dynamic variable that is filled with the user's name. [[510]] This message is fully customizable. You can format the text using different styles, such as Normal or Heading 1, and formatting tools such as bold, italics, lists, links and emojis. In addition, you can insert dynamic variables, and add lists, links and emojis. You can also add message variants. ### Image Adds images to interactions within a flow, displaying a space where you can upload or select an image. [[511]] In addition, you can make the content dynamic by clicking on the nut-shaped icon, allowing you to use Velocity Template Language (VTL) to create dynamic responses, adapting the image content to different contexts. Clicking on Add Image brings up a menu on the left side that allows you to upload an image by dragging it or by browsing for the file on your device. You can also enter a URL directly to use an online image. This menu offers options for adjusting the size of the image, including cropping or scaling the image, and setting its vertical and horizontal position, centering it as needed. ### Link Create rich links with a custom preview that includes a title, description, image and tag, providing additional context to engage the user. [[512]] If you choose to add an image, clicking on Add Image will open a menu on the left side where you can upload an image from your device or enter a URL to use an online image. This menu also allows you to adjust the size, vertical position and horizontal position of the image, in a similar way as with the Image component. In addition, you can make the content of the link dynamic by clicking on the nut-shaped edit button. This will allow you to use Velocity Template Language (VTL) to adapt the preview to different contexts and situations within the flow. ### Card Create interactive cards within a flow, where you can add an image, title, and short description. As in the Image and Link components, you can add an image by clicking on Add Image, which will open a menu on the left side with the same options as the Image and Link components. [[513]] The Card node includes one button by default, and you can add more if necessary. If you add several buttons, each one can be configured to connect to an External function or External link, or to define an event. When all buttons are configured as External function or External link, the flow follows a single branch. However, if any of the buttons is configured for an event, an additional branch will be generated in the flow for each button configured for events. This node allows you to enable or disable text interaction, keep the conversation active, or allow user input. You can also make the content dynamic in order to adapt the card to different contexts within the flow using Velocity Template Language (VTL). ### Video Adds the URL of a video, but YouTube videos are not supported. You can also paste the URL of an image in the Poster field to use it as a thumbnail or cover of the video. [[514]] It is possible to make the link content dynamic using Velocity Template Language (VTL) to adapt the video preview to different contexts and situations within the flow. In addition, the component offers the option of adding buttons to connect to an external link, define a specific event or action, and set variables that can be used as input in other interactions within the flow. Each additional button can create a new branch in the flow, unless it is configured to connect to an External function or External link, in which case it will follow the same branch. ### YouTube Allows you to insert YouTube videos into your flow. [[515]] To set up this node, paste the ID of the video in the \"Id\" field in the left menu, which appears when you click on the edit icon. Make sure to enter only the part of the URL that comes after the equal sign (=). For example, if the URL is `https://www.youtube.com/watch?v=h_5nxFWjmZc`, just enter `h_5nxFWjmZc` in the corresponding field. The video thumbnails are generated automatically, so there is no need to add an additional image. The component also offers the option to add as many buttons as you deem necessary. In addition, you can make the content dynamic using Velocity Template Language (VTL), which allows you to adapt the video presentation to different contexts and situations within the flow. ### Quick Reply Adds buttons that you can use to start actions or redirect to other flows. Each button is configured by entering its name and selecting an action option. The Event option allows you to direct the reply to the desired flow. With External Link, you can redirect the user to a website, taking them out of the current flow. Finally, the External function allows you to execute an external function. [[516]] This node also offers additional behavior settings. You can disable text interaction, keep the conversation active, or enable user input. By default, the Enabled User Input option is active, which means that, if you add multiple buttons, a branch will be generated in the flow for each button configured as an Event. Buttons configured as External function or External link will not create new branches and will follow the same path in the flow. ### Datepicker Allows you to select a date within a flow, displaying fields to choose the month, day and year. [[517]] It is possible to configure the format in which the date will be displayed, select which dates will be available, connect the date picker to a specific event, and assign\n\n#Interactions to Create a Flow The Interactions Section provides the necessary components to build custom [[322|flows]]. Here you will find tools to generate, integrate, and respond effectively to interactions in order to design flows that meet the needs of the end user. [[493]] Below are described the different categories of components available in the Interactions Section, including their functionalities and how they can be used to optimize the creation of flows in Globant Enterprise AI. ## GENERATIVE In the Generative category, you will find the assistants that use generative AI: Assistant and RAG Assistant. [[495]] ### Assistant Configures an assistant, such as [[886|Data Analyst Assistant]], [[110]], [[159]], among others, that have been previously defined in the selected project in the [[42]]. [[496]] Once the assistant is selected, the Prompt field will display the message defining the assigned task. This message is read-only and cannot be edited. To see the complete prompt, click on the configuration icon. This node always requires an input variable, such as `lastUserInput`, which receives the information provided by the user. The output variable, on the other hand, can be configured. By default, the assistant's response is stored in `lastUserInput`, but if you disable the Use as response option at the top of the node, you can set a different output variable. In addition, it is possible to save the response in JSON format. The node also allows you to include the conversation history and context variables to further customize the interaction. ### RAG Assistant It integrates an assistant of [[44|RAG]] type that has been previously defined in the selected project in the Globant Enterprise AI Backoffice. [[497]] After selecting the RAG assistant, you can define a document or tags that will act as a source for the responses. Variables are handled in the same way as in the Assistant node. That is, it always requires an input variable, and it is possible to configure a different output variable that can be in JSON format. You can also set the minimum confidence level for responses, include the conversation history and context variables, and choose to display the sources of information used. In addition, you can configure the actions to take when an answer is not found in the documents provided. ## LOGICAL This section includes logical components that allow you to control the flow of the conversation and manipulate variables or conditions within the flow. [[498]] ### User Input Collects user input and stores it in a variable for use in subsequent nodes. [[499]] ### Conditional Evaluates logical conditions written in [https://velocity.apache.org/engine/1.7/vtl-reference.html#ifelseifelse-output-conditional-on-truth-of-statements|Velocity Template Language (VTL)] to decide which path the flow will take. [[500]] When this component is added to the flow, it is divided into branches, which by default are one for If and another for Else. The If branch node is by default of Expression type, while the Else branch is of Go to type. It is possible to add more If conditions by clicking on Add condition. By doing so, a branch with a node of Expression type is added, just like the first one. These Expression nodes allow evaluating different conditional expressions. Each branch is driven by the result of the evaluated condition. Depending on the value of a variable, this allows the flow to be directed to different assistants or actions. If the condition is not met, the flow continues through the Else branch. Each branch is driven by the result of the evaluated condition. Depending on the value of a variable, this allows the flow to be directed to different assistants or actions. If the condition is not met, the flow continues through the Else branch. To configure the condition, select a variable and apply a relational operator to perform the comparison. The supported relational operators are: * Equal\n* Not equal\n* Less than\n* Less than or equal to\n* Greater than\n\nthe selected value to a variable. Additionally, this node offers options for configuring the state behavior. You can disable text interaction, keep the conversation active, or enable user input, which is enabled by default. Additionally, it is possible to make the content dynamic, which allows you to adapt the interaction with the date picker to different contexts within the flow. ### Dropdown Allows the end user to select an option from a predefined list. [[518]] To configure this node, click on Edit to the right of the node. Next, in the left menu configure Assign value to field, which can be either an entity or a variable. In addition, below Dropdown list, you must define the options to be offered. These options can be numbers, letters, phrases, or a combination of all of them. Each option can be associated with a specific response flow.\n\nOnce you have defined the options in the Dropdown list, to configure each value, go to the node and select each option in Select option. Next, in the left menu, configure the actions to perform when the user selects that option. These actions can include the execution of specific flows or any other logic you need to associate with the selected option. ### Carousel It works in a similar way to Cards, but with a different visual presentation that allows showing several cards in a rotating format. [[519]] It allows you to add an image, title, and description for each card. In addition, you can trigger an event by clicking on the CTA action button, which can redirect to another flow. It is also possible to trigger an external event that takes to another website or page, or execute an external function.\n{{{17}}}{{{12}}}\n##MARKDOWN##\n\n* Greater than or equal to **Note:** The variable must be assigned before performing the comparison. This can be done in the Variable component or any other component that allows assigning a value to a variable, such as Assistant or RAG Assistant. ### Variable This node is used to manipulate variables, allowing you to set, modify or query values to be used in the flow. [[501]] To define a variable, click on the down arrow inside the node and select the + Add new variable option. Next, enter the name of the variable and press Enter. By default, the Clear option will be enabled, but you can disable it if you want to keep the current value of the variable. To set the value of this variable, click on the edit icon to the right of the node. This will open a menu on the left with several options. One of the available options is Get from request extra data, which assigns the value of an extra property of the request data to the variable, or Set to response extra data, which assigns the value of the variable to a property of the response data. In the Value window, you can dynamically set the value of the variable using **Velocity Template Language (VTL)**. Here you can enter expressions such as $today to assign the current date to the variable. This allows you to generate and manipulate data dynamically within the flow. More information: [[834]]. ### Language The Language node allows you to change the language of the interaction according to the user's needs or the context of the flow. [[502]] ### Reset History Resets the conversation history, which is useful for starting a new interaction without previous responses influencing the new flow. [[503]] In addition, there is a Reset Context option, which is disabled by default. If you enable this option, it will not only reset the conversation history, but also reset all context variables associated with the flow. This means that any data or values previously stored in the flow variables will also be reset. ### Script Allows you to insert JavaScript code that runs on the server side, providing flexibility to implement specific functionalities not covered by standard system components. This is useful when you need a custom solution for a particular bot or case before a new component is developed. [[837]] When you insert this component, you'll see an area where you can add a description and an \"Edit Code\" button. Clicking on this button opens a code editor with a dark-themed workspace where you can write and save your JavaScript code. This component allows you to access context variables using the context object, and gives you the ability to define and use custom functions within the script. In addition, you can add debug logs using the console object to facilitate code development and validation. For detailed examples and more information on how to use this component, see [[841]]. ### Go To Connects different flows or nodes, facilitating end user navigation and redirection to different parts of the flow as needed. [[504]] When inserting a Go To in the flow, you can select the flow to which you want to redirect the user by clicking on the drop-down bar. This list shows all the available flows you can choose from. In addition, the Pick from canvas option allows you to visually select a node directly from the canvas, creating a direct connection between the nodes. The Go icon next to the Go To node allows you to navigate directly to the node to which it is connected, making it easy to review and modify connections. ## INTEGRATIONS Through the Integrations section, you can integrate the flow with other external services or systems. [[505]] ### API The API component allows you to provide responses to users using data from a server. [[506]] In this node, you can define both Parameters and Responses, depending on the configuration of your API. The Parameters option allows you to add parameters that will be sent together with the API request, such as required identifiers or filters. From Responses, you can configure how the data received from the API will be handled and processed within your flow. To configure your API within the node, you need to enter the API URL. To do so, click on the Edit icon located to the right of the node, and then click on the Expand icon next to the URL field, represented by a box with an up arrow. This is where you can paste the API address. Once the URL is set, you can select the method of the request from the drop-down menu below Method. Options include GET, POST, PUT, and DELETE, depending on the operation you need to perform. If you only want to retrieve information, select GET. In the Headers section, you can add additional settings such as key-value pairs to indicate how the response should be handled. For example, select Accept with the application/json value to expect a response in JSON format. In addition, it is possible to add variables within this field by clicking on the word Variable. If you need to add more headers, use the Add Headers option. ### Email Sends emails as part of the interactions within the flow. This is useful for notifications or confirmations. [[507]] In this node, you can configure the following fields: * **Recipients**: This is where you enter the recipients' email addresses. You can add multiple recipients separated by commas. You can also press Enter after you type each address to add it, and then type the next one.\n* **Subject**: In this field, you define the subject of the email. It is possible to insert variables by clicking on the Variable option to customize the subject with dynamic data.",
          "evaluationResultRowContextDocId": "26b674b4-d387-4cb4-a16f-b97b7e0d8204",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "ce211ef3-0e8c-4a00-98c2-b91ea70f9ffd",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?494",
          "evaluationResultRowContextName": "494",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.8,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "6ebf466d-8a42-4cb4-91a4-69c010813c43",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0034631800",
      "evaluationResultRowEndDate": "2025-05-13T11:15:14",
      "evaluationResultRowOutput": "The Conditional component in a flow evaluates logical conditions to decide which path the flow will take based on the result. It uses expressions written in Velocity Template Language (VTL) to perform these evaluations.\n\n### Key features of the Conditional component:\n- When added to a flow, it creates multiple branches: by default, one for **If </B>and another for **Else**.\n- You can add more **If </B>conditions by clicking on \"Add condition,\" which adds additional branches, each with an Expression node.\n- Each Expression node evaluates a condition based on variables and relational operators (such as Equal, Not equal, Less than, Greater than, etc.).\n- The condition is evaluated against a variable that must be assigned a value before the comparison.\n- Depending on whether the condition is met, the flow branches to the corresponding path.\n- If none of the **If </B>conditions are met, the flow continues via the **Else </B>branch.\n- This component allows the flow to be directed dynamically to different assistants or actions based on the evaluated conditions.\n\nThis mechanism provides a way to control the flow logic and route conversations or processes depending on user input or variable states.\n\nFor example, in a Mall Chat Assistant flow, conditions can evaluate a variable such as `orchestrationResponse` to decide whether the query should be routed to a Movie Ticket Assistant, a Mall Shops Catalog, or handled as a fallback in the **Else </B>branch.\n\nMore technical details and usage examples are described in the documentation here:  \nhttps://wiki.genexus.com/enterprise-ai/wiki?494 and https://wiki.genexus.com/enterprise-ai/wiki?363 [2][4]\n\n---\n\nIf you need guidance on how to set up or configure conditions specifically, I can provide more details.",
      "evaluationResultRowStartDate": "2025-05-13T11:15:07",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "7737"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#How to upload and manage Files via API It is possible to work with previously uploaded files to perform various operations such as summaries or analysis. To perform these operations, first it is necessary to upload the file using the [[450]], confirm the success of the operation, and then reference it in the requests to the models that require the file. Here is a step-by-step guide on how to upload and manage files using the File API. ## Step 1: Uploading a File Using the File API\nThe first step is to upload the file you want to manage. To do this, use the [[450##POST /files|POST /files]] endpoint of the File API. This endpoint allows you to upload a file and receive a confirmation that the file has been successfully uploaded. The name of the file, which will be used to reference it, can be specified in the `fileName` parameter or, if not provided, it will be the name of the uploaded file. ### Getting your projectId and organizationId You can get your `$PROJECT_ID` and `$ORGANIZATION_ID` using the [[22##GET /accessControl/apitoken/validate|GET /accessControl/apitoken/validate]] endpoint of the [[22]]. Keep in mind that you can replace `$SAIA_ORGANIZATION_APITOKEN` with your `$SAIA_PROJECT_APITOKEN` if needed. ### Sample cURL Command\nTo load a file, use the following sample:\n```bash\ncurl -X POST \"$BASE_URL/v1/files\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: multipart/form-data\" \\ -H \"fileName: $FILE_NAME\" \\ -H \"folder: testFolder\" \\ -H \"organizationId: $ORGANIZATION_ID\" \\ -H \"projectId: $PROJECT_ID\" \\ -F \"file=@/path/to/your_file.mp4\" ``` If you include the `fileName` parameter, that will be the name you will use to reference the file later. If you don't include it, you will need to use the original name of the uploaded file. For example, if you upload `\"file=@/path/to/your_file.mp4\"`, the reference name will be exampleFile (without the extension). ## Step 2: Validating the File Upload\nOnce the file has been successfully uploaded, the response will include information confirming the success of the operation. You don't need to use a dataFileId, but you can check the response to ensure the file was uploaded correctly. ### Sample response\n```json\n{ \"dataFileId\": \"string\", \"dataFileUrl\": \"string\", \"success\": true\n}\n```\n## Step 3: Create a Chat Assistant (if not created already) After uploading the file, you need to create a [[708]] to process the uploaded content, if you haven't already created one. This is done through the [[35##POST/assistant|POST /assistant]] endpoint of the [[35]]. ### Sample cURL Command ```bash\ncurl -X POST \"$BASE_URL/v1/assistant\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"type\": \"chat\", \"name\": \"vertex_ai/gemini-2.0-flash-exp -Test File Upload\", \"prompt\": \"You are a helpful assistant\", \"llmSettings\": { \"providerName\": \"vertex_ai\", \"modelName\": \"gemini-2.0-flash-exp\", \"temperature\": 0.10, \"maxTokens\": 8192, \"uploadFiles\": true } }'\n``` ## Step 4: Referencing the File Once the Chat Assistant is set up, you can reference the file in a request to the Assistant for analysis or summary, using the [[34]] and specifying the file name in the request body. ### Sample cURL Command to Summarize a File: ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"saia:assistant:vertex_ai/gemini-2.0-flash-exp -Test File Upload\", /* Using the assistant created in Step 3 named 'vertex_ai/gemini-2.0-flash-exp -Test File Upload' */ \"messages\": ~~[~~ { \"role\": \"user\", \"content\": \"Provide the content {file:$FILE_NAME}\" } ~~]~~, \"stream\": false }'\n``` ## Alternative: Inquire About Files - Interact Directly with LLMs You can also interact with LLMs directly, asking questions about files like images or videos. [regions]\n[region|cURL]\nSample 1: Ask GPT-4o About an Image\n```bash\ncurl --location '&lt;BASE_URL&gt;/chat' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;SAIA_PROJECT_APITOKEN&gt;' \\\n--data '{ \"model\": \"openai/gpt-4o\", \"max_tokens\": 1024, \"temperature\": 0.1, \"messages\": ~~[~~ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": ~~[~~ { \"text\": \"Tell me what this is about?\", \"type\": \"text\" }, { \"image_url\": { \"url\": \"&lt;IMAGE_FILE_URL&gt;\" }, \"type\": \"image_url\" } ~~]~~ } ~~]~~\n}'\n```\n[/region]\n[region|Python]\nSample 2: Python Code - Sending Gemini a Video File for Analysis\n```python\nimport requests\nimport json\nimport base64 # Define the API endpoint and authentication headers\nurl = \"&lt;BASE_URL&gt;/chat/completions\"\nheaders = { 'Content-Type': 'application/json', 'Authorization': 'Bearer &lt;SAIA_PROJECT_APITOKEN&gt;'\n} # Function to encode a file in Base64 format\ndef encode_file(file_path): \"\"\"Encodes a file in Base64 format for API submission.\"\"\" with open(file_path, 'rb') as file: return base64.b64encode(file.read()).decode('utf-8') # Replace 'path_to_your_file.mp4' with the actual file path\nfile_path = \"path_to_your_file.mp4\"\nencoded_file = encode_file(file_path) # Construct the API request payload\npayload = { \"model\": \"vertex_ai/gemini-1.5-pro-002\", \"messages\": ~~[~~ { \"role\": \"user\", \"content\": ~~[~~ { \"type\": \"text\", \"text\": \"What's this file about?\" }, { \"type\": \"image_url\", \"image_url\": f\"data:video/mp4;base64,{encoded_file}\" } ~~]~~ } ~~]~~\n} # Send the request to the API\nresponse = requests.post(url, headers=headers, json=payload) # Print response details\nprint(\"Status Code:\", response.status_code)\nprint(\"\\nResponse:\")\nprint(json.dumps(response.json(), indent=2))\n```\n[/region]\n[/regions]\n{{{12}}}{{{17}}} ##MARKDOWN##",
          "evaluationResultRowContextDocId": "94fd1b65-0ca6-4b3c-8acf-eed9148e7023",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "2db02359-ef1f-4721-9753-2fa521f09cf0",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?454",
          "evaluationResultRowContextName": "454",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# .web File Format The `.web` file format is a JSON file with a `.web` extension used to configure website crawling for [RAG Assistant](44.html). ### .web File Format Parameters The following parameters are available in the `.web` file format: | **Element Name** | **DataType** | **Description** |\n|---|---|---|\n| url | string | Mandatory: the base URL to crawl. |\n| excludeDirs | string[] | Optional list of web page directories to exclude; they must be absolute references. Here you can exclude unnecessary dependencies referenced in the HTML. |\n| maxDepth | number | The maximum depth to crawl. By default, it is set to 3; the system controls it cannot have a higher value. |\n| timeout | number | Timeout for each request, in the unit of milliseconds. By default, it is set to 10000 (10 seconds). |\n| preventOutside | boolean | Whether to prevent crawling outside the root base URL; _true_ by default. |\n| selectors | string[] | List of page selectors to consider; it uses the _body_ tag by default, and can be used in the following combinations: <ul><li>`*` - universal selector;</li><li>`div` - tag name;</li><li>`.foo` - class name;</li><li>`#bar` - id;</li><li>`[baz]` - attribute presence;</li><li>`[baz=buzz]` - attribute value (with any operators and also quotes and case sensitivity modifiers - [syntax](https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors#syntax);</li><li>`+` and `>` combinators (other combinators are not supported).</li></ul> It is recommended to analyze some of the web pages of the site and select the main container where the valuable text is set. |\n| linkBrackets | boolean | Add link brackets when processing an anchor tag; _false_ by default. |\n| ignoreHref | boolean | Remove href references; _false_ by default. | ### Using the .web File Format Once the file is uploaded (as shown in [Upload documents](60.html)), the ingestion process will crawl the site starting in the base URL (_url_ parameter) considering the _maxDepth_ parameter (to follow associated links in each page up-to _maxDepth_) and will exclude all base URLs (specified in the _excludeDirs_ parameter). When the process is finished, the crawl status will be displayed as usual, in this case associated to the uploaded file (in general as _Success_): ![Crawl Status](./images/292.png) You can check the [Chunks file](60.html) with the result of the processing, and validate that the detected _plain text_ by the crawler makes sense (to be used as semantic information for a RAG assistant). ### Sample Suppose you want to crawl a Wikipedia article. In this case, a possible `wikipedia_sample.web` file to upload could be configured as follows: ```json\n{ \"url\": \"https://en.wikipedia.org/wiki/GeneXus\", \"maxDepth\": 1, \"preventOutside\": true, \"timeout\": 10000, \"selectors\": [ \"#firstHeading\", \"#bodyContent\" ], \"linkBrackets\": false, \"ignoreHref\": true, \"excludeDirs\": [ \"https://en.wikipedia.org/static/\", \"https://en.wikipedia.org/w/\" ]\n}\n``` Note the following: - **url:** The base URL of the Wikipedia article.\n- **maxDepth:** Only crawl one level of the site.\n- **selectors:** Use the selectors by ID (`firstHeading`, `bodyContent`) to only process the important content from the site.\n- **linkBrackets:** Remove link references to keep the plain text as clean as possible.\n- **excludeDirs:** Exclude a couple of directories. #### Resulting Chunks File The resulting Chunks file will look something like this: ```json\n[ ... { \"pageContent\": \"GENEXUS From Wikipedia, the free encyclopedia Computer programming tool\\nThis article is about a computer programming tool. For the Fear Factory album, see Genexus (album). GeneXusDeveloper(s)GeneXusInitial release1988; 36 years ago (1988)Stable release\\nGeneXus 17\\n...\", \"metadata\": { \"source\": \"https://en.wikipedia.org/wiki/GeneXus\", \"language\": \"en\", \"description\": \"GeneXus - Wikipedia\", \"id\": \"someGUID\" } }, { \"pageContent\": \"...can be generated include COBOL, Java, Objective-C, RPG, Ruby, Visual Basic, and Visual FoxPro.[3] Some of the DBMSs supported are\\nMicrosoft SQL Server, Oracle, IBM Db2, Informix, PostgreSQL, and MySQL. GeneXus was developed by Uruguayan company ARTech Consultores SRL which later renamed to Genexus SA.[4] The latest version is...\", \"metadata\": { \"source\": \"https://en.wikipedia.org/wiki/GeneXus\", \"language\": \"en\", \"description\": \"GeneXus - Wikipedia\", \"id\": \"someGUID\" } } ...\n]\n``` ### See Also [.custom File Format](201.html)",
          "evaluationResultRowContextDocId": "9020065f-bb77-40f1-8f88-9aeeac036331",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "52381059-2339-42c5-adfb-f5fedfeb1396",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?202",
          "evaluationResultRowContextName": "202",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#RAG Assistants API This API allows you to define different [[44|RAG Assistants]]. To use the API, check the [[20##Generic Variables|generic variables]] needed. In addition, read the explanation about parameters in [[64]]. > The following endpoints require a Globant Enterprise AI API token related to **project** scope. ## Endpoints Below is a summary of the available endpoints for this API: | Method | Path | Description |\n|---|---|---|\n| `GET` | `/profiles` | Gets all RAG assistants from a project |\n| `GET` | `/profile/{name}` | Gets a specific RAG assistant |\n| `POST` | `/profile` | Creates a new RAG assistant |\n| `PUT` | `/profile/{name}` | Updates a RAG assistant |\n| `DELETE` | `/profile/{name}` | Deletes a RAG assistant |\n| `GET` | `/profile/{name}/documents` | Gets documents for a RAG assistant |\n| `PUT` | `/profile/{name}/documents`<sup>(1)</sup> | Reindex all documents for the selected RAG assistant |\n| `DELETE` | `/profile/{name}/documents` | Deletes all documents for a RAG assistant |\n| `GET` | `/profile/{name}/document/{id}` | Retrieves Document information |\n| `POST` | `/profile/{name}/document` | Uploads a Document |\n| `PUT` | `/profile/{name}/document`<sup>(1)</sup> | Reindex a Document |\n| `DELETE` | `/profile/{name}/document/{id}` | Deletes a Document |\n| `POST` | `/execute` | Executes a query against the defined RAG assistant | <sup>(1)</sup> - Supported since `2025-05` release. **Note**: Keep in mind that the `searchProfiles` and `{name}` parameters refer to RAG Assistants. In addition, the `{name}` parameter, which represents the name of the RAG Assistants, must not contain blanks. Instead, they must be replaced by the + symbol. ## GET/profiles Retrieves all the RAG Assistants for a Project. ### Response ```json\n{ \"projectActive\": boolean, \"projectDescription\": \"string\", \"projectId\": \"string\", \"projectName\": \"string\", \"projectStatus\": integer, /* 0:Active, 1:Deleted, 2:Hidden */ \"searchProfiles\": [ { \"description\": \"string\", \"name\": \"string\" }, ... ]\n}\n``` The `description` parameter is required for the [[33|chat]] option. ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/search/profiles\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"Accept: application/json\"\n``` ## GET/profile/{name} Gets RAG Assistants `{name}` details. ### Response ```json\n{ \"name\": \"string\", \"description\": \"string\", \"indexOptions\": { \"chunks\": { \"chunkOverlap\": integer, \"chunkSize\": integer } }, \"searchOptions\": { \"historyCount\": integer, \"llm\": { \"cache\": boolean, \"frequencyPenalty\": decimal, \"maxTokens\": integer, \"modelName\": \"string\", \"n\": integer, \"presencePenalty\": decimal, \"provider\": \"string\", \"stream\": boolean, \"temperature\": decimal, \"topP\": decimal, \"type\": \"string\", \"verbose\": boolean }, \"retriever\": { \"type\": \"string\" /* vectorStore, multiQuery, selfQuery, hyde, contextualCompression */ }, \"search\": { \"k\": integer, \"prompt\": \"string\", \"returnSourceDocuments\": boolean, \"scoreThreshold\": decimal, \"template\": \"string\" } }, \"status\": integer /* 1:Enabled, 2:Disabled */\n}\n``` The `type` parameter is explained in [[70|RAG Assistants - Configuration Retrieval]]. ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/search/profile/{name}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## POST/profile Creates a RAG Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) |\n| `descrition` | `string` | Description of the RAG assistant |\n| `template` | `string` | Name of an existing RAG to base the configuration (optional), empty by default | ### Request Body ```json\n{ \"name\": \"string\", /* Required */ \"description\": \"string\", \"template\": \"string\", \"searchOptions\": { \"embeddings\": { \"provider\": \"string\", \"modelName\": \"string\", \"useProxy\": boolean, \"mode\": \"string\", // \"\" or \"multimodal\" \"type\": \"string\", \"batchSize\": integer, // defaults to 100 \"dimensions\": integer // defaults to 1536 }, \"historyCount\": integer, \"llm\": { \"cache\": boolean, \"frequencyPenalty\": decimal, \"maxTokens\": integer, \"modelName\": \"string\", \"n\": integer, \"presencePenalty\": decimal, \"provider\": \"string\", \"stream\": boolean, \"temperature\": decimal, \"topP\": decimal, \"type\": \"string\", /* empty value (default) or json_object */ \"verbose\": boolean, \"guardrails\": [...] // options available are: \"input-moderation-guardrail\", \"llm-output-guardrail\", \"prompt-injection-guardrail\" }, \"search\": { \"k\": integer, \"type\": \"string\", /* similarity (default) or mmr */ \"fetchK\": number, /* valid when using mmr type */ \"lambda\": decimal, /* valid when using mmr type */ \"prompt\": \"string\", \"returnSourceDocuments\": boolean, \"scoreThreshold\": decimal, \"template\": \"string\" }, \"retriever\": { \"type\": \"string\", /* vectorStore, multiQuery, selfQuery, hyde, contextualCompression */ \"searchType\": \"similarity | similarity_hybrid | semantic_hybrid\" /* Azure AISearch specific, defaults to similarity */ \"step\": \"string\", /* all (default) | documents */ \"prompt\": \"string\" /* not needed when using vectorStore */ } }, \"indexOptions\": { \"chunks\": { \"chunkOverlap\": integer, \"chunkSize\": integer }, \"useParentDocument\": boolean, /* false by default */ \"childDocument\": { /* valid if the previous element is true */ \"childK\": decimal, \"child\": { \"chunkSize\": decimal, \"chunkOverlap\": decimal, \"contentProcessing\": \"string\" /* '' | 'clean' */ } } }, \"welcomeData\": { \"title\": \"string\", \"description\": \"string\", \"features\": [ { \"title\": \"string\", \"description\": \"string\" }, ... ], \"examplesPrompt\": [ { \"title\": \"string\", \"description\": \"string\", \"promptText\": \"string\" }, ... ] }\n}\n``` The LLM type option using *json_object* can be checked [[69##type|here]]. {{{216| If you do not provide the `WelcomeData` section when creating a RAG Assistant, it will be empty}}} ### Response Equivalent to [[29##GET/profile/{name}|Get Response]]. ### cURL Sample ```bash\ncurl -X POST \"$BASE_URL/v1/search/profile\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\" \\\n-d '{\n\"name\": \"my RAG assistant\",\n\"description\": \"My awesome profile\",\n\"searchOptions\": {\n\"historyCount\": 2,\n\"llm\": {\n\"temperature\": 0.1,\n\"maxTokens\": 1500,\n\"modelName\": \"gpt-3.5-turbo-16k\"\n},\n\"search\": {\n\"k\": 5\n}\n}\n}'\n``` #### Sample with WelcomeData ```bash\ncurl -X POST \"$BASE_URL/v1/search/profile\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\" \\\n-d '{\n\"name\": \"Test-Profile-WelcomeData\",\n\"description\": \"Test Profile with WelcomeData\",\n\"searchOptions\": {\n\"historyCount\": 2,\n\"llm\": {\n\"cache\": false,\n\"temperature\": 0.1,\n\"maxTokens\": 999,\n\"modelName\": \"gpt-3.5-turbo-16k\",\n\"verbose\": true\n},\n\"search\": {\n\"k\": 5,\n\"returnSourceDocuments\": false,\n\"scoreThreshold\": 0,\n\"prompt\": \"Use {context} and {question}\"\n}\n},\n\"indexOptions\": {\n\"chunks\": {\n\"chunkSize\": 999,\n\"chunkOverlap\": 99\n}\n},\n\"welcomeData\": {\n\"title\": \"Welcome to the RAG\",\n\"description\": \"It is a RAG created with WelcomeData via API\",\n\"features\": [\n{\n\"title\": \"Weather conditions\",\n\n{ \"type\": \"test\", \"domain\": \"Knowledge\", \"year\": 2023, \"quarter\": \"q3\"\n}\n``` The metadata can be uploaded as `Text` or directly from a `File`. #### Metadata special values Check for the following metadata special values | Item | **Description** |\n|---|---|\n| url | If you want that the document point to a specific absolute URL, add a *url* metadata element pointing to the complete url resource. |\n| description | Use this property if you want to personalize how the Frontend shows the sources section; when not present it will show the *name* property. | ### Response Equivalent to [[29##GET/profile/{name}/document/{id}|Get Response]]. Note that, once the document is uploaded, the `indexStatus` will be `Unknown` as it is queued to be ingested. Use the [[29##GET/profile/{name}/document/{id}|Get Response]] API to check the document status; the expected result is `Success`. Possible return errors: - [[23##2027|2027]]\n- [[23##2028|2028]] ### cURL Sample To upload a `SampleFile.pdf` file, you can follow these steps: ```bash\n# binary\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'filename: SampleFile.pdf' \\\n--header 'Content-Type: application/pdf' \\\n--data '@/C:/temp/SampleFile.pdf' # multi-part\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"Content-Type: multipart/form-data\" \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"' # multi-part with metadata as text\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"Content-Type: multipart/form-data\" \\\n--form 'metadata=\"{\\\"type\\\":\\\"test\\\",\\\"domain\\\":\\\"Knowledge\\\",\\\"year\\\":2023,\\\"quarter\\\":\\\"q3\\\"}\"' \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"' # multi-part with metadata as a File\ncurl -X POST \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"Content-Type: multipart/form-data\" \\\n--form 'metadata=@\"/C:/temp/upload_file_metadata.json\"' \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"'\n``` ## PUT/profile/{name}/document Reindex a Document (associated to an existing `documentId` GUID) to the associated `{name}` RAG Assistant. ### Request Headers | **Header** | **Description** |\n|---|---|\n| `documentId` | The GUID associated to an already existing document. | ### Request Form-data Body | **Key** | **Description** |\n|---|---|\n| `file` | The associated file to reindex (`optional`), if not present will use the previous existing file automatically. |\n| `metadata` | Use a metadata to reindex the content (`optional`); if not present will use the previous metadata; force the `{}` value if you want to clean up the associated metadata. | A sample metadata looks like this: ```json\n{ \"type\": \"test\", \"domain\": \"Knowledge\", \"year\": 2023, \"quarter\": \"q3\"\n}\n``` ### Response Equivalent to [[29##GET/profile/{name}/document/{id}|Get Response]]. Possible return errors: - [[23##2021|2021]] ### cURL Samples ```bash\n# reindex existing file\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'documentId: GUID' # Reindex existing file with new content\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'documentId: Document_GUID' \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"' # Reindex existing file with new metadata\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'documentId: Document_GUID' \\\n--form 'metadata=\"{\\\"type\\\":\\\"test\\\",\\\"domain\\\":\\\"Knowledge\\\",\\\"year\\\":2023,\\\"quarter\\\":\\\"q3\\\"}\"' # Reindex existing file with new content and metadata\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/document\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n--header 'documentId: Document_GUID' \\\n--form 'metadata=@\"/C:/temp/upload_file_metadata.json\"' \\\n--form 'file=@\"/C:/temp/SampleFile.pdf\"'\n``` ## DELETE/profile/{name}/document/{id} Deletes a Document. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) |\n| `id` | `string` | Document Id (required) | ### Response StatusCode `200` is shown when successfully deleted; otherwise, `400*` is displayed with a collection of errors. ### cURL Example ```bash\ncurl -X DELETE \"$BASE_URL/v1/search/profile/{name}/document/{id}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## POST/execute Executes a search query. For more details, read [[33]]. {{{12}}}{{{17}}}##MARKDOWN##\n\n\"description\": \"Obtain weather conditions in any country\"\n},\n{\n\"title\": \"Rain probability\",\n\"description\": \"Get the rain probability in any location\"\n}\n],\n\"examplesPrompt\": [\n{\n\"title\": \"First Prompt Example\",\n\"description\": \"First Prompt Example Description\",\n\"promptText\": \"You are an assistant specializing in...\"\n}\n]\n}\n}'\n``` ## PUT/profile/{name} Updates a RAG Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) | ### Request Body Equivalent to [[29##POST/profile|Post Request]], but the following elements are discarded: - `name` element\n- `indexOptions` section In addition, `status` element can be specified by taking values: - 1: Enabled\n- 2: Disabled If a RAG Assistants had `WelcomeData` and an update does not include this section, the existing information is deleted and becomes empty. Therefore, if you want to add new information to the `WelcomeData`, you must include both the old and the new information. If you only send the new one, it will completely replace the old one. ### Response Equivalent to Get Response. ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\" \\\n-d '{\n\"description\": \"Updated RAG assistant\",\n\"status\": 1,\n\"searchOptions\": {\n\"historyCount\": 4,\n\"llm\": {\n\"temperature\": 0.5,\n\"maxTokens\": 1000\n},\n\"search\": {\n\"k\": 2,\n\"prompt\": \"You are an Assistant, only reply using the following context:\\n{context}\\n Question is: {question}\\n\",\n\"scoreThreshold\": 0.2\n}\n}\n}'\n``` #### Sample with Welcome Data Update ```bash\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\" \\\n-d '{\n\"name\": \"Test-Profile-WelcomeData\",\n\"welcomeData\": {\n\"title\": \"Welcome to RAG Update\",\n\"description\": \"It is a RAG created with WelcomeData via API\",\n\"features\": [\n{\n\"title\": \"Updated weather conditions\",\n\"description\": \"Get the state of the weather in any country\"\n},\n{\n\"title\": \"Rain probability updated\",\n\"description\": \"Get the rain probability in any location\"\n}\n],\n\"examplesPrompt\": [\n{\n\"title\": \"First Prompt Example\",\n\"description\": \"First Prompt Example Description\",\n\"promptText\": \"You are an assistant specializing in...\"\n}\n]\n}\n}'\n``` ## DELETE/profile/{name} Deletes a RAG Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) | ### Response StatusCode `200` is shown when successfully deleted; otherwise, StatusCode `400*` is shown with a collection of errors. ### cURL Sample ```bash\ncurl -X DELETE \"$BASE_URL/v1/search/profile/{name}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## GET/profile/{name}/documents Lists the documents for a RAG Assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name |\n| `skip` | `integer` | Number of documents to skip |\n| `count` | `integer` | Number of documents to return (defaults to 10) | ### Response ```json\n{ \"documents\": [ { \"extension\": \"string\", \"id\": \"string\", \"name\": \"string\", \"timestamp\": \"timestamp\", \"url\": \"string\", \"indexStatus\": \"string\", /* Unknown, Starting, Failed, Pending, Success */ \"indexDetail\": \"string\" }, ... ], \"count\": \"integer\" /* Total number of documents */\n}\n``` ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/search/profile/{name}/documents\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n# Use the optional skip and count parameters\n$BASE_URL/v1/search/profile/{name}/documents?skip={skip}&count={count}\n``` ## PUT/profile/{name}/documents Reindex all existing documents for the RAG assistant. ### Response An empty Json with statusCode: 200. ```json\n{}\n``` ### cURL Sample ```bash\ncurl -X PUT \"$BASE_URL/v1/search/profile/{name}/documents\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n``` ## DELETE/profile/{name}/documents Deletes all documents associated to the specified RAG assistant. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) | ### Response StatusCode `200` is shown when successfully deleted; otherwise, StatusCode `400*` is shown with a collection of errors. ### cURL Sample ```bash\ncurl -X DELETE \"$BASE_URL/v1/search/profile/{name}/documents\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## GET/profile/{name}/document/{id} Using the `{name}` RAG Assistants, it gets details about the `{id}` document. ### Parameters | Name | Type | Description |\n|---|---|---|\n| `name` | `string` | RAG assistant name (required) |\n| `id` | `string` | Document Id (required) | ### Response ```json\n{ \"extension\": \"string\", \"id\": \"string\", \"indexStatus\": \"string\", /* Unknown, Starting, Failed, Pending, Success */ \"indexDetail\": \"string\", \"keyName\": \"string\", \"metadata\": [ { \"key\": \"string\", \"value\": \"string\" }, ... ], \"name\": \"string\", \"timestamp\": \"timestamp\", \"url\": \"string\"\n}\n``` For the *Failed* status, check the returned *Index Detail* field for further information. | **Index Detail** | **Description** |\n|---|---|\n| Invalid content | The associated file does not have text content; the common case is a PDF file composed of images. These files are not able to be used in a RAG Assistant. | ### cURL Sample ```bash\ncurl -X GET \"$BASE_URL/v1/search/profile/{name}/document/{id}\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"accept: application/json\"\n``` ## POST/profile/{name}/document Uploads a Document to the associated `{name}` RAG Assistants. Note that the file extension must be a [[137|supported one]]. ### Request Body The supported options are `binary` or `multipart/form-data` including a `File` type. #### Binary It is useful for its simplicity and encodes the binary data directly in the request body. Set the request with the associated `Content-Type` header to indicate the type of data being sent (e.g., `application/pdf`, `text/plain`). It is mandatory to set a `filename` header value with the document name and extension. For example: ```\nfilename: SampleFile.pdf\n``` Note that this option does not enable to upload [[60##Step 3: Upload documents|metadata]], for this case use the other upload option below (Form-Data). #### Form-Data This format allows you to include both binary data and other form fields in a single request. Each part of the data (binary file, text fields, etc.) is separated by a boundary and sent as separate parts. It is expected to be used for large files. You can also specify different [[581|ingestion options]] that will decide how files are processed. If you want to attach metadata to the file to be processed during ingestion, add a `metadata` form-data variable with the desired value; remember that the expected format is a `key/value` JSON list. For example, the following is a valid metadata for a Document: ```json",
          "evaluationResultRowContextDocId": "886dd74f-79ea-4603-afb3-b7ca8298555b",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "b9c4c8d7-5879-465c-9ea6-c487b1a9310d",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?29",
          "evaluationResultRowContextName": "29",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#File Support in Flows File support in Flows allows you to send several types of files, such as documents, images, videos, and audio, to different [[973|Agents]] within the [[321|Flow]]. Flows can also be exposed through multiple channels, including Web, WhatsApp, Teams, or Slack. This provides the ability to send files to one or more Agents through these channels. {{{216| File support for Teams and Slack has not been released yet, but will be included in future releases}}} ## Scenarios The end user will be able to: * Use a Flow to combine Agents that support and do not support file upload; for example, ask an Agent to describe an Image and then use the result with an Agent that only handles text. * Expose a Flow using an Agent with File support through one of the supported channels; for example, allowing the user to send audio from WhatsApp to the Agent. ## Functionality When the end user attaches a file to a Flow, that file will be sent to Agents within the Flow that support that file type. This means that: * If an Agent in the Flow does not support File upload, the file will NOT be sent to that Agent. * If an Agent in the Flow does not support the file type, the file will NOT be sent to that Agent. The supported file types are configured independently for the Flow and the Agent. File support in Flows is enabled by default. ## Configuration Each [[321|Flow]] has its own File configuration. You can enable or disable file attachment support and also define the supported file extension. To view the File support configuration, go to Configuration > Flows properties. [[1054]] ###File properties * **File support enabled**: Enables/disables file support for the Flow. * **Max files to upload**: Maximum number of files that can be uploaded in one message. * **Max file size**: Maximum size for each uploaded file. * **Supported file extensions**: List of supported file extensions for the Flow. {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "25786a18-4b37-482f-a30f-539ffe70b39d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "e18ebdf5-58b6-4d03-921b-f4bcaf4e6227",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?1053",
          "evaluationResultRowContextName": "1053",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Responses API This API provides access to the Responses endpoint from Globant Enterprise AI (GEAI). It includes plain text inputs, function calling, and file inputs (PDFs and images). Check the [[20]] for generic variables needed to use the API. **Note:** This endpoint currently supports only OpenAI models. ## Endpoint\n| Method | Path |\n| --- | --- |\n| **POST** | `/responses` | </br>\nThis endpoint has the same interface (Request/Response) as the [https://platform.openai.com/docs/api-reference/responses|OpenAI Responses API]. ## 1. Basic text completion\n#### Request\n```\ncurl --request POST \\ --url $BASE_URL/responses \\ -H \"Authorization: Bearer $YOUR_SAIA_API_TOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"openai/o1-pro\", \"input\": \"Tell me a three\u2011sentence bedtime story about a unicorn.\", \"stream\": false, \"max_tokens\": 2000\n}'\n```\n</br>\n#### Expected Response ```\n{ \"instructions\": null, \"metadata\": {}, \"reasoning\": { \"effort\": \"medium\" }, \"usage\": { \"completion_tokens\": 272, \"prompt_tokens\": 18, \"total_cost\": 0.1659, \"completion_tokens_details\": { \"reasoning_tokens\": 192 }, \"prompt_tokens_details\": { \"cached_tokens\": 0 }, \"total_tokens\": 290, \"currency\": \"USD\", \"completion_cost\": 0.1632, \"prompt_cost\": 0.0027 }, \"created_at\": 1746473821, \"error\": null, \"tools\": [], \"output\": [ { \"summary\": [], \"id\": \"rs_6819...\", \"type\": \"reasoning\", \"status\": null }, { \"role\": \"assistant\", \"id\": \"msg_68191...\", \"type\": \"message\", \"content\": [ { \"annotations\": [], \"text\": \"Once upon a time, in a hidden meadow where flowers swayed like colored ribbons, a unicorn named Moonlight pranced gracefully beneath the twinkling stars. By day, she roamed among the sun-kissed hills, her soft mane shimmering with every gentle breeze. At night, Moonlight\u2019s silvery horn glowed softly, granting sweet, soothing dreams to all who believed in magic.\", \"type\": \"output_text\" } ], \"status\": \"completed\" } ], \"top_p\": 1, \"previous_response_id\": null, \"parallel_tool_calls\": true, \"temperature\": 1, \"tool_choice\": \"auto\", \"model\": \"o1-pro-2025-03-19\", \"id\": \"resp_bGl...\", \"text\": { \"format\": { \"type\": \"text\" } }, \"incomplete_details\": null, \"truncation\": \"disabled\", \"user\": null, \"object\": \"response\", \"max_output_tokens\": null, \"status\": \"completed\"\n}\n``` ## 2. Function calling\n#### Request\n```\ncurl --request POST \\ --url $BASE_URL/responses \\ -H \"Authorization: Bearer $YOUR_SAIA_API_TOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"openai/o1-pro\", \"input\": \"What is the weather like in Paris today?\", \"tools\": [ { \"type\": \"function\", \"name\": \"get_weather\", \"description\": \"Get current temperature for a given location.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"City and country e.g. Bogot\u00e1, Colombia\" } }, \"required\": [ \"location\" ], \"additionalProperties\": false } } ]\n}'\n```\n</br>\n#### Expected Response\n```\n{ \"instructions\": null, \"metadata\": {}, \"reasoning\": { \"effort\": \"medium\" }, \"usage\": { \"total_tokens\": 0, \"currency\": \"USD\" }, \"created_at\": 1745243167, \"error\": null, \"tools\": [ { \"name\": \"get_weather\", \"description\": \"Get current temperature for a given location.\", \"strict\": true, \"type\": \"function\", \"parameters\": { \"additionalProperties\": false, \"type\": \"object\", \"properties\": { \"location\": { \"description\": \"City and country e.g. Bogot\u00e1, Colombia\", \"type\": \"string\" } }, \"required\": [ \"location\" ] } } ], \"output\": [ { \"summary\": [], \"id\": \"rs_6806...\", \"type\": \"reasoning\", \"status\": null }, { \"name\": \"get_weather\", \"arguments\": \"{\\\"location\\\":\\\"Paris, France\\\"}\", \"id\": \"fc_6806...\", \"type\": \"function_call\", \"call_id\": \"call_ruYPRIF9qcOxft5XrfjCBQg8\", \"status\": \"completed\" } ], \"top_p\": 1, \"previous_response_id\": null, \"parallel_tool_calls\": true, \"temperature\": 1, \"tool_choice\": \"auto\", \"model\": \"o1-pro-2025-03-19\", \"id\": \"resp_6806...\", \"text\": { \"format\": { \"type\": \"text\" } }, \"incomplete_details\": null, \"truncation\": \"disabled\", \"user\": null, \"object\": \"response\", \"max_output_tokens\": null, \"status\": \"completed\"\n}\n``` ## 3. Sending files\n### 3.1 Image input There are two ways to attach an image: * URL \u2013 pass a publicly reachable image_url. Recommended when the file is already hosted and you want a minimal payload.\n* Base\u201164 inline \u2013 embed the binary data in a data:image/...;base64, URI. Recommended for local files, private content, or CI pipelines with no public storage. </br>\n**Option A \u2014 Public image URL (cURL)**\n```\ncurl --request POST \\ --url $BASE_URL/responses \\ -H \"Authorization: Bearer $YOUR_SAIA_API_TOKEN\" \\ -H \"Content-Type: application/json\" \\ --data '{ \"model\": \"openai/gpt-4.1\", \"input\": [ { \"role\": \"user\", \"content\": [ {\"type\": \"input_text\", \"text\": \"What is in this image?\"}, { \"type\": \"input_image\", \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" } ] } ] }'\n```\n</br>\n**Option B \u2014 Inline base\u201164 (Python)**\n```\nimport base64, json, requests token = \"YOUR_SAIA_API_TOKEN\"\nurl = \"$BASE_URL/responses\"\nheaders = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"} def encode_file(file_path): with open(file_path, 'rb') as file: return base64.b64encode(file.read()).decode('utf-8') file_path = \"/path/to/your/image.png\"\nencoded_file = encode_file(file_path) payload = { \"model\": \"openai/gpt-4.1\", \"input\": [ { \"role\": \"user\", \"content\": [{ \"type\": \"input_text\", \"text\": \"What's this image about?\" }, { \"type\": \"input_image\", \"image_url\": f\"data:image/png;base64,{encoded_file}\" } ] } ]\n}\nr = requests.post(url, headers=headers, json=payload)\nprint(json.dumps(r.json(), indent=2))\n```\n</br>\n**Supported MIME types for images** | Extension(s) | MIME type | Base64 prefix |\n| --- | --- | --- |\n| .png | image/png | data:image/png;base64, |\n| .jpg, .jpeg | image/jpeg | data:image/jpeg;base64, |\n| .gif | image/gif | data:image/gif;base64, |\n| .webp | image/webp | data:image/webp;base64, ### 3.2 PDF input (Python example)\n```\nimport base64, json, requests token = \"YOUR_SAIA_API_TOKEN\"\nurl = \"$BASE_URL/responses\"\nheaders = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"} def encode_file(file_path): with open(file_path, 'rb') as file: return base64.b64encode(file.read()).decode('utf-8') file_path = \"/path/to/your/sample.pdf\"\nencoded_file = encode_file(file_path) payload = { \"model\": \"openai/o4-mini\", \"input\": [ { \"role\": \"user\", \"content\": [ { \"type\": \"input_text\", \"text\": \"What's this file about?\" }, { \"type\": \"input_file\", \"filename\": \"sample.pdf\", \"file_data\": f\"data:application/pdf;base64,{encoded_file}\" } ] } ]\n}\nr = requests.post(url, headers=headers, json=payload)\nprint(json.dumps(r.json(), indent=2))\n``` **Note:** Only models that support both text and image inputs, such as gpt-4o, gpt-4.1, or o1, can accept PDF files as input. ## Availability Since May 2025 release. {{{12}}} {{{17}}}\n##MARKDOWN##",
          "evaluationResultRowContextDocId": "60900ab1-258b-445f-9a0c-ec190a5d4cc8",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "f9fc7aca-4573-4999-8f15-499d26ce180d",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?1180",
          "evaluationResultRowContextName": "1180",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataSetRowId": "87d23cc0-b9f8-42e3-a6c8-e33f2aaed70f",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0048375800",
      "evaluationResultRowEndDate": "2025-05-13T11:15:24",
      "evaluationResultRowOutput": "Claro, aqu\u00ed tienes un ejemplo de c\u00f3mo subir un archivo usando la API de archivos de Globant Enterprise AI con cURL:\n\n```bash\ncurl -X POST \"$BASE_URL/v1/files\" \\\n-H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\\n-H \"Content-Type: multipart/form-data\" \\\n-H \"fileName: $FILE_NAME\" \\\n-H \"folder: testFolder\" \\\n-H \"organizationId: $ORGANIZATION_ID\" \\\n-H \"projectId: $PROJECT_ID\" \\\n-F \"file=@/path/to/your_file.mp4\"\n```\n\n- Reemplaza `$BASE_URL` por la URL base de la API.\n- Usa tu token en `Authorization: Bearer $SAIA_PROJECT_APITOKEN`.\n- En `fileName` puedes especificar el nombre con el que quieres referenciar tu archivo (opcional).\n- En `folder` puedes indicar una carpeta para organizar el archivo (opcional).\n- `organizationId` y `projectId` deben obtenerse usando el endpoint de validaci\u00f3n de token.\n- Cambia `/path/to/your_file.mp4` por la ruta local del archivo que deseas subir.\n\nEste comando sube el archivo y, si el par\u00e1metro `fileName` no se incluye, el nombre de referencia ser\u00e1 el nombre original del archivo.\n\nDespu\u00e9s de la subida exitosa, recibir\u00e1s una respuesta JSON que confirma el \u00e9xito, por ejemplo:\n\n```json\n{\n  \"dataFileId\": \"string\",\n  \"dataFileUrl\": \"string\",\n  \"success\": true\n}\n```\n\nEste procedimiento es parte del flujo para cargar y gestionar archivos v\u00eda API en Globant Enterprise AI. \n\nReferencia: [C\u00f3mo subir y gestionar archivos v\u00eda API - Globant Enterprise AI](https://wiki.genexus.com/enterprise-ai/wiki?454) [6](https://wiki.genexus.com/enterprise-ai/wiki?454)",
      "evaluationResultRowStartDate": "2025-05-13T11:15:14",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "11087"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# Chroma Vector Store Provider [Chroma](https://www.trychroma.com/)<sup>(1)</sup> is an open-source AI application database with support for Embeddings, vector search, document storage, full-text search, metadata filtering, and multi-modal. <sup>(1)</sup> - Not enabled by default, available for [on-premises](441.html) installations.",
          "evaluationResultRowContextDocId": "12fb00eb-9f75-4e43-9b85-d017dc3a0550",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "11a3469c-d6a4-4ea8-90f3-3880f7d49297",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?537",
          "evaluationResultRowContextName": "537",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Supported Vector Store Provider A **Vector Store** is a system used to store data that has been converted into vectors using *embedding* techniques. This conversion allows unstructured data, such as free text or images, to be transformed into numerical representations (vectors) that reflect the characteristics of the original data. Once the data has been embedded and stored as vectors, it is possible to perform vector searches. This means that when an unstructured query is performed, it is also converted into a vector, and the system retrieves the most similar vectors stored, which facilitates the identification of information related to the query. In the context of a [[44|RAG Assistant]], a default **Vector Store Provider** is assigned according to the instance configuration. To configure it, go to the [[44|RAG Assistant]]. The Globant Enterprise AI SAAS solution offers the following options: - [[270|Pinecone]] (default) Other available Vector Store Provider (available for on-premises installations) - [[271|Azure AI Search]]\n- [[439|pgvector]]\n- [[437|Redis Search]]\n- [[438]]\n- [[537|Chroma]]\n- [[579|Elasticsearch]]\n- [[580|Opensearch]] {{{12}}} {{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "966ae8e5-d663-41ea-a56b-356a4d343b81",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "2025a94d-aea8-4e5f-b630-ca991d25e7fb",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?267",
          "evaluationResultRowContextName": "267",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Redis Search Vector Store Provider [Redis for vector database](https://redis.io/solutions/vector-database/)<sup>(1)</sup> is a high-performance vector database that lets you perform semantic searches over vector embeddings. <sup>(1)</sup> - Not enabled by default, available for [on-premises](441.html) installations.",
          "evaluationResultRowContextDocId": "5ed31304-dd23-4046-9e1e-118a7e72e11a",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "45a054df-0519-40ed-ac2a-6800f27ed51e",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?437",
          "evaluationResultRowContextName": "437",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Configuration - Retrieval This subsection allows you to specify how to obtain the augmented information sent to the context: ![Image](./images/67.png) ## Retriever Type It indicates the type of retriever used to obtain information; the default value is VectorStore. The values it can take are: - ### [HyDE](#HyDE) Specific retrieval method called [Hypothetical Document Embeddings](https://arxiv.org/abs/2212.10496) (HyDE). This method uses embedding techniques to take queries, generate hypothetical answers, embed the generated document, and use it as the final example. By default, the HyDE class comes with some default queries, but you can also create custom queries that must have a single input variable {question}. - ### [Contextual Compression](#Contextual+Compression) It tries to improve the answers returned from vector store document similarity searches by better taking into account the context from the query. It wraps another retriever and uses a Document Compressor as an intermediate step after the initial similarity search that removes information irrelevant to the initial query from the retrieved documents. It aims to reduce the amount of distraction a subsequent chain has to deal with when parsing the retrieved documents and making its final judgments. - ### [VectorStore](#VectorStore) This is the default value, which directly uses the defined VectorStore without any further pre-processing. - ### [Self Query](#Self+Query) It first queries itself to retrieve filter information based on the natural language query. It parses the response and processes it as a JSON structure; then it executes a second query to the LLM with the query and filters applied based on the first one. As a prerequisite, each document (or chunk) needs to be ingested with associated filters applied, check a [Self Query Use Case](150.html). The *score* field is not returned to the client. - ### [Multi Query](#Multi+Query) It automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the MultiQueryRetriever might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results. The *score* field is not returned to the client. - ### [Score Threshold](#Score+Threshold) It uses a feature called Recursive Similarity Search. With it, you can do a similarity search without having to rely solely on the \u201cDocument Count\u201d value. The system will return all similar question matches based on the minimum score threshold configured (check below the associated parameter). The *score* field is not returned to the client. - ### [Graph](#Graph+) It enables the use of a graph-based information representation approach for retrieval, more information [Graph Retrieval](203.html). - ### [Amazon Knowledge Base](#Amazon+Knowledge+Base) Check detail [here](577.html). ## Retriever prompt It specifies the query that is sent to the retriever to search for information. This query could be a question or a specific request. When the following values are configured, this parameter is not taken into account: - VectorStore\n- scoreThreshold\n- selfQuery Clicking on the Set Default Prompt button automatically sets the Retriever Prompt to default values according to the option selected in \"Retriever Type\". For example, if the value set in Retriever Type is graph, clicking on the Set Default Prompt button will populate the [Retrieval Prompt](203.html). Below are the default values of \"Retriever prompt\" according to the value set in Retriever Type. - #### [Hypothetical Document Embeddings](#Hypothetical+Document+Embeddings) ```json { \"prompt\": \"Please write a minimal passage to answer the question only\\nQuestion: {question}\\nPassage:\" } ``` The specific query for HyDE requesting a minimal paragraph to answer the question. - #### [Contextual Compression](#Contextual+Compression) ```json { \"prompt\": \"Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant, return empty.\\n> Question: {question}\\n> Context:\\n>>>\\n{context}\\n>>>\\nExtracted relevant parts:\" } ``` The query for Contextual Compression looks for relevant parts of the context that answer the question. - #### [Multi Query](#Multi+Query) ```plaintext You are an AI language model assistant. Your task is to generate {queryCount} different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of distance-based similarity search. Provide these alternative questions separated by newlines between XML tags. For example: <questions> Question 1 Question 2 Question 3 </questions> Original question: {question} ``` Multi Query generates 5 additional queries from different perspectives. Each generated query is used to retrieve a set of relevant documents from the configured vectorStore, and then the single join of all document sets is performed to obtain a larger set of potentially relevant documents. ## ScoreThreshold It defines the minimal valid value to consider the information as valid when retrieved from the VectorStore; otherwise, it is discarded. If there are no valid Documents, no interaction takes place with the LLM. The default value is 0.0. To select a correct threshold value you need to execute experiments on your data, type of expected questions and embeddings model. Notice that when changing any of these parameters you may need to retune the threshold parameter. Check these links that can guide you on how to get a good value for your use case - [Rule of thumb cosine similarity thresholds?](https://community.openai.com/t/rule-of-thumb-cosine-similarity-thresholds/693670/)\n- [Query embedding threshold evaluation with curbing dimension](https://community.openai.com/t/query-embedding-threshold-evaluation-with-curbing-dimension/894554) ## Profile Metadata This field activates the advanced configuration. By default, it remains disabled. For more information, see [RAG Profile Metadata configuration](298.html). ## Endpoint URL address pointing to the specific server or service where the models or retrieval methods are hosted; the value is optional.",
          "evaluationResultRowContextDocId": "fd527594-cf98-4f9a-a3de-293caf485c54",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "7b5b6a2a-b6c2-4e59-8ea3-f83d319bf696",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?70",
          "evaluationResultRowContextName": "70",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# RAG Assistants Introduction [Retrieval Augmented Generation](https://www.promptingguide.ai/techniques/rag) (RAG) is an approach that combines information retrieval from unstructured data and text generation to improve performance on tasks such as question answering. In the retrieval phase, a selective search is performed on a set of documents, identifying related information and efficiently reducing the search space. This approach ensures that focus is placed on the most relevant and meaningful information. The information retrieved is effectively integrated and expanded in prompting, providing deeper connections and insights into unstructured data. This process enriches the information with additional details and relationships. In this way, it encompasses not only immediate relevance, but also broader contextual connections and helps in understanding the retrieved information. In the subsequent text generation phase, this expanded data set is used to produce coherent and contextual responses. The generative model, by working on the information previously retrieved and added to the prompting (in-context learning), not only improves the system's ability to generate accurate and relevant responses, but also effectively incorporates the enriched details from the textual information. This process enhances the system's ability to generate accurate and relevant content, effectively completing the cycle in the comprehensive RAG approach. ## RAG Assistants in Globant Enterprise AI ![Globant Enterprise AI](./images/43.png) Globant Enterprise AI makes it possible to use RAG assistants to chat with or search for information stored in documents (unstructured data). This functionality is enabled through the [RAG Assistants API](29.html) or RAG Assistants section of the Globant Enterprise AI backend. The different phases of this process are described below: ### Data Ingestion The initial phase, known as the Ingestion stage, involves loading various types of documents from multiple sources. This phase is not only limited to data acquisition but also includes the configuration of chunks. #### Documents and other data sources Data can be loaded in different formats and from different sources. In addition, other data sources generated by end users are considered. #### Configuration of Chunks In parallel with document loading, the configuration of chunks is performed to optimize information management. These chunks act as organizational fragments, enabling efficient data segmentation. This process goes beyond simple data partitioning, as it integrates with the Index Profile and its chunking strategy. This connection ensures that data segmentation is consistent and aligned with the specific requirements of the index, thus optimizing data preparation for subsequent processing. The default chunking strategy is as follows: - chunkSize: 1000 characters\n- chunkOverlap: 100 characters ### Retrieval In the Retrieval stage, the data retrieval process is started, leveraging the previously ingested and organized information. During this phase, the vector database is accessed and the documents loaded during Ingestion are indexed and stored efficiently. The main component of this stage is data extraction through embeddings and access to the vector store. #### Embeddings Embeddings, derived from the context and represented by a numeric array, capture the contextual essence of the documents in terms of chunks and the user's query. #### Vector Store The Vector Store, powered by embeddings and metadata, connects with providers, index parameters, and distance metrics to ensure accurate and contextualized retrieval of data chunks relevant to the given query. ### Generation The Generation stage marks the point at which the Globant Enterprise AI architecture focuses on generating relevant and contextually consistent responses. In this process, the system uses the RAG Assistant configuration to know which model to access and with which parameters. #### RAG Assistant This assistant incorporates key elements such as Prompts, LLMs, and search retrieval parameters to define the search strategy. Prompts act as guides to contextualize the responses, while the LLM contributes to the consistency and relevance of the generated content. In addition, the assistant makes it possible to add variable-based adjustments and filters, enhancing the customization of the generated responses. This adaptability allows for responses to be specific and relevant to the particular needs of the end user. ### End user interaction Finally, the Globant Enterprise AI architecture enables interaction with the user through the API. This interface facilitates smooth and efficient communication between end users and RAG Assistants, completing the cycle and providing answers to queries in an efficient manner.",
          "evaluationResultRowContextDocId": "29364b9b-2163-4296-b051-ecf6ef244703",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "c153d6dd-5011-4cbe-b847-4f82405178ed",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?44",
          "evaluationResultRowContextName": "44",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Elasticsearch Vector Store Provider [https://www.elastic.co/|Elasticsearch] is a powerful search and analytics engine built on Apache Lucene. It is designed to handle large volumes of structured or unstructured data, providing full-text search, embeddings, real-time indexing, and distributed storage. ### Considerations * [[206|MMR]] is not supported\n* Not enabled by default, available for [[441|on-premises]] installations. {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "b21d4a90-6083-4430-8306-24f5b2aef928",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "d877c133-ee46-4152-accd-6634c6a782a5",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?579",
          "evaluationResultRowContextName": "579",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# VectorStore Search Options VectorStore is a type of retriever used by the [RAG Assistant](44.html) to get information. To configure it, go to the [Retrieval subsection](70.html) and set the VectorStore value in the [Retriever Type](70.html) parameter. Vector stores are specialized databases designed to store and manage high-dimensional vectors efficiently. These vectors often represent complex data such as text, images, or other features, enabling advanced search and retrieval functionalities. Two common techniques used in the context of vector stores are: - Similarity search\n- Maximal Marginal Relevance (MMR) While both techniques aim to enhance data retrieval, they serve different purposes and offer distinct advantages. ## [Similarity Search](#Similarity+Search) It involves finding vectors within a vector store that are most similar to a given query vector. This technique relies on distance metrics such as *cosine similarity*, *euclidean distance*, or *inner product* to measure the closeness between vectors. The primary goal is to retrieve items that are nearest to the query, making it highly effective for tasks like finding similar documents, images, or other entities. Similarity search is particularly useful when the objective is to identify items that are most alike to the query in terms of content or features. This is the default value used. ## [Max Marginal Relevance](#Max+Marginal+Relevance) Maximal Marginal Relevance (MMR) is a technique designed to balance relevance and diversity in the retrieved results. While similarity search focuses solely on retrieving the closest matches, MMR aims to diversify the results by considering both the relevance of the items to the query and their novelty relative to the already selected items. This is achieved by iteratively selecting items that maximize marginal relevance, which is a combination of similarity to the query and dissimilarity to the items already chosen. MMR is especially beneficial in scenarios where it's important to avoid redundancy and provide broader coverage of the query's aspects, such as in document summarization or recommendation systems. In short, this method is designed to fetch text chunks from the store that are at the same time relevant to the query but as different as possible from each other, with the goal of providing a broader context to the building of the final answer. To use this advanced configuration, you need to add the following parameters to the [Profile metadata](70.html). Check the sample for further details. ```json\n{ \"chat\": { \"search\": { \"fetchK\": number, \"lambda\": number, \"type\": \"mmr\" // by default if not set it assumes \"similarity\" and the vectorstore default metric } }\n}\n``` Additional parameters used: | Parameter | Description |\n|---|---|\n| fetchK | Number of documents to fetch before passing to the MMR algorithm; it defaults to 20. |\n| lambda | Number between 0 and 1 that determines the degree of diversity among the results, where 0 corresponds to maximum diversity and 1 to minimum diversity; it defaults to 0.5. | Note that the [Chunk Count](69.html) parameter (also referred as *k*) will still be used as it determines the final number of results returned to the user. The *fetchK* extra item controls the breadth of the initial search before further refinement. Leveraging both parameters effectively can enhance the relevance, diversity, and overall quality of the search results. ### [Sample Configuration](#Sample+Configuration) To enable MMR, a valid configuration is as follows. Also, change *fetchK* and *lambda* accordingly. ```json\n{ \"chat\": { \"search\": { \"fetchK\": 20, \"lambda\": 0.5, \"type\": \"mmr\" } }\n}\n``` ### [Sample](#Sample) Suppose there is a vector store with the following indexed information: ```plaintext\nchunk 1: Apple unveils new iPhone with advanced features.\nchunk 2: Apple's new iPhone is a game changer in the smartphone industry.\nchunk 3: Samsung releases new Galaxy phone to compete with Apple.\nchunk 4: The latest iPhone from Apple includes several new innovations.\nchunk 5: Tech industry reacts to Apple's latest iPhone release.\nchunk 6: Consumers debate between Apple's iPhone and Samsung's Galaxy.\n``` When asking for *Apple's new iPhone*; these are the resulting chunks returned: ```plaintext\nMMR Case\nchunk 1\nchunk 6\nchunk 2\nSimilarity Case\nchunk 1\nchunk 2\nchunk 4\n``` Note the Maximal marginal relevance case where it optimizes for similarity to the query AND diversity among selected documents (including the 6 items). You can experiment with the *lambda* parameter to adjust the diversity. ## [Considerations](#Considerations) - ChromaDB does not support MMR.\n- MMR is not taken into account when using *ScoreThreshold* retrievers.",
          "evaluationResultRowContextDocId": "652a928a-a382-4bed-9352-81f9e6328601",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "f53b0332-e3d7-48da-9824-a6d1ef5224ca",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?206",
          "evaluationResultRowContextName": "206",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.5,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "9b94eb6e-2bbc-47e6-97dd-efad87954fde",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0019732000",
      "evaluationResultRowEndDate": "2025-05-13T11:15:29",
      "evaluationResultRowOutput": "Los Vector Stores soportados por Globant Enterprise AI son:\n\n- Pinecone (por defecto en la soluci\u00f3n SaaS)\n- Azure AI Search (disponible para instalaciones on-premises)\n- pgvector (on-premises)\n- Redis Search (on-premises)\n- Chroma (on-premises)\n- Elasticsearch (on-premises)\n- Opensearch (on-premises)\n\nEsta lista incluye tanto los proveedores disponibles en la soluci\u00f3n SaaS como aquellos que est\u00e1n disponibles para instalaciones locales (on-premises). Pinecone es el valor por defecto en SaaS.\n\nM\u00e1s detalles en la documentaci\u00f3n oficial: [Soportados Vector Store Providers](https://wiki.genexus.com/enterprise-ai/wiki?267) [5](https://wiki.genexus.com/enterprise-ai/wiki?267)",
      "evaluationResultRowStartDate": "2025-05-13T11:15:24",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "4593"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# Flows in Globant Enterprise AI Flows is a Globant Enterprise AI solution that allows creating conversational solutions tailored to the specific needs of each company by leveraging the capabilities of generative and deterministic artificial intelligence. With Flows, companies can significantly improve interaction with their end users. The solution not only enables the development of chatbots that respond accurately and appropriately, but also facilitates the integration of specialized assistants such as [Data Analyst Assistant](886.html), [API Assistant](110.html), [Chat with Data Assistant](159.html), and [RAG Assistant](44.html). This means that a conversational flow can simultaneously draw on data analysis, integration with external APIs, conversation with documents, and generation of contextual responses. Rather than being limited to a single type of interaction, Flows allows you to seamlessly combine these functions, enriching the conversation and providing more complete and accurate responses to end users.",
          "evaluationResultRowContextDocId": "2b2be084-a211-4463-bc6c-162c331a549e",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "2eca8e29-5e3c-4710-8973-81f052ac0495",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?321",
          "evaluationResultRowContextName": "321",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# RAG Assistants configuration All interaction with this component is configured through the RAG Assistants section. A default RAG Assistant is created during initialization and later it can be modified or new ones created to change the assistant's behavior. RAG Assistant is located in the Project Options section and allows you to customize options specific to each RAG Assistant by clicking on UPDATE. ![RAG Assistant](./images/62.png) The set of options is organized into the following subsections: ![Options Subsections](./images/63.jpg) - [General Information](68.html): key details that allow you to quickly understand the assistant's capabilities and features.\n- [Prompt](69.html): information on how the assistant is configured to receive and process user queries.\n- [Retrieval](70.html): specifies how the information is retrieved. ### [See Also](#See+Also) [Index Configuration](145.html) [Using variables in Prompts](85.html) [Context Prompt Template](205.html) [Start Page](408.html)",
          "evaluationResultRowContextDocId": "1a844a6d-9741-4178-bc4f-ce0030f01f09",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "a9faae35-d74a-4597-bc3a-6a5756045a34",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?64",
          "evaluationResultRowContextName": "64",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "* **Body**: This section allows you to write the content of the email. You can format the text using basic editing options such as bold, italics, underline, lists and links. You can also insert emojis or variables. ### Spreadsheet Allows you to write data into a spreadsheet directly from a flow. You can use this node to store user responses or any other information in real time, organizing the data in specific columns of the spreadsheet. [[508]] To configure the Spreadsheet node, click on the Edit icon located on the right side of the node. Clicking on Edit will open a side menu where you can enter the Spreadsheet ID and the Sheet name. The Spreadsheet ID is obtained from the URL of the Google Spreadsheet, and the Sheet name corresponds to the specific tab within the file where you want to save the data, such as \"Sheet 1\". In addition, make sure to upload the JSON file with the service account credentials and that the service account has permissions on the spreadsheet. The node displays a drop-down field where you can select the variables to be written to the spreadsheet. You must arrange the variables in the order in which they appear in the spreadsheet. You can also add other context variables if necessary. ## RESPONSES The Responses section contains the nodes responsible for generating responses to the end user. [[509]] ### Message Sends custom messages within a flow, such as an initial greeting. For example, you can use: \"Hello {name}, how can I help you?\", where {name} is a dynamic variable that is filled with the user's name. [[510]] This message is fully customizable. You can format the text using different styles, such as Normal or Heading 1, and formatting tools such as bold, italics, lists, links and emojis. In addition, you can insert dynamic variables, and add lists, links and emojis. You can also add message variants. ### Image Adds images to interactions within a flow, displaying a space where you can upload or select an image. [[511]] In addition, you can make the content dynamic by clicking on the nut-shaped icon, allowing you to use Velocity Template Language (VTL) to create dynamic responses, adapting the image content to different contexts. Clicking on Add Image brings up a menu on the left side that allows you to upload an image by dragging it or by browsing for the file on your device. You can also enter a URL directly to use an online image. This menu offers options for adjusting the size of the image, including cropping or scaling the image, and setting its vertical and horizontal position, centering it as needed. ### Link Create rich links with a custom preview that includes a title, description, image and tag, providing additional context to engage the user. [[512]] If you choose to add an image, clicking on Add Image will open a menu on the left side where you can upload an image from your device or enter a URL to use an online image. This menu also allows you to adjust the size, vertical position and horizontal position of the image, in a similar way as with the Image component. In addition, you can make the content of the link dynamic by clicking on the nut-shaped edit button. This will allow you to use Velocity Template Language (VTL) to adapt the preview to different contexts and situations within the flow. ### Card Create interactive cards within a flow, where you can add an image, title, and short description. As in the Image and Link components, you can add an image by clicking on Add Image, which will open a menu on the left side with the same options as the Image and Link components. [[513]] The Card node includes one button by default, and you can add more if necessary. If you add several buttons, each one can be configured to connect to an External function or External link, or to define an event. When all buttons are configured as External function or External link, the flow follows a single branch. However, if any of the buttons is configured for an event, an additional branch will be generated in the flow for each button configured for events. This node allows you to enable or disable text interaction, keep the conversation active, or allow user input. You can also make the content dynamic in order to adapt the card to different contexts within the flow using Velocity Template Language (VTL). ### Video Adds the URL of a video, but YouTube videos are not supported. You can also paste the URL of an image in the Poster field to use it as a thumbnail or cover of the video. [[514]] It is possible to make the link content dynamic using Velocity Template Language (VTL) to adapt the video preview to different contexts and situations within the flow. In addition, the component offers the option of adding buttons to connect to an external link, define a specific event or action, and set variables that can be used as input in other interactions within the flow. Each additional button can create a new branch in the flow, unless it is configured to connect to an External function or External link, in which case it will follow the same branch. ### YouTube Allows you to insert YouTube videos into your flow. [[515]] To set up this node, paste the ID of the video in the \"Id\" field in the left menu, which appears when you click on the edit icon. Make sure to enter only the part of the URL that comes after the equal sign (=). For example, if the URL is `https://www.youtube.com/watch?v=h_5nxFWjmZc`, just enter `h_5nxFWjmZc` in the corresponding field. The video thumbnails are generated automatically, so there is no need to add an additional image. The component also offers the option to add as many buttons as you deem necessary. In addition, you can make the content dynamic using Velocity Template Language (VTL), which allows you to adapt the video presentation to different contexts and situations within the flow. ### Quick Reply Adds buttons that you can use to start actions or redirect to other flows. Each button is configured by entering its name and selecting an action option. The Event option allows you to direct the reply to the desired flow. With External Link, you can redirect the user to a website, taking them out of the current flow. Finally, the External function allows you to execute an external function. [[516]] This node also offers additional behavior settings. You can disable text interaction, keep the conversation active, or enable user input. By default, the Enabled User Input option is active, which means that, if you add multiple buttons, a branch will be generated in the flow for each button configured as an Event. Buttons configured as External function or External link will not create new branches and will follow the same path in the flow. ### Datepicker Allows you to select a date within a flow, displaying fields to choose the month, day and year. [[517]] It is possible to configure the format in which the date will be displayed, select which dates will be available, connect the date picker to a specific event, and assign\n\n#Interactions to Create a Flow The Interactions Section provides the necessary components to build custom [[322|flows]]. Here you will find tools to generate, integrate, and respond effectively to interactions in order to design flows that meet the needs of the end user. [[493]] Below are described the different categories of components available in the Interactions Section, including their functionalities and how they can be used to optimize the creation of flows in Globant Enterprise AI. ## GENERATIVE In the Generative category, you will find the assistants that use generative AI: Assistant and RAG Assistant. [[495]] ### Assistant Configures an assistant, such as [[886|Data Analyst Assistant]], [[110]], [[159]], among others, that have been previously defined in the selected project in the [[42]]. [[496]] Once the assistant is selected, the Prompt field will display the message defining the assigned task. This message is read-only and cannot be edited. To see the complete prompt, click on the configuration icon. This node always requires an input variable, such as `lastUserInput`, which receives the information provided by the user. The output variable, on the other hand, can be configured. By default, the assistant's response is stored in `lastUserInput`, but if you disable the Use as response option at the top of the node, you can set a different output variable. In addition, it is possible to save the response in JSON format. The node also allows you to include the conversation history and context variables to further customize the interaction. ### RAG Assistant It integrates an assistant of [[44|RAG]] type that has been previously defined in the selected project in the Globant Enterprise AI Backoffice. [[497]] After selecting the RAG assistant, you can define a document or tags that will act as a source for the responses. Variables are handled in the same way as in the Assistant node. That is, it always requires an input variable, and it is possible to configure a different output variable that can be in JSON format. You can also set the minimum confidence level for responses, include the conversation history and context variables, and choose to display the sources of information used. In addition, you can configure the actions to take when an answer is not found in the documents provided. ## LOGICAL This section includes logical components that allow you to control the flow of the conversation and manipulate variables or conditions within the flow. [[498]] ### User Input Collects user input and stores it in a variable for use in subsequent nodes. [[499]] ### Conditional Evaluates logical conditions written in [https://velocity.apache.org/engine/1.7/vtl-reference.html#ifelseifelse-output-conditional-on-truth-of-statements|Velocity Template Language (VTL)] to decide which path the flow will take. [[500]] When this component is added to the flow, it is divided into branches, which by default are one for If and another for Else. The If branch node is by default of Expression type, while the Else branch is of Go to type. It is possible to add more If conditions by clicking on Add condition. By doing so, a branch with a node of Expression type is added, just like the first one. These Expression nodes allow evaluating different conditional expressions. Each branch is driven by the result of the evaluated condition. Depending on the value of a variable, this allows the flow to be directed to different assistants or actions. If the condition is not met, the flow continues through the Else branch. Each branch is driven by the result of the evaluated condition. Depending on the value of a variable, this allows the flow to be directed to different assistants or actions. If the condition is not met, the flow continues through the Else branch. To configure the condition, select a variable and apply a relational operator to perform the comparison. The supported relational operators are: * Equal\n* Not equal\n* Less than\n* Less than or equal to\n* Greater than\n\nthe selected value to a variable. Additionally, this node offers options for configuring the state behavior. You can disable text interaction, keep the conversation active, or enable user input, which is enabled by default. Additionally, it is possible to make the content dynamic, which allows you to adapt the interaction with the date picker to different contexts within the flow. ### Dropdown Allows the end user to select an option from a predefined list. [[518]] To configure this node, click on Edit to the right of the node. Next, in the left menu configure Assign value to field, which can be either an entity or a variable. In addition, below Dropdown list, you must define the options to be offered. These options can be numbers, letters, phrases, or a combination of all of them. Each option can be associated with a specific response flow.\n\nOnce you have defined the options in the Dropdown list, to configure each value, go to the node and select each option in Select option. Next, in the left menu, configure the actions to perform when the user selects that option. These actions can include the execution of specific flows or any other logic you need to associate with the selected option. ### Carousel It works in a similar way to Cards, but with a different visual presentation that allows showing several cards in a rotating format. [[519]] It allows you to add an image, title, and description for each card. In addition, you can trigger an event by clicking on the CTA action button, which can redirect to another flow. It is also possible to trigger an external event that takes to another website or page, or execute an external function.\n{{{17}}}{{{12}}}\n##MARKDOWN##\n\n* Greater than or equal to **Note:** The variable must be assigned before performing the comparison. This can be done in the Variable component or any other component that allows assigning a value to a variable, such as Assistant or RAG Assistant. ### Variable This node is used to manipulate variables, allowing you to set, modify or query values to be used in the flow. [[501]] To define a variable, click on the down arrow inside the node and select the + Add new variable option. Next, enter the name of the variable and press Enter. By default, the Clear option will be enabled, but you can disable it if you want to keep the current value of the variable. To set the value of this variable, click on the edit icon to the right of the node. This will open a menu on the left with several options. One of the available options is Get from request extra data, which assigns the value of an extra property of the request data to the variable, or Set to response extra data, which assigns the value of the variable to a property of the response data. In the Value window, you can dynamically set the value of the variable using **Velocity Template Language (VTL)**. Here you can enter expressions such as $today to assign the current date to the variable. This allows you to generate and manipulate data dynamically within the flow. More information: [[834]]. ### Language The Language node allows you to change the language of the interaction according to the user's needs or the context of the flow. [[502]] ### Reset History Resets the conversation history, which is useful for starting a new interaction without previous responses influencing the new flow. [[503]] In addition, there is a Reset Context option, which is disabled by default. If you enable this option, it will not only reset the conversation history, but also reset all context variables associated with the flow. This means that any data or values previously stored in the flow variables will also be reset. ### Script Allows you to insert JavaScript code that runs on the server side, providing flexibility to implement specific functionalities not covered by standard system components. This is useful when you need a custom solution for a particular bot or case before a new component is developed. [[837]] When you insert this component, you'll see an area where you can add a description and an \"Edit Code\" button. Clicking on this button opens a code editor with a dark-themed workspace where you can write and save your JavaScript code. This component allows you to access context variables using the context object, and gives you the ability to define and use custom functions within the script. In addition, you can add debug logs using the console object to facilitate code development and validation. For detailed examples and more information on how to use this component, see [[841]]. ### Go To Connects different flows or nodes, facilitating end user navigation and redirection to different parts of the flow as needed. [[504]] When inserting a Go To in the flow, you can select the flow to which you want to redirect the user by clicking on the drop-down bar. This list shows all the available flows you can choose from. In addition, the Pick from canvas option allows you to visually select a node directly from the canvas, creating a direct connection between the nodes. The Go icon next to the Go To node allows you to navigate directly to the node to which it is connected, making it easy to review and modify connections. ## INTEGRATIONS Through the Integrations section, you can integrate the flow with other external services or systems. [[505]] ### API The API component allows you to provide responses to users using data from a server. [[506]] In this node, you can define both Parameters and Responses, depending on the configuration of your API. The Parameters option allows you to add parameters that will be sent together with the API request, such as required identifiers or filters. From Responses, you can configure how the data received from the API will be handled and processed within your flow. To configure your API within the node, you need to enter the API URL. To do so, click on the Edit icon located to the right of the node, and then click on the Expand icon next to the URL field, represented by a box with an up arrow. This is where you can paste the API address. Once the URL is set, you can select the method of the request from the drop-down menu below Method. Options include GET, POST, PUT, and DELETE, depending on the operation you need to perform. If you only want to retrieve information, select GET. In the Headers section, you can add additional settings such as key-value pairs to indicate how the response should be handled. For example, select Accept with the application/json value to expect a response in JSON format. In addition, it is possible to add variables within this field by clicking on the word Variable. If you need to add more headers, use the Add Headers option. ### Email Sends emails as part of the interactions within the flow. This is useful for notifications or confirmations. [[507]] In this node, you can configure the following fields: * **Recipients**: This is where you enter the recipients' email addresses. You can add multiple recipients separated by commas. You can also press Enter after you type each address to add it, and then type the next one.\n* **Subject**: In this field, you define the subject of the email. It is possible to insert variables by clicking on the Variable option to customize the subject with dynamic data.",
          "evaluationResultRowContextDocId": "26b674b4-d387-4cb4-a16f-b97b7e0d8204",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "c9d1ca61-113a-4ac5-8b17-d3dc27cd0384",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?494",
          "evaluationResultRowContextName": "494",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# RAG Assistants Introduction [Retrieval Augmented Generation](https://www.promptingguide.ai/techniques/rag) (RAG) is an approach that combines information retrieval from unstructured data and text generation to improve performance on tasks such as question answering. In the retrieval phase, a selective search is performed on a set of documents, identifying related information and efficiently reducing the search space. This approach ensures that focus is placed on the most relevant and meaningful information. The information retrieved is effectively integrated and expanded in prompting, providing deeper connections and insights into unstructured data. This process enriches the information with additional details and relationships. In this way, it encompasses not only immediate relevance, but also broader contextual connections and helps in understanding the retrieved information. In the subsequent text generation phase, this expanded data set is used to produce coherent and contextual responses. The generative model, by working on the information previously retrieved and added to the prompting (in-context learning), not only improves the system's ability to generate accurate and relevant responses, but also effectively incorporates the enriched details from the textual information. This process enhances the system's ability to generate accurate and relevant content, effectively completing the cycle in the comprehensive RAG approach. ## RAG Assistants in Globant Enterprise AI ![Globant Enterprise AI](./images/43.png) Globant Enterprise AI makes it possible to use RAG assistants to chat with or search for information stored in documents (unstructured data). This functionality is enabled through the [RAG Assistants API](29.html) or RAG Assistants section of the Globant Enterprise AI backend. The different phases of this process are described below: ### Data Ingestion The initial phase, known as the Ingestion stage, involves loading various types of documents from multiple sources. This phase is not only limited to data acquisition but also includes the configuration of chunks. #### Documents and other data sources Data can be loaded in different formats and from different sources. In addition, other data sources generated by end users are considered. #### Configuration of Chunks In parallel with document loading, the configuration of chunks is performed to optimize information management. These chunks act as organizational fragments, enabling efficient data segmentation. This process goes beyond simple data partitioning, as it integrates with the Index Profile and its chunking strategy. This connection ensures that data segmentation is consistent and aligned with the specific requirements of the index, thus optimizing data preparation for subsequent processing. The default chunking strategy is as follows: - chunkSize: 1000 characters\n- chunkOverlap: 100 characters ### Retrieval In the Retrieval stage, the data retrieval process is started, leveraging the previously ingested and organized information. During this phase, the vector database is accessed and the documents loaded during Ingestion are indexed and stored efficiently. The main component of this stage is data extraction through embeddings and access to the vector store. #### Embeddings Embeddings, derived from the context and represented by a numeric array, capture the contextual essence of the documents in terms of chunks and the user's query. #### Vector Store The Vector Store, powered by embeddings and metadata, connects with providers, index parameters, and distance metrics to ensure accurate and contextualized retrieval of data chunks relevant to the given query. ### Generation The Generation stage marks the point at which the Globant Enterprise AI architecture focuses on generating relevant and contextually consistent responses. In this process, the system uses the RAG Assistant configuration to know which model to access and with which parameters. #### RAG Assistant This assistant incorporates key elements such as Prompts, LLMs, and search retrieval parameters to define the search strategy. Prompts act as guides to contextualize the responses, while the LLM contributes to the consistency and relevance of the generated content. In addition, the assistant makes it possible to add variable-based adjustments and filters, enhancing the customization of the generated responses. This adaptability allows for responses to be specific and relevant to the particular needs of the end user. ### End user interaction Finally, the Globant Enterprise AI architecture enables interaction with the user through the API. This interface facilitates smooth and efficient communication between end users and RAG Assistants, completing the cycle and providing answers to queries in an efficient manner.",
          "evaluationResultRowContextDocId": "29364b9b-2163-4296-b051-ecf6ef244703",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "e8ee696a-e65e-454c-8a65-49558f0a1be4",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?44",
          "evaluationResultRowContextName": "44",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.8,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "a073dfe8-c95f-45e8-a0a2-caa99d8b0dde",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0033849200",
      "evaluationResultRowEndDate": "2025-05-13T11:15:41",
      "evaluationResultRowOutput": "Para implementar un asistente RAG (Retrieval Augmented Generation) en Globant Enterprise AI, se deben seguir las siguientes etapas y configuraciones:\n\n1. **Creaci\u00f3n y Configuraci\u00f3n del Asistente RAG </B> \n   - Durante la inicializaci\u00f3n del proyecto, se crea un asistente RAG por defecto que luego puede ser modificado o pueden crearse nuevos.  \n   - La configuraci\u00f3n del asistente RAG se realiza en la secci\u00f3n de **RAG Assistants </B>dentro del apartado de opciones del proyecto. All\u00ed se pueden personalizar detalles clave, el prompt (mensaje que define la tarea), y c\u00f3mo se realizar\u00e1 la recuperaci\u00f3n de informaci\u00f3n.  \n   - Se definen los documentos o etiquetas que ser\u00e1n la fuente para las respuestas del asistente.  \n   - Se configuran par\u00e1metros de b\u00fasqueda y b\u00fasquedas vectoriales en la etapa de recuperaci\u00f3n, incluyendo embebidos (embeddings), base de datos vectorial, y m\u00e9tricas de distancia para mejorar la precisi\u00f3n del contenido recuperado.  \n   - Se establece el nivel m\u00ednimo de confianza para las respuestas y se decide si se mostrar\u00e1n o no las fuentes utilizadas.  \n\n2. **Proceso General de Funcionamiento </B> \n   - **Ingesta de Datos: </B>Se cargan los documentos y fuentes de datos en diferentes formatos, dividi\u00e9ndolos en \"chunks\" (fragmentos) para optimizar la recuperaci\u00f3n.  \n   - **Recuperaci\u00f3n (Retrieval): </B>El asistente accede a la base de datos vectorial para buscar los fragmentos m\u00e1s relevantes basados en la consulta del usuario.  \n   - **Generaci\u00f3n: </B>Utilizando el modelo generativo LLM (Modelo de Lenguaje de grandes dimensiones) junto con el contexto recuperado, el asistente crea una respuesta coherente y contextualizada.  \n\n3. **Integraci\u00f3n en Flows </B> \n   - En la secci\u00f3n de Interacciones para crear flujos, se puede usar el componente **RAG Assistant </B>para integrarlo dentro de un flujo de conversaci\u00f3n. Este componente requiere una variable de entrada con la consulta del usuario y permite configurar una variable de salida para la respuesta. Adem\u00e1s, puede incluir el historial de conversaci\u00f3n y variables de contexto para mejorar la interacci\u00f3n.  \n   - Si la respuesta no se encuentra en los documentos, puede configurarse una acci\u00f3n alternativa.\n\nEn resumen, la implementaci\u00f3n consiste en definir un asistente RAG con sus documentos, configurar c\u00f3mo se realiza la b\u00fasqueda y generaci\u00f3n de respuesta, y finalmente integrarlo en un flujo conversacional usando los nodos espec\u00edficos disponibles en la plataforma.\n\nPara m\u00e1s detalle t\u00e9cnico y paso a paso, puede revisar los siguientes recursos en el portal Globant Enterprise AI:\n\n- Configuraci\u00f3n del asistente RAG en Project Options.  \n- Uso del nodo RAG Assistant dentro de flujos para incluir l\u00f3gica conversacional.  \n- Proceso de ingesta, chunking, b\u00fasqueda con vectores y generaci\u00f3n con LLM.\n\nEstas explicaciones se encuentran en la documentaci\u00f3n oficial de Globant Enterprise AI, especialmente en las secciones sobre RAG Assistants y Flows.\n\nReferencias:  \n[RAG Assistants configuration y explicaci\u00f3n general](https://wiki.genexus.com/enterprise-ai/wiki?64)  \n[RAG Assistants Introducci\u00f3n con detalle del proceso de ingesti\u00f3n, recuperaci\u00f3n y generaci\u00f3n](https://wiki.genexus.com/enterprise-ai/wiki?44)  \n[Interacciones para crear un flujo con el nodo RAG Assistant](https://wiki.genexus.com/enterprise-ai/wiki?494)  \n[Descripci\u00f3n general de Flows y asistentes integrados](https://wiki.genexus.com/enterprise-ai/wiki?321)",
      "evaluationResultRowStartDate": "2025-05-13T11:15:29",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "6459"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# Allowing Frontend Access for Users from Specific Domains This document describes the steps to allow end users from specific domains to access the Frontend of a project in Globant Enterprise AI. The process is based on the configuration of a [role](245.html), the [assignment of permissions](245.html) to specific assistants and the invitation of a generic user that guarantees access to the Frontend to all users who, once authenticated on the platform, belong to the defined domain. Below are the steps to configure this access. ## [Step 1: Define the Role](#Step+1%3A+Define+the+Role) In the project where you want to enable access, start by defining a role, following the steps detailed in [Creating Project Roles](245.html). In this case, the name Frontend has been used, but you can choose any name that best suits your case. ![Image](./images/658.png) ## [Step 2: Assign Permissions to the Role](#Step+2%3A+Assign+Permissions+to+the+Role) Once the role has been created, it is necessary to associate the corresponding assistants. To do so, click on PERMISSIONS in the newly created role. When accessing PERMISSIONS, you will be able to see the list of assistants and select those to which the role will have access. ## [Step 3: Add a Member](#Step+3%3A+Add+a+Member) After configuring the role and permissions, you must generate the invitation for the end user. To do so, follow the steps indicated in [Invite New Members](253.html). The objective is to allow users whose email belongs to a domain to access without the need to generate an invitation for each user of that domain. For this, you must define an invitation with the email address following the format **all-members@domain.com**. That is, all users whose email has **@domain.com** will be able to authenticate in the Frontend and will automatically be assigned the role(s) assigned to this generic member. ![Image](./images/659.png) Select the **role defined** in step 1 and click the CONFIRM button. Once the invitation has been processed, any end user with the domain **@domain.com** will be able to access the Frontend of the project. ## [Considerations](#Considerations) - **Access to Multiple Projects and Assistants**: The invited user can be added to additional projects, which will enable them to access the Frontend to multiple projects and the assistants configured within each of them.\n- **Access Deletion**: If the membership of a user who has been invited under this method is deleted, they will lose access to the projects they could only access through that generic user details. ## [Restrictions to Generic Domain Users](#Restrictions+to+Generic+Domain+Users) It is important to take into account the following restrictions when configuring domain user access: - **Assigning Backend roles is not allowed**: Users with this type of invitation cannot be assigned to Backend roles in projects. If an attempt is made to assign a Frontend or a Backend role, the system will display a warning message and only invitations for Frontend roles will be processed.\n- **It is not possible to add these users to Organizations**: This type of user can't be assigned as a member of Organizations. When trying to make this assignment, the system will display a warning stating that it is not allowed. ## [Configuration of Identity Providers excluded from this Frontend access mechanism](#Configuration+of+Identity+Providers+excluded+from+this+Frontend+access+mechanism) By default, the \u201clocal\u201d Identity Provider (GAM Auth Type) can't be used to provide access through this mechanism, so only other IDPs configured in the platform (for example, Google, Azure AD, etc.) may be used. To modify the default Identity Providers exclusion list, you must have SuperAdmin access and follow these steps: 1. Under SuperAdmin options, click on System and select Parameters.\n2. In the Parameters section, look for the parameter **DOMAIN_GENERIC_MEMBER_EXCLUDED_AUTHTYPES**.\n3. Set the parameter value to the domain you want to exclude, for example, local or any other domain or comma-separated list of domains. ## [Define the Generic User for the Invitation](#Define+the+Generic+User+for+the+Invitation) To configure the generic user that will be used to generate the invitations, follow these steps: 1. Under SuperAdmin options, click on System and select Parameters.\n2. In the Parameters section, look for the parameter **DOMAIN_GENERIC_MEMBER_USERNAME**.\n3. Set the value of this parameter to the generic user name expected for that domain. The default value is **all-members**, but you can modify it if necessary (note that this applies to the whole environment).",
          "evaluationResultRowContextDocId": "861c072e-7749-4dd6-9e1a-e3c90dd979be",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "1b6f79c8-e9b4-4e23-a23d-ea7b45d00bad",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?657",
          "evaluationResultRowContextName": "657",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "- New Average Request Time metric added in the Project Dashboard. ### January 8th - The option formerly known as 'Search Documents' has been improved and renamed to RAG Assistant (Retrieval Augmented Generation) to provide an optimized experience when searching and generating information. #### Frontend - Feedback is provided during conversations with RAG Assistants, indicating where you are in the process.\n- 'Response streaming' support for RAG Assistants.\n- Settings are hidden when selecting an assistant, except when 'Chat with LLMs' is selected. ## 2023 ### December 19th - Fixed: Too Many Redirects when accessing Playground using a browser in Spanish language. ### December 6th - New backoffice design.\n- Access to the Playground from the backoffice to chat with the assistants defined in the project.\n- Upload images for analysis with GPT-4 Vision.\n- Google Analytics support at the frontend.\n- Keep a conversation thread when chatting with documents.\n- An email notification is sent automatically when a new member is invited to join the organization or project. ### November 28th - First version officially released!! ### November 6th - The following OpenAI models are supported: GTP-4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), and GPT-4 Vision (gpt-4-vision-preview). ### October 18th - [Manage quotas per project](71.html): It is possible to set quota limits to control project expenses or usage. ### October 11th - [AI-Driven Load Balancing](32.html): The platform automatically manages the Load Balancing process when you work with generative AI providers, efficiently addressing the limits imposed by LLM platforms.\n\n- In the [Organization API](22.html), the ability to set and manage usage limits on projects through the [POST /project](22.html) and [GET /project/{id}](22.html) endpoints has been added.\n- [Quota Limit](71.html) now includes improvements such as highlighting the active quota in green, offering options to cancel active quotas, among others. ### October 17th - Rebranding to Globant Enteprise AI\n- Improvements in RAG - New option not to [compact the conversation history](297.html) - Support for Redis, MongoDB Atlas and PG Vector [vectorstores](267.html) - New endpoint to get an embedding. Check details here: [Embeddings API](422.html)\n- Playground improvements\n- File management improvements\n- New [LLMs supported](200.html) - NVIDIA provider with new models supported - nvidia.nemotron-mini-4b-instruct - meta.llama-3.1-8b-instruct - meta.llama-3.1-70b-instruct - meta.llama-3.1-405b-instruct - meta.llama-3.2-3b-instruct - Groq provider supported - groq/llama-3.1-70b-versatile - groq/llama-3.2-11b-vision-preview - groq/llama-3.2-3b-previewgroq/llama-3.2-1b-preview\n- New [embeddings models](447.html) added - Vertex AI: - vertex_ai/textembedding-gecko - vertex_ai/text-embedding-004 - vertex_ai/textembedding-gecko-multilingual - Nvidia: - nvidia/nvclip - nvidia/nv-embed-v1 - nvidia/baai.bge-m3 - nvidia/snowflake.arctic-embed-l - nvidia/nv-embedqa-mistral-7b-v2 - nvidia/embed-qa-4 - nvidia/nv-embedqa-e5-v5 ### September 25th - [Support for file](346.html) processing with prompt-based assistants. This will enable many scenarios, such as uploading documents and summarizing, extracting, and checking information, etc. Also, depending on the model used by the assistant, it will be able to process audio, video, or images.\n- [Support for multi-modal](345.html) LLMs allow processing docs, audio, video, and images in models like GPT-4o or Gemini Pro.\n- Chat with data assistants - The model used to build the queries was updated with GPT-4o, which improves the quality of the generated query. - Configure the query builder server by organization and project. This means you can connect with different DBMS from each project when building Chat with data assistants. - Show an explanation of how the query was built.\n- New [Playground Interface](349.html) design - New design - Upload documents from the front end to chat with them.\n- [Flows builder](321.html) - There will be two types of Flows, one more oriented to build a conversational UI and the other to build assistant flows. Access to these flows will only be available through [Chat API](34.html) or through the [channels](384.html) offered by Flows.\n- New models hosted in AWS Bedrock added: - Amazon Titan Express v1 - Amazon Titan Lite v1 - Anthropic Claude 3 Haiku - Anthropic Claude 3 Sonnet - Anthropic Claude 3.5 Sonnet - Cohere Command - Meta Llama 3 8B - Meta LLama 3 70B\n- It is now possible to provide clear guidance on the assistant's capabilities, allowing you to add information such as descriptions, features, and example prompts. This configuration can be done from the Backoffice, [Start Page](408.html), or WelcomeData section of the [Assistant API](35.html) and [RAG Assistants API](29.html) endpoints.\n- RAG Assistants - New endpoint in [RAG Assistants API](29.html): using [DELETE/profile/{name}/documents](29.html), all documents associated with the specified RAG Assistant are deleted. ### August 9th - Support of new models - GPT-4o mini\n- RAG Assistants - New option called [CLEANUP](60.html) allows to delete the documents associated to a RAG Assistant. - When creating a new assistant, the following defaults are updated: - [Chunk Count](69.html) updated from 2 to 5. - [Model Name](68.html) from gpt-3.5-turbo to gpt-4o-mini.\n- Data Analyst Assistant - New [Conversational Module](291.html) for Data Analyst Assistant.\n- Considerations - **Important**: [max_tokens integer below minimum value](290.html) error may occur if you are using the _max_tokens_ value as -1; those Assistants must be updated to use a correct positive value. ### July 4th - Permissions by assistants in role-based access control. A new [role and permissions management system](242.html) has been implemented to provide greater flexibility and control. Please read [Migration to the new Roles and Permissions Management System](264.html).\n- Dashboard performance optimizations. Please read [Using the Request Summary Table for Improved Dashboard Performance](273.html).\n- RAG Improvements - New option to customize the [context](205.html). - Support for [Max Marginal Relevance](206.html) - Support for _similarity_hybrid_ and _semantic_hybrid_ for [Azure AISearch](207.html) - Advanced options for SelfQuery retriever prompt case. See [Self Query Use Case](150.html) - Support for Parent Document retrieval option. See [Parent Document Retrieval Strategy](204.html) ### June 10th - [Enterprise AI Proxy](19.html) is deprecated. Use [Chat API](34.html) instead.\n- Support for new LLMs - OpenAI new model GPT-4o - Models in Google Vertex - Gemini 1.0 Pro - Gemini 1.5 Flash preview-0514 - Gemini 1.5 Pro preview-0514 - Claude 3 Haiku - Claude 3 Opus - Claude 3 Sonnet\n- RAG Improvements - New option to initialize RAG Assistant based on another when creating a new RAG Assistant. - New option to export document list in View Documents over a RAG Assistant. - Added filter options when browsing Documents. - SelfQuery RAG retriever partial support for a [customized Prompt](150.html). - Support for text-embedding-004 in Google models to generate the embeddings.\n- Deprecated [Assistant API](35.html) endpoints. - /assistant/text/begin - /assistant/text\n- Support to deploy in Google Cloud Platform. ### May 8th - New [Chat with Data Assistant](159.html).\n- New [Ingestion SDK](172.html) to automate document ingestion in RAG assistants.\n- New models hosted in NVIDIA platform supported. See [Supported Chat Models](200.html) for more details.\n- New option to export information about projects and members available for the organization administrator.\n- New API to extend dataset for [Data Analyst Assistant 1.0](89.html).\n- New filter by user email in Requests.\n- Update default to use _text-embedding-3-small_ OpenAI Embeddings for new RAG assistants.\n- Support for gemini-1.5-pro-preview-0409 model added. ### April 3rd - New [API Assistant](110.html).\n- New [Data Analyst Assistant 1.0](89.html).\n- The user Dashboard displays the email to identify the most active user or the one who generates the most costs. ### March 11th - GeneXus Identity Provider is implemented, expanding the login options in the [Backoffice](42.html) of the production environment. This allows for login not only with Google but also with Apple or GeneXus Account.\n- It is possible to customize the icon for each assistant. ### February 29th - [Frontend](82.html) improvements in UI/UX.\n- Option to get feedback from end users when interacting with [RAG Assistant](44.html).\n- Gemini Pro LLM support.\n- New [Dashboard](562.html) with user metrics.\n\n# Updates Log This log shows the most important fixes or features added to the platform. ## Log ## 2025 ### April 10th - New [Globant Enterprise AI Lab](972.html) - The Lab is designed for defining, managing, and orchestrating autonomous AI agents. It provides a standardized model for representing agents, their capabilities, and their interactions within complex workflows. The core components of the Lab include: - Agents & Tools: This module allows for the definition and management of individual agents and their resources, such as skills and tools. It serves as a central hub for cataloging and managing the agent workforce. - Agentic Processes: This component enables the definition of processes based on tasks executed by Agents. These workflows facilitate collaboration among agents to achieve larger objectives. More details at [How to create an Agentic Process](985.html). - Agent RunTime: This module provides the execution environment for agentic workflows, where agents perform tasks based on their skills and interact with artifacts, driven by events and the flow of knowledge. - The Lab aims to meet the growing demand for intelligent, self-sufficient AI agents capable of collaborating and solving complex problems with minimal human intervention. It offers a flexible and adaptable model, allowing for the creation and management of a diverse range of agents, from co-pilots working alongside humans to fully automated agents executing complex tasks. Implemented as a module of Globant Enterprise AI, the Lab supports the development of intelligent agents that can work autonomously or in collaboration with humans and other agents.\n- New features in Flows - Agent Integration Component: You can now directly integrate agents created with the AI Lab into a Flow. These agents can be exposed through platforms like WhatsApp, Teams, or Slack. - [File Upload Support](1053.html) from WhatsApp: Flows now support receiving file attachments such as documents, images, audio, and video directly from WhatsApp interactions. - Audio and Video Attachment in Web Chat: The Web Chat component now allows users to attach audio and video files, enhancing the interaction experience.\n- New Features for the Data Analyst Agent - Reduced Configuration Requirements: The setup needed to enable the assistant to respond to a wide range of questions has been minimized. - Enhanced Analysis Module: An additional analysis module has been incorporated to complement the responses with relevant business conclusions and interpretations of the obtained data.\n- New metrics to track processed tokens. - A new view has been added to each project dashboard to monitor token-based consumption. More details at [Tokens tab in Dashboard](965.html). - To enable this feature, you must run the initialization process described in [Reprocessing Requests and Loading the Summary Table](1015.html).\n- LLMs: - New Gemini 2.5 Pro (via providers Vertex AI and Gemini): Gemini 2.5 is Google\u2019s latest reasoning model, engineered to tackle increasingly complex challenges. This model is designed for tasks that demand advanced analytical thinking and robust problem-solving capabilities. More details at [LLM API](663.html). - Migration to Gemini 2.0 series (Vertex AI): Based on recommendations from Vertex AI, we have migrated from the legacy Gemini 1.0 and 1.5 models to the more advanced Gemini 2.0 series, offering improved performance, scalability, and integration capabilities. For comprehensive information, please refer to the [Deprecated Models](Deprecated Models). - New Azure OpenAI models (o1, o1-mini and o3-mini): We have expanded our model availability by introducing these models via Azure, providing the same high-quality capabilities as those offered through the OpenAI provider. - DeepSeek-R1 via AWS Bedrock: Recently added through a Serverless API, DeepSeek-R1 offers reliable inference with a substantial 128K token context window and up to 32K maximum output tokens. - OpenAI's new models with built-in web search tool: These specialized models integrate web search capabilities directly into the Chat Completions API, enabling them to both interpret and execute search queries in real time. - openai/gpt-4o-search-preview - openai/gpt-4o-mini-search-preview - New models - Beta only: - gemini/gemma-3-27b-it: Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 has a large, 128K context window, and multilingual support in over 140 languages. - SambaNova: - DeepSeek-R1: This Provider offers the fastest performance for running DeepSeek, processing up to 198 tokens per second per user, with a 16K token context window. The model is hosted in US data centers; privately and securely. - DeepSeek-V3-0324: This model significantly outperforms its predecessor with enhanced reasoning benchmarks, improved code executability and refined web aesthetics, and superior Chinese writing aligned with the R1 style. It also offers better multi-turn interactive rewriting, translation quality, detailed report analysis, and more accurate function calling. - QwQ-32B: SambaNova provides access to QwQ-32B-Preview, the best open source test-time compute model released by Alibaba. - Llama 4 collection by Meta: - Llama 4 Scout: A 17B-parameter multimodal MoE model with 16 experts that excels in text and image understanding. The model is currently in beta and is available via providers Nvidia, Groq and SambaNova. Via Groq, it supports a 128k tokens context window with fast inference at 460 tokens/sec, while SambaNova Cloud runs at 697 tokens/second/user. - Llama 4 Maverick: Available via Nvidia, this 17-billion-parameter model featuring 128 experts supports a 32k-token context window. ### March 10th - New LLMs: - GPT-4.5 - Claude 3.7 Sonnet (Providers Anthropic, Vertex AI and AWS Bedrock) - Updates in Gemini 2.0 series: - vertex_ai/gemini-2.0-flash-lite-preview-02-05 - vertex_ai/gemini-2.0-flash-thinking-exp-01-21\n- RAG Revision #6 - Support for o3-mini, gpt-4.5-preview, claude-3-7-sonnet-20250219, new DeepSeek, Gemini2* and sambanova LLM providers. - New pinecone provider available for [embeddings](447.html) and [rerankers](638.html). - The [CleanUp](60.html) action message has been corrected to clearly specify that it will permanently delete the RAG Assistant files and update the information in the RDS. - Added [usage](712.html) element on every response. - Improvements when changing the [LLM](69.html)/[Embeddings](68.html) settings; all models and providers are normalized to be selected from standard combo-box items; use the [override mechanism](209.html) if you need other options. - Support for [guardrails](840.html). - New [documentAggregation](298.html) property to decide how sources are grouped and returned.\n- It is possible to [provide feedback](170.html) on the response of the Chat with Data Assistant in the Frontend.\n\n- The new [Evaluation APIs](893.html) introduce key functionalities through three interconnected APIs: [DataSet API](839.html), [Evaluation Plan API](849.html), and [Evaluation Result API](856.html). This version is primarily designed for users with a data science profile and is mainly accessed via APIs, complemented by a series of Jupyter notebooks that demonstrate their use. For a comprehensive guide on how to use these APIs, you can refer to [How to evaluate an AI Assistant](881.html) and the [EvaluationAPITutorial.ipynb](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?887,,) notebook, which provide practical examples and code for working through the evaluation process.\n- File attachment support in Flows (version 0.9).\n- Support for Full Story integration in the Workspace/Playground to generate user access statistics in Full Story.\n- In the [LLM API](663.html), for models that have descriptions in the specified languages, the `descriptions` property is included in the Response, which contains the descriptions in the available languages, such as `Spanish`, `English` and `Japanese`.\n- [Data Analyst Assistant 2.0](886.html) version presents important improvements, simplifying the interaction with the data by reducing the main components to just two: **Dispatcher** and **Thinker**. In addition, the metadata structure is automatically generated when loading the datasets, streamlining the setup process. For more information, see [How to create a Data Analyst Assistant](105.html).\n- The option to consult version-specific documentation is now available. Articles with versions show the option \u201cOther document versions\u201d in the header. Clicking on \u201cOther document versions\u201d brings up a menu that allows you to choose between the most recent version (\u201cLatest\u201d) or earlier versions (e.g. \u201c2025-02 or prior\u201d). If you select a version other than \u201cLatest\u201d, a message appears: \u201cThis is not the latest version of this document; to access the latest version, click here\u201d. This message provides a direct link to the most up-to-date documentation. [Components Version Update](822.html) ### February 10th - New documentation with details about [Supported Chart Types](731.html).\n- New [Usage Limits API](802.html).\n- Flows - Logs associated with conversations are accessible from the [conversation history](378.html).\n- RAG Revision #5 - Ingestion supports new [file formats](137.html). For on-premises installations, update the _FILE_TYPES_ parameter. - New optional _cleanProcessing_ parameter when using [Parent Document Retrieval Strategy](204.html). - [Embeddings API](422.html) cache support; disabled by default from a [RAG assistant](68.html). - New [mode](68.html) property for using multimodal embeddings and creating a [Multimodal RAG](783.html).\n- New endpoint [GET /accessControl/apitoken/validate](22.html) returns information about the organization and project associated with the provided apitoken.\n- New LLMs: - Already in production - **OpenAI**: o3-mini - Already in Beta - **DeepSeek**: - deepseek/deepseek-reasoner - deepseek/deepseek-chat - azure/deepseek-r1 - nvidia/deepseek-ai-deepseek-r1 - groq/deepseek-r1-distill-llama-70b - sambanova/DeepSeek-R1-Distill-Llama-70B - Updates in Gemini 2.0 series: - gemini-2.0-flash-thinking-exp-01-21 (Via Providers Gemini and Vertex AI) - gemini/gemini-2.0-flash-lite-preview - gemini/gemini-2.0-pro-exp - vertex_ai/gemini-2.0-flash-001 - sambanova/Llama-3.1-Tulu-3-405B [Components Version Update](822.html) ### January 13th - Internationalization, [Backoffice](42.html), and frontend support for Japanese.\n- Invitations now include information about the organization and project in the subject.\n- New LLMs - Already in Production - **OpenAI**: o1 (2024-12-17 version) - Already in Beta - New Providers: **Cohere** - **Cohere**: Cohere-r\n- [Guardrails](682.html) configured by assistant.\n- [Rerank API](625.html) to semantically order a list of document chunks given a query.\n- New optional RAG [Retrieve and Rerank](620.html) adds an extra layer of precision to ensure that only the most relevant information reaches the model used in the generation step. ## 2024 ### December 12th - [Automatic Creation of Default Assistant](640.html)\n- [Organization Usage Limits](666.html): It is possible to set quota limits to control organization expenses or usage.\n- Chat with Data Assistant - [Show details](170.html) about the generated query in the Playground. - Support in [Chat API](34.html) to interact with Chat with Data Assistant.\n- Flows - Support for markdown when showing the response on the different channels supported by Flows (web, Slack, WhatsApp, and Teams). - New component for connecting flows to the agent overflow console (Human-in-the-loop) via B2Chat. Please read [How to connect a Flow to B2Chat](674.html).\n- RAG - [Chroma](537.html), [Elasticsearch](579.html), [OpenSearch](580.html) vectorstore support. - [Improvements](581.html) when processing PDF documents (update to RAG module revision #4 or higher). - Integration with [AWS Knowledge Bases](577.html) at query time.\n- Data Analyst Assistant - Option to update metadata options. - New version by default in new Data Analyst assistants.\n- New LLMs - OpenAI: gpt-4o-2024-11-20 - AWS Bedrock: Anthropic Claude 3.5 Haiku - Amazon Nova models (Micro, Lite, and Pro) - Llama 3.1 405B on Vertex AI - Beta: - Support for providers Cerebras, SambaNova and xAI (Grok models). - All new Gemini Experimental models.\n- Security - It is possible to define a user 'all-members@domain.com' to give access to the Frontend to all users of the domain '@domain.com'. Permissions are enforced based on the roles assigned to this user. Please read [Allowing Frontend Access for Users from Specific Domains](657.html) ### November 12th - Flows execution integrated into the Playground\n- New [LLMs support](200.html) - **OpenAI**: o1-preview and o1-mini - **Claude Sonnet 3.5 v2 - Providers**: Anthropic, Vertex AI, and AWS Bedrock - **Llama 3.2 models - Providers**: Vertex AI and AWS Bedrock\n- Chat with data assistants - Possibility to edit metadata, [entities](170.html), and attribute descriptions. - The Properties tab has been renamed to [Settings](170.html) along with the options that can be configured in it.\n- RAG - New [returnSourceDocuments](298.html) option to disable returning the documents section used to answer the question. - New [step](298.html) option to use the assistant as a retrieval tool. - Support for [custom history](85.html) in conversations using the _chat_history_ variable.\n- Stand-alone Frontend based on the new Playground UI - Options to customize the Frontend to use the client logo, color palette, welcome message, and descriptions. - Feature to collect feedback (thumbs up/down) in each response. - Google Analytics support.\n- Data Analyst Assistant - Support to upload large CSV files.",
          "evaluationResultRowContextDocId": "68ace6a6-8b1b-4736-a466-9ed0a67b97a9",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "6f98f1c5-b344-4849-bc8e-d328e1046745",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?31",
          "evaluationResultRowContextName": "31",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Managing Roles and Permissions in Globant Enterprise AI The management of roles and permissions is a fundamental part of security management. It allows defining which actions and functionalities users can perform according to their role. Instead of assigning permissions directly to users, permissions are managed through roles. These roles, assigned to users, control their access to actions in the [Backoffice](42.html) and also determine access to specific assistants from the [Frontend](82.html). When creating a project, three default [roles](245.html) are generated that allow different levels of access and functions: - #### [Project Member:](#Project+Member%3A) This role grants access to the Backoffice for working with the project, including creating assistants, viewing requests, viewing and creating API tokens, and more. - #### [Project Administrator:](#Project+Administrator%3A) This role includes the \"Project Member\" role; in addition, it allows working with the list of members and defining roles within the project. - #### [Access all Project Assistants:](#Access+all+Project+Assistants%3A) This role grants Frontend access to all active project assistants. Each project user is assigned one or more roles, and the management of users and permissions is carried out in the [Members section](253.html). Here you can view the list of users with access to the project, modify the assigned roles, or revoke access completely. In addition, new members can be added by entering their email address and selecting the corresponding roles. Invitations are processed automatically if the user is logged in to Globant Enterprise AI. Keep in mind that the invitation is sent by email and is valid for 72 hours if the user has never logged in; after that, it expires and a new one must be generated. The Invitation History records all actions related to the assignment and revoked roles, including the person responsible for each action. > **Note**: The management of Roles and Members is only available for users with 'Organization member' and 'Project Administrator' roles. ### [See Also](#See+Also) [Migration to the new Roles and Permissions Management System](264.html)",
          "evaluationResultRowContextDocId": "452af474-b7bc-419b-b0ce-da8ddf6fcbea",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "b2723d5b-31e5-4f88-9bd9-595000fa8391",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?242",
          "evaluationResultRowContextName": "242",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Managing Roles and Permissions in Globant Enterprise AI: Roles Roles are fundamental to managing security and permissions within Globant Enterprise AI. They define which actions and functionalities users can perform based on their responsibilities within a project. This section focuses on managing these roles, which control frontend access to assistants. ## Project Roles Overview **Location:** Roles are created and edited within each Project. To achieve this, you must enter the [Globant Enterprise AI Backoffice](42.html). On the left side of the screen, you can find the Backoffice menu. In this menu, click on Roles below \"PROJECT OPTIONS\". ![Image](./images/246.png) **Role Description:** Each project has four default roles that allow different levels of access and functions: [Project Member](242.html), [Project Administrator](242.html), [Access all Project Assistants](242.html), and [Access for Project Assistants without Membership](242.html). **Role Origin:** It takes the value System for the roles that are defined by default when creating the project. If you create a new role, the value will be user-defined. **Role Type:** Roles are categorized as \"Backend\" or \"Frontend\". Backend roles implicitly grant Frontend access. **Permissions:** Allows defining which assistants are available to each role with the 'Frontend' value in the Role Type column, ensuring that members who are assigned these roles can access only the configured assistants, from the Frontend interface. Available only for the \"Access for Project Assistants without Membership\" role and for user-defined roles. **User Reference:** Each role displays a quick reference to the users assigned to it. Clicking on the \"USERS\" column reveals the list of assigned users. ![Image](./images/247.png) **System-Defined Roles:** Backend roles are system-defined. All roles you create will be \"Frontend\" roles. ## Creating Project Roles **Default Roles:** Every new project starts with four system-defined roles. **Creating New Roles:** To create a new role, follow these steps: 1. **Select Project:** In the Project Dynamic Combo Box, select the project you want to work with.\n2. **Navigate to Roles:** Navigate to the \"Roles\" section below \"PROJECT OPTIONS\".\n3. **Create New:** Click on the \"CREATE NEW\" button. ![Image](./images/248.png) **Role Details:** When creating a new role, provide a description (which also acts as the name) and optionally an \"External Id\". The \"External Id\" defaults to the internal identifier of the role but can be used to map the role to a role in another IDP (like Microsoft Entra ID). You can update the \"External Id\" later by editing the role. ![Image](./images/249.png) **Editing and Deleting Roles:** User-defined roles can be edited (name/description, external ID) and deleted, but only if the role is not assigned to any user. ![Image](./images/250.png) ## Assigning Assistant Permissions to Roles **List of assistants:** By clicking on \"PERMISSIONS\", a window opens where you can edit the permissions of a role, selecting from the list the assistants you wish to include. ![Image](./images/251.png) **Confirmation:** To confirm, click on the ADD button. Once you have confirmed the permissions, the list of assistants included in the role is displayed. ![Image](./images/252.png) **Deleting assistants:** To remove an assistant from the list, use the \"DELETE\" action at the end of each item. ## Frontend Access with Project Roles **Assistant Visibility:** Frontend users with a specific role assigned will only see the assistants included in that role when accessing the Frontend. **Multiple Roles:** If a user has multiple Frontend roles assigned, they will see the combined list of assistants included in all their assigned roles, even if assistants are repeated across roles. **Backend role:** Users who have a Backend role can see all the project's assistants in the Frontend. This is because Backend roles imply full access to the Frontend.",
          "evaluationResultRowContextDocId": "4b5cb192-a002-4bbf-8ea7-bf0d2c52dc67",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "e98bbe56-b0c2-471f-b990-dc455a771668",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?245",
          "evaluationResultRowContextName": "245",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.8,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "a7750cf8-89f2-494c-acb0-62e7f32a8bdf",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0034383400",
      "evaluationResultRowEndDate": "2025-05-13T11:15:46",
      "evaluationResultRowOutput": "If a user has multiple Frontend roles assigned, they will see the combined list of assistants included in all their assigned roles when accessing the Frontend. This means they will have access to every assistant that is part of any of the roles they possess, even if some assistants are repeated across roles.\n\nThis access behavior contrasts with Backend roles, where users can see all the project's assistants in the Frontend because Backend roles imply full access.\n\nYou can find this explained under the section \"Frontend Access with Project Roles\" in the documentation about roles and permissions management. \n\n[2](https://wiki.genexus.com/enterprise-ai/wiki?245)",
      "evaluationResultRowStartDate": "2025-05-13T11:15:41",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "8353"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#Embeddings API Gets a vector representation of a given input that can be easily consumed by machine learning models and algorithms. Check the [[20##Generic Variables|generic variables]] needed to use the API and the [[447|supported models]]. ## Endpoints |**Method**|**Path**|**Description**|\n|---|---|---|\n|`POST`|`/embeddings`|Creates an embedding vector representing the input| ## POST/embeddings Creates an embedding vector representing the input provided. |**Parameter**|**Type**|**Description**|\n|---|---|---|\n|`input`|`string` or `array`|Input to embed, encoded as a string<sup>(4)</sup>. To embed multiple inputs in a single request, pass an array of strings. The input must not exceed the max input tokens for the model and cannot be an empty string|\n|`model`|`string`|`provider`/`modelId` to use|\n|`encoding_format`<sup>(1)</sup>|`string`|The format to return the embeddings. It can be either `float` (default) or `base64` (optional)|\n|`dimensions`<sup>(1)</sup>|`integer`|The number of dimensions the resulting output embeddings should have. Only supported in `text-embedding-3*` and later models (optional)|\n|`user`<sup>(3)</sup>|`string`|A unique identifier representing your `end-user`|\n|`input_type`<sup>(2)</sup>|`string`|Defines how the input data will be used when generating embeddings (optional)|\n|`timeout`|`integer`|The maximum time, in seconds, to wait for the API to respond. Defaults to 600 seconds| Check the valid providers in the Providers column of the [[200]]. <sup>(1)</sup>: Only supported by `openAI` provider. The maximum input token is 8191 (roughly ten standard pages of text). <sup>(2)</sup>: Check if the selected embeddings model supports this option. <sup>(3)</sup>: `openAI` specific. <sup>(4)</sup>: If the embedding model support image embeddings; pass in a base64 encoded image string to the input parameter with the associated mime type (check the samples section). You can use the `X-Saia-Cache-Enabled` (`true` or `false` default) to cache the embeddings for the model; it applies by Organization/Project. ### Request Body [regions]\n[region|Using a string]\n<pre class=\"Code\">\n{ \"model\": \"openai/text-embedding-3-small\", \"input\" : \"Help me with Globant Enterprise AI!\"\n}\n</pre> [/region]\n[region|Using an array]\n<pre class=\"Code\">\n{ \"model\": \"openai/text-embedding-3-small\", \"input\" : [\"Help me with\", \"Globant Enterprise AI!\"]\n}\n</pre>\n[/region]\n[/regions] ### Response The expected response is similar to the following: <pre class=\"Code\">\n{ \"model\": \"text-embedding-3-small\", \"data\": [ { \"embedding\": [...], \"index\": 0, \"object\": \"embedding\" },... ], \"object\": \"list\", \"usage\": { \"completion_tokens\": number, \"prompt_tokens\": number, \"total_tokens\": number, \"completion_tokens_details\": null }\n}\n</pre> ### Samples [regions]\n[region|OpenAI using one string]\n```bash\ncurl --location '$BASE_URL/embeddings' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n--data '{ \"model\": \"openai/text-embedding-3-small\", \"input\" : \"Help me with Globant Enterprise AI!\"\n}'\n```\n[/region]\n[region|OpenAI and cache]\n```bash\ncurl --location '$BASE_URL/embeddings' \\\n--header 'X-Saia-Cache-Enabled: true' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n--data '{ \"model\": \"openai/text-embedding-3-small\", \"input\" : [\"Help me with\", \"Globant Enterprise AI!\"] }'\n```\n[/region]\n[region|VertexAI and optional parameters]\n```bash\ncurl --location '$BASE_URL/embeddings' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n--data '{ \"model\": \"vertex_ai/text-embedding-004\", \"input_type\": \"SEMANTIC_SIMILARITY\", // RETRIEVAL_QUERY, RETRIEVAL_DOCUMENT and other options \"input\" : [\"Help me with\", \"Globant Enterprise AI!\"]\n}'\n```\n[/region]\n[region|Nvidia and optional parameters]\n```bash\ncurl --location '$BASE_URL/embeddings' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n--data '{ \"model\": \"nvidia/nv-embedqa-e5-v5\", \"input\" : [\"identify\", \"yourself!\"], \"input_type\": \"passage\" // passage or query\n}'\n```\n[/region]\n[region|Bedrock text and Image content]\n```bash\ncurl --location '$BASE_URL/embeddings' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\\n--data '{ \"model\": \"awsbedrock/amazon.titan-embed-image-v1\", \"input\" : [ \"Help me with Globant Enterprise AI!\", \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAIAAACQd1PeAAAAEElEQVR4nGK6HcwNCAAA//8DTgE8HuxwEQAAAABJRU5ErkJggg==\" ]\n}'\n```\n[/region]\n[/regions] ## See Also\n[[20|API Reference]] [https://platform.openai.com/docs/api-reference/embeddings|openAI Embeddings] [https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api|VertexAI Text Embeddings] {{{12}}} {{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "1dc9a910-de7a-4e43-bc70-f0bfd94d38f7",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "3b20e8e2-169c-44f9-8a9e-ba8fd0df0c5d",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?422",
          "evaluationResultRowContextName": "422",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Configuration - General Information [[65]] ## Name\nIdentifying name of the assistant that can be customized according to your preferences. ## Description\nDetailed description of the purpose and capabilities of your assistant. This is a place to provide information about how the assistant enhances the end user experience. ## Status\nIndicates whether the assistant is enabled or disabled for use. ## Embeddings Settings\nAn embedding model is a type of machine learning model designed to convert data\u2014like words, phrases, or sentences\u2014into dense numerical vectors, known as embeddings. These embeddings capture the semantic meaning of the input. Specific parameters related to embeddings and related model characteristics: ### Provider Name\nDetermines the embeddings model service provider used by your RAG assistant. This is a mandatory parameter that can take any of the following values: - **azure**: To use Azure OpenAI. - **cohere**: To use cohere models. - **vertex_ai**: To use Google Vertex AI, check valid models [https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding|here], you can set for example *embedding-001*, *text-embedding-004*, *text-embedding-005* embeddings models<sup>(1)</sup> of 768 Dimensions. - **awsbedrock**: To use embeddings models from AWS Bedrock. - **openai**: To use OpenAI. ### Model Name\nSpecific name of the model being used; [[447|supported values]] depend on the Provider Name selected. For example, if the Provider Name parameter has the \"openai\" value, Model Name takes the value 'text-embedding-ada-002' for assistants created up to 4/11/2024 and 'text-embedding-3-small' onwards. {{{216|If you need to use a model not detailed on the list, use the [[298##Not registered Providers and Models|overide mechanism]]|||}}} ### apiKey\nAPI authentication key provided to access the language model service. For example, if the Provider parameter is \"azureopenai\", you must specify \"apiKey\" with the authentication key. ### Type\nIt allows configuring the information processing approach for the embeddings model. It only applies for the following providers: - CoHere (using awsprovider): `search_document` (default), `search_query`, `classification`, `clustering`. - Google: `RETRIEVAL_DOCUMENT` (default for embeddings), `RETRIEVAL_QUERY` (default for querying), `SEMANTIC_SIMILARITY`, `CLASSIFICATION`, `CLUSTERING`, `TASK_TYPE_UNSPECIFIED` - nvidia: `passage`, `query`; check [https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/task-types|here] for more detail. ### useProxy\nUse Globant Enterprise AI proxy to get embeddings, all requests will be logged (enabled by default). ### cache\nCache the embeddings for the current Organization/Project (disabled by default), needs to enable the `useProxy` property too. ### BatchSize\nDetermines how many items are grouped in each batch when calling the embeddings model during the ingestion process. By default, `batchSize` is set to 100 items. However, you may need to adjust this parameter based on the specific embeddings model you are using. ### Mode\nDefined the mode options are: - `empty` (default) will use text embeddings. - `multimodal` support for image embeddings (notice the model must support it). ### endpoint\nThe URL pointing to the language model service (advanced). ### Dimensions\nIt allows configuring the index dimensionality in the vectorStore. Each model has a specific dimension, and the default value is 1536, associated with the OpenAI provider. <sup>(1)</sup>: Depending on the **Provider Name** and **Model Name** selected for your embeddings, you will need to check the **Dimensions** value. By default, it assumes 1536, which is valid for OpenAI *text-embedding-ada-002*, *text-embedding-3-small* and *text-embedding-3-large* (3072) shrunk to that value. Check the [[140|troubleshooting]] section for more detail. **Notes:** - Please [[274|contact the help desk ]] if you want to use a **Model Name** with a **Dimensions** value other than 1536, as it must match the **Vector Store Provider** configuration in the Index section.\n- If you change the Provider Name, Model Name, Dimensions, it is recommended to reingest all your data. {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "b496e5f2-2bce-4300-aad8-66eb65337440",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "4d4b53a1-b2bf-4398-af0a-e32a080449fe",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?68",
          "evaluationResultRowContextName": "68",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#RAG Assistants Use Guidelines This document is intended to provide guidelines on how to ask questions to get information that was ingested in a particular [[44|RAG Assistant]]. ## General Considerations ### Use Natural Language Type your question naturally just like you would ask a colleague for information. It doesn't matter if you make spelling mistakes or use different synonyms. It is important to convey your intention to the assistant correctly. **Do:** What's the correct reset procedure for the FFS security system? **Don't:** Reset FFS. ### Avoid Jargon Even though the assistant may understand technical terms, simpler language can often provide more accurate results. **Do:** How can I increase traffic to my website? **Don't:** What are the best SEO practices for SERP dominance in Q4? ### Provide Context You can add a bit of context or background to your query to help the assistant understand the scope of your request. **Do:** I'm working on a 2014 Ford Ranger and need to know how to replace the brake pads. What are the steps? **Don't:** How do I change brake pads? ### Use Clear and Concise Language Be specific when you know what you need and clearly articulate what you're looking for to receive more accurate and relevant information. When possible, avoid ambiguity by phrasing your questions or requests in a way that minimizes ambiguity. This helps the assistant to provide precise responses. **Do:** What is the process for Uruguayan citizens to apply for a tourist visa to Japan? **Don't:** How do I travel to Japan? ### Ask One Question at a Time Consider breaking it down into simpler parts for better comprehension if you have a complex query. **Do:** What is the average temperature in Paris in June? What are some popular tourist attractions there? **Don't:** What is the weather like in Paris and what should I do during my visit? ### Iterate and Refine your Questions If the initial response is different from what you need, consider refining your question based on the assistant's response. You can ask follow-up questions to complement what you are looking for. At what time does the museum close? **The museum closes at 5 PM.** Could you also tell me what days of the week is it open? **From Tuesday to Sunday.** ### Provide Alternative Phrasings Experiment with different ways of phrasing your query, including synonyms and variations, to explore the assistant's understanding. What's the best way to increase productivity at work? **If it doesn\u2019t understand the question, the assistant might provide general tips on productivity rather than specific workplace strategies.** Can you suggest specific methods for improving team productivity in an office setting? **This rephrasing clarifies that the focus is on team productivity in a professional environment, not on individual productivity tips.** I'm looking for techniques to enhance collaborative efficiency among employees in a corporate office. What are some proven strategies? ## Retrieval hyperparameters | **Parameter** | **Description** |\n|---|---|\n| Chunk Size | Size of each chunk; it affects granularity of retrieval. |\n| Chunk Overlap | Overlap between chunks. Helps capture relevant information across chunk boundaries. |\n| Top K | Maximum number of retrieved contexts to balance recall and precision. |\n| Vector Distance threshold| Filters contexts based on similarity. A stricter threshold prioritizes precision. |\n| Embedding model | Model used to convert text to embeddings. It impacts retrieval accuracy. | ## Questions that the Assistant Can't Answer - Avoid asking for information related to dates. For example, \u201cShow me the latest documents\u201d or \u201cWhat was the contract between March and April?\u201d.\n- Comparisons or contrasting information are not expected to get good results; all these use cases may require different techniques for interaction.\n- Information that is not part of the documents that were used to give context to the RAG Assistant.\n- It can't summarize or count items.\n- It is not a search tool based on keywords; for this reason, use the general recommendations in the previous section.\n- Texts in images are not processed. {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "a82ade71-fe15-43e3-9c5b-a3b2c879cea7",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "e5fad391-240d-46eb-afc8-3866e445cbc4",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?61",
          "evaluationResultRowContextName": "61",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.7,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "b8018f86-6794-461a-941f-ef05f3b130b3",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0019805200",
      "evaluationResultRowEndDate": "2025-05-13T11:15:57",
      "evaluationResultRowOutput": "The Embeddings API accepts the following types of input:\n\n- A **string**: A single text string to embed.\n- An **array of strings**: Multiple text inputs can be embedded in a single request by passing an array of strings.\n- For models that support image embeddings, a **base64 encoded image string with the associated MIME type </B>can also be passed as input.\n\nAdditional details:\n- The input must not exceed the maximum token limit for the selected model.\n- The input cannot be an empty string.\n\nThese input options allow embedding different kinds of content, including text and potentially images depending on the model capabilities.\n\nReference example for text input:\n```json\n{ \"model\": \"openai/text-embedding-3-small\", \"input\" : \"Help me with Globant Enterprise AI!\" }\n```\nReference example for multiple inputs:\n```json\n{ \"model\": \"openai/text-embedding-3-small\", \"input\" : [\"Help me with\", \"Globant Enterprise AI!\"] }\n```\nReference example for text and image inputs:\n```json\n{ \"model\": \"awsbedrock/amazon.titan-embed-image-v1\", \"input\" : [\"Help me with Globant Enterprise AI!\", \"data:image/png;base64,iVBORw0KGgo...\"] }\n```\n\nYou can also control how inputs are interpreted with optional parameters like `input_type`, but the core input is string(s) or base64 encoded image strings.\n\nThis information is detailed in the Embeddings API documentation page: [Embeddings API](https://wiki.genexus.com/enterprise-ai/wiki?422) [1].",
      "evaluationResultRowStartDate": "2025-05-13T11:15:46",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "4104"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# Managing quotas per project For each project, it is possible to configure quota limits to control the project expenses or usage. To access and configure quota limits, you need to be a member of the organization. This means you must be listed under \"Members\" in the [Organization Options](42.html) section. ![Image](./images/573.png) ## [Access and Limits Configuration](#Access+and+Limits+Configuration) To manage quota limits, follow the steps below: 1. Enter the [Globant Enterprise AI Backoffice](42.html). On the left side of the screen, you can find the Backoffice menu. In this menu, under the group of options called ORGANIZATION OPTIONS you will find the Projects section, where you can manage quota limits for each project. 2. Select the desired project and click on USAGE LIMITS. ![Image](./images/78.png) ## [Quota Status Display](#Quota+Status+Display) When you enter the installments section, you will see a screen with the following information for each quota: ![Image](./images/536.png) - **Valid from** and **Valid until**: Start and end dates of the quota.\n- **Status**: Projects can operate with or without defined quotas. If no quotas are defined, the status is **No Limits**, meaning there are no restrictions on consumption. However, once a quota is defined, this status cannot be reverted. When quotas are defined, the project can transition into one of the following statuses: - **Active**: Indicates that the quota is in use and has an available balance. This quota is highlighted in green. Only one quota can be active at a time in a project. - **Expired**: This status is displayed when a quota period definition has reached its end date. If it was marked as renewable, a new registry will be available as Active on the next renewal date. However, if it is not marked as renewable, the project will need a new limit definition to continue operating. - **Empty**: This status occurs when the **Hard limit** of an active quota has been reached, exhausting the available balance. The quota remains in green and is highlighted as active but with a full usage balance. If the quota is marked as renewable, the balance will be automatically restored at renewal time. - **Cancelled**: If a quota has been manually cancelled, it appears with an orange dot. This indicates that you have voluntarily removed the usage restriction to create a new one, or simply that you no longer want it to be active in the project. - **Type**: Type of quota, which can be daily, weekly, monthly, or Freemium. By choosing **Freemium**, a one-time or trial usage quota is established, with a specific limit of requests or costs and no expiration date. This means that the quota is non-renewable and only allows limited access to the service.\n- **Renewable?**: Indicates whether the quota is renewable at the end of its term. If this option is enabled (marked with a check), the quota is automatically renewed when it reaches its expiration date or when the available balance is exhausted (**Empty** status). If the quota is not renewable (no check), the project will require a new quota to be defined manually once the current quota has expired or its balance has been exhausted.\n- **Unit**: Unit of measure, such as requests (Requests) or costs (USD).\n- **Soft limit alert**: Sets a usage limit that, when reached, automatically sends an email notification to the project administrator.\n- **Hard limit**: Defines the maximum allowed usage limit. Once reached, the platform blocks any additional requests and logs an error.\n- **Used** and **Remaining**: Current usage and available balance. ## [Creation of a New Quota Limit](#Creation+of+a+New+Quota+Limit) To create a new quota limit, follow these steps: 1. Click the **CREATE NEW** button on the quota management screen. This will open a configuration window.\n2. Complete the configuration fields according to the required values, such as subscription type, unit of measure, alert limits, and renewal option. ![Image](./images/539.png) If you select **Freemium** as the subscription type, the **Renewable?** option will not be available. This is because Freemium represents a one-time usage or trial limit with no possibility of automatic renewal.\n3. Click on **CONFIRM** to save the quota limit, or **CANCEL** to discard the changes. ## [Actions available for active quotas](#Actions+available+for+active+quotas) In the quotas screen, you can perform the following actions on active quotas: ![Image](./images/541.png) - **UPDATE**: Allows you to modify parameters of the active quota, such as soft limit, enable or disable renewal, and increase the hard limit. If an active quota runs out of balance but is renewable, you can increase the hard limit to restore the balance without creating a new quota.\n- **CANCEL**: This option allows you to cancel the active quota, making it possible to define a new limit instead. This is useful when you want to completely modify the usage restrictions for the project. ### [Example of Updating the Hard Limit](#Example+of+Updating+the+Hard+Limit) If the **Status** of an active and renewable quota changes to **Empty** due to depletion of its balance, you can restore its availability by increasing the **Hard limit** using the **UPDATE** action. ![Image](./images/542.png) ![Image](./images/543.png)",
          "evaluationResultRowContextDocId": "3d3270f0-4ca5-455e-8b7a-58326655cf64",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "743fc306-7652-446a-9c01-9611946c36d3",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?71",
          "evaluationResultRowContextName": "71",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Organization Usage Limits The organization usage limit defines the maximum amount of money allocated for LLM consumption across the entire organization. This feature allows you to set both a soft limit for alerts and a hard limit as the maximum cap, with the option for daily, weekly, monthly renewals or Freemium. For each organization, you can configure quota limits to control project expenses or usage. To access and configure quota limits, you must have SuperAdmin access. ## Access and Limits Configuration To manage quota limits, follow these steps: 1. Enter the [[42]]. You need administrator privileges.\n2. On the left side of the screen, find the Backoffice menu.\n3. Under the SuperAdmin Options group, select the Organizations section.\n4. Use the search field to filter organizations by name. There are two options to configure the filter:<br> - **Contains:** Searches for organizations whose name contains the entered term, such as \u201cDocumTeam\u201d. - **Starts with:** Filters organizations whose names begin with the entered text. 5. Select the desired organization from the filtered list. 6. Click on USAGE LIMITS for the selected organization. [[667]] ## Quota Status Display When you enter the Quota Limit section, you will see a screen with the following information for each quota: [[668]] - **Valid from and Valid until:** Start and end dates of the quota.\n- **Status:** Organizations can operate with or without defined quotas. Possible statuses include:<br> - **No Limits:** No restrictions on consumption (default when no quotas are defined). - **Active:** Quota is in use with an available balance (highlighted in green). - **Expired:** Quota period has reached its end date. - **Empty:** Hard limit of an active quota has been reached. - **Cancelled:** Quota has been manually cancelled (orange dot). - **Type:** Can be daily, weekly, monthly, or Freemium. By choosing Freemium, a one-time or trial usage fee is set, with a specific cost limit and no expiration date. This means that the fee is not renewable and only allows limited access to the service.\n- **Renewable?:** Indicates if the quota automatically renews at the end of its term.\n- **Unit:** Measure unit (USD for organization limits).\n- **Soft limit alert:** Usage limit that triggers an email notification when reached.\n- **Hard limit:** Maximum allowed usage limit.\n- **Used and Remaining:** Current usage and available balance. Once a quota is created using the CREATE NEW button, two additional options become available: [[669]] - **UPDATE:** Allows you to modify the parameters of the active quota.\n- **CANCEL:** This option allows you to cancel the active quota, enabling the creation of a new limit. {{{216|Keep in mind that to update an existing quota, the new Hard limit value must be greater than the previous value. For example, if the previously set Hard limit was 300, when updating it, the new value must be greater than 300, such as 350 or 400. In addition, the Soft limit and Hard limit must be equal to or greater than the sum of all Hard limit and Soft limit quotas per project}}} ## Important Considerations 1. The organization limit acts as a maximum cap for all projects.\n2. When setting an organization limit, ensure it is not lower than any existing project limit, as project limits cannot exceed the organization limit.\n3. If an organization limit is defined for projects without quotas, those projects will start using the organization's account balance.\n4. Projects with specific quotas will respect their limits while adhering to the organization's limits. {{{17}}} {{{12}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "c6e6c48a-93dd-4db5-96e9-d4f9b065a282",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "85b77e60-91eb-42ea-901c-adebd52343e4",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?666",
          "evaluationResultRowContextName": "666",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "- New Average Request Time metric added in the Project Dashboard. ### January 8th - The option formerly known as 'Search Documents' has been improved and renamed to RAG Assistant (Retrieval Augmented Generation) to provide an optimized experience when searching and generating information. #### Frontend - Feedback is provided during conversations with RAG Assistants, indicating where you are in the process.\n- 'Response streaming' support for RAG Assistants.\n- Settings are hidden when selecting an assistant, except when 'Chat with LLMs' is selected. ## 2023 ### December 19th - Fixed: Too Many Redirects when accessing Playground using a browser in Spanish language. ### December 6th - New backoffice design.\n- Access to the Playground from the backoffice to chat with the assistants defined in the project.\n- Upload images for analysis with GPT-4 Vision.\n- Google Analytics support at the frontend.\n- Keep a conversation thread when chatting with documents.\n- An email notification is sent automatically when a new member is invited to join the organization or project. ### November 28th - First version officially released!! ### November 6th - The following OpenAI models are supported: GTP-4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), and GPT-4 Vision (gpt-4-vision-preview). ### October 18th - [Manage quotas per project](71.html): It is possible to set quota limits to control project expenses or usage. ### October 11th - [AI-Driven Load Balancing](32.html): The platform automatically manages the Load Balancing process when you work with generative AI providers, efficiently addressing the limits imposed by LLM platforms.\n\n- In the [Organization API](22.html), the ability to set and manage usage limits on projects through the [POST /project](22.html) and [GET /project/{id}](22.html) endpoints has been added.\n- [Quota Limit](71.html) now includes improvements such as highlighting the active quota in green, offering options to cancel active quotas, among others. ### October 17th - Rebranding to Globant Enteprise AI\n- Improvements in RAG - New option not to [compact the conversation history](297.html) - Support for Redis, MongoDB Atlas and PG Vector [vectorstores](267.html) - New endpoint to get an embedding. Check details here: [Embeddings API](422.html)\n- Playground improvements\n- File management improvements\n- New [LLMs supported](200.html) - NVIDIA provider with new models supported - nvidia.nemotron-mini-4b-instruct - meta.llama-3.1-8b-instruct - meta.llama-3.1-70b-instruct - meta.llama-3.1-405b-instruct - meta.llama-3.2-3b-instruct - Groq provider supported - groq/llama-3.1-70b-versatile - groq/llama-3.2-11b-vision-preview - groq/llama-3.2-3b-previewgroq/llama-3.2-1b-preview\n- New [embeddings models](447.html) added - Vertex AI: - vertex_ai/textembedding-gecko - vertex_ai/text-embedding-004 - vertex_ai/textembedding-gecko-multilingual - Nvidia: - nvidia/nvclip - nvidia/nv-embed-v1 - nvidia/baai.bge-m3 - nvidia/snowflake.arctic-embed-l - nvidia/nv-embedqa-mistral-7b-v2 - nvidia/embed-qa-4 - nvidia/nv-embedqa-e5-v5 ### September 25th - [Support for file](346.html) processing with prompt-based assistants. This will enable many scenarios, such as uploading documents and summarizing, extracting, and checking information, etc. Also, depending on the model used by the assistant, it will be able to process audio, video, or images.\n- [Support for multi-modal](345.html) LLMs allow processing docs, audio, video, and images in models like GPT-4o or Gemini Pro.\n- Chat with data assistants - The model used to build the queries was updated with GPT-4o, which improves the quality of the generated query. - Configure the query builder server by organization and project. This means you can connect with different DBMS from each project when building Chat with data assistants. - Show an explanation of how the query was built.\n- New [Playground Interface](349.html) design - New design - Upload documents from the front end to chat with them.\n- [Flows builder](321.html) - There will be two types of Flows, one more oriented to build a conversational UI and the other to build assistant flows. Access to these flows will only be available through [Chat API](34.html) or through the [channels](384.html) offered by Flows.\n- New models hosted in AWS Bedrock added: - Amazon Titan Express v1 - Amazon Titan Lite v1 - Anthropic Claude 3 Haiku - Anthropic Claude 3 Sonnet - Anthropic Claude 3.5 Sonnet - Cohere Command - Meta Llama 3 8B - Meta LLama 3 70B\n- It is now possible to provide clear guidance on the assistant's capabilities, allowing you to add information such as descriptions, features, and example prompts. This configuration can be done from the Backoffice, [Start Page](408.html), or WelcomeData section of the [Assistant API](35.html) and [RAG Assistants API](29.html) endpoints.\n- RAG Assistants - New endpoint in [RAG Assistants API](29.html): using [DELETE/profile/{name}/documents](29.html), all documents associated with the specified RAG Assistant are deleted. ### August 9th - Support of new models - GPT-4o mini\n- RAG Assistants - New option called [CLEANUP](60.html) allows to delete the documents associated to a RAG Assistant. - When creating a new assistant, the following defaults are updated: - [Chunk Count](69.html) updated from 2 to 5. - [Model Name](68.html) from gpt-3.5-turbo to gpt-4o-mini.\n- Data Analyst Assistant - New [Conversational Module](291.html) for Data Analyst Assistant.\n- Considerations - **Important**: [max_tokens integer below minimum value](290.html) error may occur if you are using the _max_tokens_ value as -1; those Assistants must be updated to use a correct positive value. ### July 4th - Permissions by assistants in role-based access control. A new [role and permissions management system](242.html) has been implemented to provide greater flexibility and control. Please read [Migration to the new Roles and Permissions Management System](264.html).\n- Dashboard performance optimizations. Please read [Using the Request Summary Table for Improved Dashboard Performance](273.html).\n- RAG Improvements - New option to customize the [context](205.html). - Support for [Max Marginal Relevance](206.html) - Support for _similarity_hybrid_ and _semantic_hybrid_ for [Azure AISearch](207.html) - Advanced options for SelfQuery retriever prompt case. See [Self Query Use Case](150.html) - Support for Parent Document retrieval option. See [Parent Document Retrieval Strategy](204.html) ### June 10th - [Enterprise AI Proxy](19.html) is deprecated. Use [Chat API](34.html) instead.\n- Support for new LLMs - OpenAI new model GPT-4o - Models in Google Vertex - Gemini 1.0 Pro - Gemini 1.5 Flash preview-0514 - Gemini 1.5 Pro preview-0514 - Claude 3 Haiku - Claude 3 Opus - Claude 3 Sonnet\n- RAG Improvements - New option to initialize RAG Assistant based on another when creating a new RAG Assistant. - New option to export document list in View Documents over a RAG Assistant. - Added filter options when browsing Documents. - SelfQuery RAG retriever partial support for a [customized Prompt](150.html). - Support for text-embedding-004 in Google models to generate the embeddings.\n- Deprecated [Assistant API](35.html) endpoints. - /assistant/text/begin - /assistant/text\n- Support to deploy in Google Cloud Platform. ### May 8th - New [Chat with Data Assistant](159.html).\n- New [Ingestion SDK](172.html) to automate document ingestion in RAG assistants.\n- New models hosted in NVIDIA platform supported. See [Supported Chat Models](200.html) for more details.\n- New option to export information about projects and members available for the organization administrator.\n- New API to extend dataset for [Data Analyst Assistant 1.0](89.html).\n- New filter by user email in Requests.\n- Update default to use _text-embedding-3-small_ OpenAI Embeddings for new RAG assistants.\n- Support for gemini-1.5-pro-preview-0409 model added. ### April 3rd - New [API Assistant](110.html).\n- New [Data Analyst Assistant 1.0](89.html).\n- The user Dashboard displays the email to identify the most active user or the one who generates the most costs. ### March 11th - GeneXus Identity Provider is implemented, expanding the login options in the [Backoffice](42.html) of the production environment. This allows for login not only with Google but also with Apple or GeneXus Account.\n- It is possible to customize the icon for each assistant. ### February 29th - [Frontend](82.html) improvements in UI/UX.\n- Option to get feedback from end users when interacting with [RAG Assistant](44.html).\n- Gemini Pro LLM support.\n- New [Dashboard](562.html) with user metrics.\n\n# Updates Log This log shows the most important fixes or features added to the platform. ## Log ## 2025 ### April 10th - New [Globant Enterprise AI Lab](972.html) - The Lab is designed for defining, managing, and orchestrating autonomous AI agents. It provides a standardized model for representing agents, their capabilities, and their interactions within complex workflows. The core components of the Lab include: - Agents & Tools: This module allows for the definition and management of individual agents and their resources, such as skills and tools. It serves as a central hub for cataloging and managing the agent workforce. - Agentic Processes: This component enables the definition of processes based on tasks executed by Agents. These workflows facilitate collaboration among agents to achieve larger objectives. More details at [How to create an Agentic Process](985.html). - Agent RunTime: This module provides the execution environment for agentic workflows, where agents perform tasks based on their skills and interact with artifacts, driven by events and the flow of knowledge. - The Lab aims to meet the growing demand for intelligent, self-sufficient AI agents capable of collaborating and solving complex problems with minimal human intervention. It offers a flexible and adaptable model, allowing for the creation and management of a diverse range of agents, from co-pilots working alongside humans to fully automated agents executing complex tasks. Implemented as a module of Globant Enterprise AI, the Lab supports the development of intelligent agents that can work autonomously or in collaboration with humans and other agents.\n- New features in Flows - Agent Integration Component: You can now directly integrate agents created with the AI Lab into a Flow. These agents can be exposed through platforms like WhatsApp, Teams, or Slack. - [File Upload Support](1053.html) from WhatsApp: Flows now support receiving file attachments such as documents, images, audio, and video directly from WhatsApp interactions. - Audio and Video Attachment in Web Chat: The Web Chat component now allows users to attach audio and video files, enhancing the interaction experience.\n- New Features for the Data Analyst Agent - Reduced Configuration Requirements: The setup needed to enable the assistant to respond to a wide range of questions has been minimized. - Enhanced Analysis Module: An additional analysis module has been incorporated to complement the responses with relevant business conclusions and interpretations of the obtained data.\n- New metrics to track processed tokens. - A new view has been added to each project dashboard to monitor token-based consumption. More details at [Tokens tab in Dashboard](965.html). - To enable this feature, you must run the initialization process described in [Reprocessing Requests and Loading the Summary Table](1015.html).\n- LLMs: - New Gemini 2.5 Pro (via providers Vertex AI and Gemini): Gemini 2.5 is Google\u2019s latest reasoning model, engineered to tackle increasingly complex challenges. This model is designed for tasks that demand advanced analytical thinking and robust problem-solving capabilities. More details at [LLM API](663.html). - Migration to Gemini 2.0 series (Vertex AI): Based on recommendations from Vertex AI, we have migrated from the legacy Gemini 1.0 and 1.5 models to the more advanced Gemini 2.0 series, offering improved performance, scalability, and integration capabilities. For comprehensive information, please refer to the [Deprecated Models](Deprecated Models). - New Azure OpenAI models (o1, o1-mini and o3-mini): We have expanded our model availability by introducing these models via Azure, providing the same high-quality capabilities as those offered through the OpenAI provider. - DeepSeek-R1 via AWS Bedrock: Recently added through a Serverless API, DeepSeek-R1 offers reliable inference with a substantial 128K token context window and up to 32K maximum output tokens. - OpenAI's new models with built-in web search tool: These specialized models integrate web search capabilities directly into the Chat Completions API, enabling them to both interpret and execute search queries in real time. - openai/gpt-4o-search-preview - openai/gpt-4o-mini-search-preview - New models - Beta only: - gemini/gemma-3-27b-it: Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 has a large, 128K context window, and multilingual support in over 140 languages. - SambaNova: - DeepSeek-R1: This Provider offers the fastest performance for running DeepSeek, processing up to 198 tokens per second per user, with a 16K token context window. The model is hosted in US data centers; privately and securely. - DeepSeek-V3-0324: This model significantly outperforms its predecessor with enhanced reasoning benchmarks, improved code executability and refined web aesthetics, and superior Chinese writing aligned with the R1 style. It also offers better multi-turn interactive rewriting, translation quality, detailed report analysis, and more accurate function calling. - QwQ-32B: SambaNova provides access to QwQ-32B-Preview, the best open source test-time compute model released by Alibaba. - Llama 4 collection by Meta: - Llama 4 Scout: A 17B-parameter multimodal MoE model with 16 experts that excels in text and image understanding. The model is currently in beta and is available via providers Nvidia, Groq and SambaNova. Via Groq, it supports a 128k tokens context window with fast inference at 460 tokens/sec, while SambaNova Cloud runs at 697 tokens/second/user. - Llama 4 Maverick: Available via Nvidia, this 17-billion-parameter model featuring 128 experts supports a 32k-token context window. ### March 10th - New LLMs: - GPT-4.5 - Claude 3.7 Sonnet (Providers Anthropic, Vertex AI and AWS Bedrock) - Updates in Gemini 2.0 series: - vertex_ai/gemini-2.0-flash-lite-preview-02-05 - vertex_ai/gemini-2.0-flash-thinking-exp-01-21\n- RAG Revision #6 - Support for o3-mini, gpt-4.5-preview, claude-3-7-sonnet-20250219, new DeepSeek, Gemini2* and sambanova LLM providers. - New pinecone provider available for [embeddings](447.html) and [rerankers](638.html). - The [CleanUp](60.html) action message has been corrected to clearly specify that it will permanently delete the RAG Assistant files and update the information in the RDS. - Added [usage](712.html) element on every response. - Improvements when changing the [LLM](69.html)/[Embeddings](68.html) settings; all models and providers are normalized to be selected from standard combo-box items; use the [override mechanism](209.html) if you need other options. - Support for [guardrails](840.html). - New [documentAggregation](298.html) property to decide how sources are grouped and returned.\n- It is possible to [provide feedback](170.html) on the response of the Chat with Data Assistant in the Frontend.\n\n- The new [Evaluation APIs](893.html) introduce key functionalities through three interconnected APIs: [DataSet API](839.html), [Evaluation Plan API](849.html), and [Evaluation Result API](856.html). This version is primarily designed for users with a data science profile and is mainly accessed via APIs, complemented by a series of Jupyter notebooks that demonstrate their use. For a comprehensive guide on how to use these APIs, you can refer to [How to evaluate an AI Assistant](881.html) and the [EvaluationAPITutorial.ipynb](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?887,,) notebook, which provide practical examples and code for working through the evaluation process.\n- File attachment support in Flows (version 0.9).\n- Support for Full Story integration in the Workspace/Playground to generate user access statistics in Full Story.\n- In the [LLM API](663.html), for models that have descriptions in the specified languages, the `descriptions` property is included in the Response, which contains the descriptions in the available languages, such as `Spanish`, `English` and `Japanese`.\n- [Data Analyst Assistant 2.0](886.html) version presents important improvements, simplifying the interaction with the data by reducing the main components to just two: **Dispatcher** and **Thinker**. In addition, the metadata structure is automatically generated when loading the datasets, streamlining the setup process. For more information, see [How to create a Data Analyst Assistant](105.html).\n- The option to consult version-specific documentation is now available. Articles with versions show the option \u201cOther document versions\u201d in the header. Clicking on \u201cOther document versions\u201d brings up a menu that allows you to choose between the most recent version (\u201cLatest\u201d) or earlier versions (e.g. \u201c2025-02 or prior\u201d). If you select a version other than \u201cLatest\u201d, a message appears: \u201cThis is not the latest version of this document; to access the latest version, click here\u201d. This message provides a direct link to the most up-to-date documentation. [Components Version Update](822.html) ### February 10th - New documentation with details about [Supported Chart Types](731.html).\n- New [Usage Limits API](802.html).\n- Flows - Logs associated with conversations are accessible from the [conversation history](378.html).\n- RAG Revision #5 - Ingestion supports new [file formats](137.html). For on-premises installations, update the _FILE_TYPES_ parameter. - New optional _cleanProcessing_ parameter when using [Parent Document Retrieval Strategy](204.html). - [Embeddings API](422.html) cache support; disabled by default from a [RAG assistant](68.html). - New [mode](68.html) property for using multimodal embeddings and creating a [Multimodal RAG](783.html).\n- New endpoint [GET /accessControl/apitoken/validate](22.html) returns information about the organization and project associated with the provided apitoken.\n- New LLMs: - Already in production - **OpenAI**: o3-mini - Already in Beta - **DeepSeek**: - deepseek/deepseek-reasoner - deepseek/deepseek-chat - azure/deepseek-r1 - nvidia/deepseek-ai-deepseek-r1 - groq/deepseek-r1-distill-llama-70b - sambanova/DeepSeek-R1-Distill-Llama-70B - Updates in Gemini 2.0 series: - gemini-2.0-flash-thinking-exp-01-21 (Via Providers Gemini and Vertex AI) - gemini/gemini-2.0-flash-lite-preview - gemini/gemini-2.0-pro-exp - vertex_ai/gemini-2.0-flash-001 - sambanova/Llama-3.1-Tulu-3-405B [Components Version Update](822.html) ### January 13th - Internationalization, [Backoffice](42.html), and frontend support for Japanese.\n- Invitations now include information about the organization and project in the subject.\n- New LLMs - Already in Production - **OpenAI**: o1 (2024-12-17 version) - Already in Beta - New Providers: **Cohere** - **Cohere**: Cohere-r\n- [Guardrails](682.html) configured by assistant.\n- [Rerank API](625.html) to semantically order a list of document chunks given a query.\n- New optional RAG [Retrieve and Rerank](620.html) adds an extra layer of precision to ensure that only the most relevant information reaches the model used in the generation step. ## 2024 ### December 12th - [Automatic Creation of Default Assistant](640.html)\n- [Organization Usage Limits](666.html): It is possible to set quota limits to control organization expenses or usage.\n- Chat with Data Assistant - [Show details](170.html) about the generated query in the Playground. - Support in [Chat API](34.html) to interact with Chat with Data Assistant.\n- Flows - Support for markdown when showing the response on the different channels supported by Flows (web, Slack, WhatsApp, and Teams). - New component for connecting flows to the agent overflow console (Human-in-the-loop) via B2Chat. Please read [How to connect a Flow to B2Chat](674.html).\n- RAG - [Chroma](537.html), [Elasticsearch](579.html), [OpenSearch](580.html) vectorstore support. - [Improvements](581.html) when processing PDF documents (update to RAG module revision #4 or higher). - Integration with [AWS Knowledge Bases](577.html) at query time.\n- Data Analyst Assistant - Option to update metadata options. - New version by default in new Data Analyst assistants.\n- New LLMs - OpenAI: gpt-4o-2024-11-20 - AWS Bedrock: Anthropic Claude 3.5 Haiku - Amazon Nova models (Micro, Lite, and Pro) - Llama 3.1 405B on Vertex AI - Beta: - Support for providers Cerebras, SambaNova and xAI (Grok models). - All new Gemini Experimental models.\n- Security - It is possible to define a user 'all-members@domain.com' to give access to the Frontend to all users of the domain '@domain.com'. Permissions are enforced based on the roles assigned to this user. Please read [Allowing Frontend Access for Users from Specific Domains](657.html) ### November 12th - Flows execution integrated into the Playground\n- New [LLMs support](200.html) - **OpenAI**: o1-preview and o1-mini - **Claude Sonnet 3.5 v2 - Providers**: Anthropic, Vertex AI, and AWS Bedrock - **Llama 3.2 models - Providers**: Vertex AI and AWS Bedrock\n- Chat with data assistants - Possibility to edit metadata, [entities](170.html), and attribute descriptions. - The Properties tab has been renamed to [Settings](170.html) along with the options that can be configured in it.\n- RAG - New [returnSourceDocuments](298.html) option to disable returning the documents section used to answer the question. - New [step](298.html) option to use the assistant as a retrieval tool. - Support for [custom history](85.html) in conversations using the _chat_history_ variable.\n- Stand-alone Frontend based on the new Playground UI - Options to customize the Frontend to use the client logo, color palette, welcome message, and descriptions. - Feature to collect feedback (thumbs up/down) in each response. - Google Analytics support.\n- Data Analyst Assistant - Support to upload large CSV files.",
          "evaluationResultRowContextDocId": "68ace6a6-8b1b-4736-a466-9ed0a67b97a9",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "9e14c049-0208-4b1b-bb33-9343254edbe5",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?31",
          "evaluationResultRowContextName": "31",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Managing Roles and Permissions in Globant Enterprise AI: Roles Roles are fundamental to managing security and permissions within Globant Enterprise AI. They define which actions and functionalities users can perform based on their responsibilities within a project. This section focuses on managing these roles, which control frontend access to assistants. ## Project Roles Overview **Location:** Roles are created and edited within each Project. To achieve this, you must enter the [Globant Enterprise AI Backoffice](42.html). On the left side of the screen, you can find the Backoffice menu. In this menu, click on Roles below \"PROJECT OPTIONS\". ![Image](./images/246.png) **Role Description:** Each project has four default roles that allow different levels of access and functions: [Project Member](242.html), [Project Administrator](242.html), [Access all Project Assistants](242.html), and [Access for Project Assistants without Membership](242.html). **Role Origin:** It takes the value System for the roles that are defined by default when creating the project. If you create a new role, the value will be user-defined. **Role Type:** Roles are categorized as \"Backend\" or \"Frontend\". Backend roles implicitly grant Frontend access. **Permissions:** Allows defining which assistants are available to each role with the 'Frontend' value in the Role Type column, ensuring that members who are assigned these roles can access only the configured assistants, from the Frontend interface. Available only for the \"Access for Project Assistants without Membership\" role and for user-defined roles. **User Reference:** Each role displays a quick reference to the users assigned to it. Clicking on the \"USERS\" column reveals the list of assigned users. ![Image](./images/247.png) **System-Defined Roles:** Backend roles are system-defined. All roles you create will be \"Frontend\" roles. ## Creating Project Roles **Default Roles:** Every new project starts with four system-defined roles. **Creating New Roles:** To create a new role, follow these steps: 1. **Select Project:** In the Project Dynamic Combo Box, select the project you want to work with.\n2. **Navigate to Roles:** Navigate to the \"Roles\" section below \"PROJECT OPTIONS\".\n3. **Create New:** Click on the \"CREATE NEW\" button. ![Image](./images/248.png) **Role Details:** When creating a new role, provide a description (which also acts as the name) and optionally an \"External Id\". The \"External Id\" defaults to the internal identifier of the role but can be used to map the role to a role in another IDP (like Microsoft Entra ID). You can update the \"External Id\" later by editing the role. ![Image](./images/249.png) **Editing and Deleting Roles:** User-defined roles can be edited (name/description, external ID) and deleted, but only if the role is not assigned to any user. ![Image](./images/250.png) ## Assigning Assistant Permissions to Roles **List of assistants:** By clicking on \"PERMISSIONS\", a window opens where you can edit the permissions of a role, selecting from the list the assistants you wish to include. ![Image](./images/251.png) **Confirmation:** To confirm, click on the ADD button. Once you have confirmed the permissions, the list of assistants included in the role is displayed. ![Image](./images/252.png) **Deleting assistants:** To remove an assistant from the list, use the \"DELETE\" action at the end of each item. ## Frontend Access with Project Roles **Assistant Visibility:** Frontend users with a specific role assigned will only see the assistants included in that role when accessing the Frontend. **Multiple Roles:** If a user has multiple Frontend roles assigned, they will see the combined list of assistants included in all their assigned roles, even if assistants are repeated across roles. **Backend role:** Users who have a Backend role can see all the project's assistants in the Frontend. This is because Backend roles imply full access to the Frontend.",
          "evaluationResultRowContextDocId": "4b5cb192-a002-4bbf-8ea7-bf0d2c52dc67",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "e8488000-5c10-44ca-84cf-4b049124678b",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?245",
          "evaluationResultRowContextName": "245",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.5,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "be8b94fa-eb8b-4c88-a16a-7624a725e4c6",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0039011600",
      "evaluationResultRowEndDate": "2025-05-13T11:16:04",
      "evaluationResultRowOutput": "To configure quota limits for projects in Globant Enterprise AI, you need to be a member of the organization. Specifically, you must be listed under \"Members\" in the Organization Options section to access and manage quota limits per project.\n\nThis means that at minimum, membership in the organization is required, which implies having sufficient permissions to manage organization-related settings, typically associated with administrative roles within the organization. For configuring organization-wide quotas, SuperAdmin access is required, but for project-level quota configuration, being a member of the organization is the stated prerequisite.\n\nThis is summarized in the documentation here:\n\n> \"To access and configure quota limits, you need to be a member of the organization. This means you must be listed under 'Members' in the Organization Options section.\"  \n> [source](https://wiki.genexus.com/enterprise-ai/wiki?71)\n\nFor organization-level quota limits (which apply across all projects), SuperAdmin privileges are required: \n\n> \"To access and configure quota limits, you must have SuperAdmin access.\"  \n> [source](https://wiki.genexus.com/enterprise-ai/wiki?666)\n\nTherefore, the role requirement depends on the scope:\n\n- For **project quota limits**: being a member of the organization (having membership under Organization Options).\n- For **organization quota limits**: SuperAdmin role is required.\n\n[1](https://wiki.genexus.com/enterprise-ai/wiki?71)  \n[6](https://wiki.genexus.com/enterprise-ai/wiki?666)",
      "evaluationResultRowStartDate": "2025-05-13T11:15:57",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "8974"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# Start Page The Globant Enterprise AI Start Page allows offering end users a clear guide to the assistant's capabilities and examples of use. To achieve this, the Start Page allows you to define the following elements that guide end users in their interaction with the assistant: - Define a welcome message that clearly describes the purpose of the assistant. - Describe the assistant's key functions, explaining the problems it is designed to solve, the actions it can perform correctly and its known limitations, ensuring that the end user has realistic expectations about its performance.\n- Add sample prompts that guide the user on how to interact with the assistant. These prompts can be specific requests such as \"Show me the composition of the company\u2019s profit during the last quarter\" or \"Create a bar chart with the sales data for the first 6 months of the year\", which provide insight into the interactions that the assistant can effectively handle. These guidelines are not only fundamental to improving the end user experience, but can also be leveraged in technical scenarios such as batch evaluations or synthetic data generation, providing a clear framework for understanding and analyzing the assistant's behavior in more advanced contexts. ## Start Page Presentation Depending on the type of assistant you are configuring, the layout of the Start Page may vary slightly. There are two presentations: one for the Assistants ([Data Analyst Assistant](886.html), [API Assistant](110.html), [Chat with Data Assistant](159.html)) and one for the [RAG Assistants](44.html). Despite these visual differences, both versions share the same essential functionalities. To configure the Start Page for both Assistants and RAG Assistants, first go to the corresponding page in the [Backoffice](42.html) to work with your data and click on 'UPDATE'. After that, depending on the type of assistant, the following tabs will be displayed: - Assistants: 'General Information' and 'Start Page'.\n- RAG Assistants: 'General Information', 'Prompt', 'Retrieval' and 'Start Page'. ## Assistants In the Assistants Start Page option, first you will find the option to configure the Welcome Message, where you define a title and a description to explain how the assistant will help the end user. Clicking on the 'CONFIRM' button will save this data and the message will appear in the Playground when accessing the assistant, helping the user to get started. In addition, you can add information about specific features of the assistant using the CREATE FEATURE button. Clicking on it opens a window for you to enter a title and a short description about the feature you are describing. After entering the title and description, click on CONFIRM to save the defined feature. Once saved, the feature will be displayed in the Playground along with its description. You can also set up sample prompts by clicking on CREATE EXAMPLE PROMPT. These predefined prompts will appear in the Playground and, when selected, will be completed automatically, facilitating the end user's interaction with the assistant and showing practical suggestions to perform certain tasks. Clicking on the CREATE EXAMPLE PROMPT button opens a window to enter the following information: - Name: Specify a name for the prompt.\n- Description: This field is optional; you can add a brief explanation describing the functionality of the prompt and how it should be used.\n- Text: This field is mandatory and here you define the content that will be used as an example in the prompt. The assistant will take this text to perform the configured task. Once you have entered the information, click on CONFIRM to save the prompt. ## RAG Assistants The RAG Assistants Start Page has a different layout compared to that of Assistants. Instead of the Welcome Message section used in Assistants, for RAG Assistants this section is called Basic Information. Here you define a title and description that clearly explain how the assistant will help end users in their tasks. This information is essential to provide users with information about the assistant's capabilities. Once you have completed the title and description, click on CONFIRM to save the changes. In RAG Assistants, instead of two buttons to create functionalities and examples of prompts, there is only one button: CREATE INFORMATION ITEM. When you click on CREATE INFORMATION ITEM, a window will open where you can enter a title, a description and the text that will make up the body of the information item. In addition, in this window you must select the type of item you want to create: Prompt or Feature. If you choose Prompt, you will define specific examples that will guide the end user when interacting with the assistant. If you select Feature, you will be describing a specific property of the assistant, clearly explaining its usefulness and how it can be used. Once you have completed the required fields, click on the SAVE button, and then click on CONFIRM (in Start Page) to save your changes.",
          "evaluationResultRowContextDocId": "0e63b8cf-b481-4530-8320-a108aaca5167",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "2da1c385-8898-4450-bf0c-a2512601b0a2",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?408",
          "evaluationResultRowContextName": "408",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Configuration - Prompt ![Image](./images/66.png) ## Prompt In this section, you can set up instructions that guide the assistant on how to approach and answer questions. These instructions establish clear guidelines for the assistant to provide relevant and useful answers based on the context provided. The default value is: ```\nYou are a helpful AI assistant. Use the following pieces of context to answer the question at the end.\nIf you don't know the answer, just say you don't know. DO NOT try to make up an answer.\nIf the question is not related to the context, politely respond that you are tuned to only answer questions that are related to the context.\nUse all this context to answer: <context>\n{context}\n</context> Question: {question}\nHelpful answer in markdown gives an extensive response:\n``` Keep the `context` and `question` [variables](85.html) set because they will be replaced with the associated information before the interaction. The `context` variable can be [customized](205.html) and will reflect the chunks obtained from the VectorStore closest to the question. The `question` variable will reflect what the customer asked. The `context` and `question` variables are mandatory on a RAG prompt, otherwise the following error will be detailed: ```\nMissing {context}|{question} in prompt definition\n``` ## Chunk Count It defines how many chunks are retrieved from the VectorStore to augment the context; defaults to 5 for assistants created since 6/08/2024; 2 otherwise. Check the [Context Prompt Template](205.html) to customize how each Chunk is handled. ## History Message Count It sets the number of historical messages that are taken into account in the conversation. This is useful for tracking the interaction history and understanding the context gathered in the conversation. Note that this value refers to the end user's question and associated answer. That is, if the \"History Message Count\" value is set to 4, it means that you are interested in considering the last 4 interactions: - Previous Question\n- Previous Answer\n- Last Question\n- Last Answer The minimum value it can take is 0, which indicates that the conversation history is not of interest. When the value is greater than 0, it is used together with the \"History Prompt\" parameter. ## History Prompt It applies when the History Message Count is higher than 0; it will compact the previous interactions so as not to lose valuable context data; check [here](297.html) for more detail and how to manage the history in a RAG assistant. ## Llm Settings Configuration of the model used by the assistant to generate the answer, including the service provider, model name, temperature, maximum token limit, and other important parameters that affect how answers are generated. See available options listed in [Supported Chat Models](200.html). The LLM configuration parameters are listed below: ### Provider It indicates the language model service provider used. The possible values it can take are: \"openai\", \"azure\", \"azureopenai\", \"cohere\", \"google\", \"vertex_ai\", \"anthropic\", \"nvidia.nemo\", \"awsbedrock\". ### apiKey API authentication key provided to access the language model service. For example, if the Provider parameter is \"azureopenai\", you must specify \"apiKey\" with the authentication key. ### Model Name It specifies the name of the language model used. For example, if the Provider parameter has the value \"openai\", modelName takes the value 'gpt-3.5-turbo' and since 6/08/2024 the value 'gpt-4o-mini'. Check available models in [Supported Chat Models](200.html). **Note**: If you need to use a model not detailed on the list, use the [override mechanism](298.html). ### Temperature It controls the randomness of response generation, with values represented by decimal numbers. A value of 0.00 indicates a more deterministic generation, while higher values introduce more variability and surprise in the generated answers. ### maxTokens It sets the maximum number of text units in an answer. The setting is expressed as an integer; for example, 1000. Depending on the selected model, you will have to check for the maximum supported value. ### topP It controls the diversity of generated answers. It is expressed as decimal numeric values, where a value of 1 indicates that all possible options are considered. This parameter allows regulating the diversity of the generated answers, determining the extent of the options considered during the text generation process. ### Stream This value is deprecated. Check [Chat API](34.html) for streaming usage. ### Verbose It enables or disables the level of detail in the information generated by the model during the process. It can be configured with Boolean values, where 'true' enables the detailed output and 'false' disables it. ### Cache It controls whether to search the cache if the query has already been made. It is configured with Boolean values, where 'true' enables caching and 'false' disables it. Note that all assistant configuration values are taken into account in the cache search. ### frequencyPenalty It establishes the text generation process by favoring diversity and reducing the repetition of frequent terms, thus optimizing the quality and variety of the generated answers. It is configured with decimal numeric values from -2.0 to 2.0. More information: [https://platform.openai.com/docs/api-reference/chat/create#chat-create-frequency_penalty](https://platform.openai.com/docs/api-reference/chat/create#chat-create-frequency_penalty) ### presencePenalty It establishes the text generation process by regulating the appearance of certain terms in the answers. It is configured with decimal numeric values from -2.0 to 2.0. More information: [https://platform.openai.com/docs/api-reference/chat/create#chat-create-frequency_penalty](https://platform.openai.com/docs/api-reference/chat/create#chat-create-frequency_penalty) ### endpoint It configures the URL pointing to the specific server or service where the language models are hosted. The endpoint configuration varies depending on the service provider used. For example, for 'openai', the URL must be precise and contain the specific engine version: `https://api.openai.com/v1/chat/completions` In the case of 'azureopenai', the URL must include the name, implementation, and version: `https://{name}.openai.azure.com/openai/deployments/{implementation}/chat/completions?api-version={version}` ### type It configures the response format. The valid options are: - **Empty string (\"\")**: This is the default option and returns the response in plain text.\n- **\"json_object\"**: This option returns the response in JSON format. To use this option, you must explicitly state in your prompt that you want a JSON output. For more details on how to format your prompt for JSON output, see [Create chat completion](https://platform.openai.com/docs/api-reference/chat/create#chat-create-response_format). Here's an example of how to use the \"json_object\" option:",
          "evaluationResultRowContextDocId": "ffcb6c2f-2302-4f07-99c0-8fb70eff5bde",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "3d8c9049-61ba-4ad3-b436-084ce9418063",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?69",
          "evaluationResultRowContextName": "69",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Using variables in Prompts Prompt Engineering is a crucial skill for shaping precise and contextually accurate AI responses. By crafting effective prompts, you can ensure optimal performance and end user satisfaction from your AI assistants. One powerful technique in prompt engineering is the use of variables. Incorporating variables within prompts allows for dynamic content substitution at runtime, enabling the system to respond contextually to user inputs and enhancing the overall conversational experience. It is very common to add context, user, metadata, and other information that should be added to the prompt in specific places and will vary. Also, it should be composable from the caller. From a Prompt Perspective, it is to inject data with the desired values at certain locations. ## Variable Syntax Use the `{variableName}` pattern as a placeholder for variable substitution. Variable names can include alphanumeric characters, underscores, and hyphens. ### Defining and Using Variables 1. **Define Variables:** You need to define the variables you want to use in your prompt. This involves specifying the variable name and its corresponding value.\n2. **Placeholders in Prompts:** Insert the variable placeholders (`{variableName}`) within your prompt where you want the values to be substituted.\n3. **Runtime Substitution:** When you execute the prompt, the defined variables will be substituted into their corresponding placeholders. ## Reserved Variable Names The following variable names are reserved: - `{inputText}:` This is the raw input text provided by the user. It's the unprocessed text before any potential modifications or analysis.\n- `{context}:` This is the context provided to the AI assistant. It can be any relevant information, such as text, code, or data. Within [[69##Prompt|RAG Assistants]], this variable represents the chunks obtained from the vectorstore closest to the end user's question. The context is automatically populated based on the most relevant information retrieved from the vectorstore.\n- `{chat_history}:` This represents the history of the conversation between the user and the AI assistant. It includes all previous messages exchanged.\n- `{question}:` This is the question the user asks the AI assistant. > **Note**: The `{context}`, `{chat_history}`, `{question}` variables are specifically used in the [[69|RAG Assistant]]. For RAG Assistants, `{context}` and `{question}` are fixed and required. ## Manual History Management If you need to manually [[297|manage the chat history]] in a RAG Assistant, use the `chat_history` variable to send the complete history as a list of JSON messages, representing the interactions between the user and the assistant. For example, to include the following history in an interaction: ```\nuser: hi\nassistant: Hello username: saia@globant.com, how can I help you?\nuser: What date is today?\nassistant: Today is 2024/01/01\n``` You must add the variable with the JSON representation of that interaction when calling the [[34]]: ```\n~~\n\"variables\": [ {\"key\": \"chat_history\", \"value\": \"[{\\\"content\\\": \\\"hi\\\",\\\"role\\\": \\\"user\\\"}, {\\\"content\\\": \\\"Hello username: saia@globant.com, how can I help you?\\\", \\\"role\\\": \\\"assistant\\\"}, {\\\"content\\\": \\\"What date is today?\\\", \\\"role\\\": \\\"user\\\"}, {\\\"content\\\": \\\"Today is 2024/01/01\\\", \\\"role\\\": \\\"assistant\\\"}]\"}\n]~~\n``` ## Variable Substitution with the Assistants API From the API caller, you will not only need to define the prompt strategy on where to place the definition of the variables, but also to fill in the associated data at runtime. The [[34]] supports the use of an optional `variables` collection definition using the `key`(string)/`value`(string) pattern for each element. This allows for dynamic substitution of variable definitions within a Prompt. The supported endpoints are: - [[34##Endpoint|/chat]]\n- [[34##Endpoint|\u202f/chat/completions]] ## Sample Imagine you want to create an AI assistant with variables to specify the type of assistant, a disclaimer, the context for the question, and a hint for the response style. The format would be as follows: ```\n{ ... \"prompt\": You are a {type}. Use the following pieces of context to answer the question at the end. {disclaimer} {hardDisclaimer} {context} Question: {question} {responseHint}\n}\n``` Where the variables that have been defined are the following: - `{type}`: The type of AI assistant (e.g., \"helpful\", \"creative\", \"technical\").\n- `{disclaimer}`: A disclaimer about the assistant's limitations.\n- `{hardDisclaimer}`: A stricter disclaimer about the assistant's limitations.\n- `{responseHint}`: A hint for the assistant's response style (e.g., \"Provide a concise answer\", \"Explain in detail\"). Once you have created an assistant with variables, you need to pass the actual values for those variables when you use the assistant: ```\n~~{ ... \"variables\": [ {\"key\": \"type\",\"value\": \"helpful AI assistant\"}, {\"key\": \"disclaimer\",\"value\": \"If you don't know the answer, just say you don't know. DO NOT try to make up an answer.\"}, {\"key\": \"hardDisclaimer\",\"value\": \"If the question is not related to the context, politely respond that you are tuned to only answer questions that are related to the context.\"}, {\"key\": \"responseHint\",\"value\": \"Helpful answer in Markdown:\"} ]\n}~~\n``` In the resulting prompt after the variable substitution process, the `{context}` and `{question}` variables are left unchanged: ```\nYou are a helpful AI assistant. Use the following pieces of {context} to answer the question at the end.\nIf you don't know the answer, just say you don't know. DO NOT try to make up an answer.\nIf the question is not related to the context, politely respond that you are tuned to only answer questions that are related to the context. {context} Question: {question}\nHelpful answer in Markdown:\n``` The Assistant execution process will continue as usual. ## JSON Value Notation When sending JSON values in the `value` element, it's important to use the correct notation, which involves using an extra set of delimiters: - **JSON Objects:** Use curly brackets (`{{}}`).\n- **JSON Arrays:** Use square brackets (`~~[]~~`). **Note:** [[29|RAG Assistants API]] requires double curly brackets (`{{}}`) for JSON values. ### Sample Below are samples of valid values: #### String case ```\n{\"key\": \"type\", \"value\": \"AI assistant\"}\n``` #### Array case ```\n# Assistant sample\n~~{\"key\": \"properties\", \"value\": \"[{\\\"name\\\": \\\"something\\\", \\\"value\\\": \\\"something else\\\"}]\"}~~\n# RAG Assistants sample\n~~{\"key\": \"properties\", \"value\": \"[{{\\\"name\\\": \\\"something\\\", \\\"value\\\": \\\"something else\\\"}}]\"}~~\n``` #### Object case ```\n# Assistant samplecase\n{\"key\": \"item\", \"value\": \"{\\\"name\\\": \\\"something\\\"}\"}\n# RAG Assistants sample\n{\"key\": \"item\", \"value\": \"{{\\\"name\\\": \\\"something\\\"}}\"}\n``` ## Considerations - Parameter substitution applies to all prompts, including those used with [[44|RAG Assistants]] (depending on the [[70|retriever]]).\n- Parameter substitution is Case Insensitive.\n\n- No server-side validation is performed if variables are not substituted.\n- Use the [[563|Console Requests]] section to review the resulting prompt. {{{12}}} {{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "f71ef0f7-7bf4-49ab-964c-aae3d83f3c4a",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "689e63ec-ea3a-4612-9158-db5f1c551693",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?85",
          "evaluationResultRowContextName": "85",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Globant Enterprise AI Frontend **Warning**: Globant Enterprise AI Frontend will be discontinued as of February 10th, 2025. [Playground Interface](349.html) is on the way to replace this. The Globant Enterprise AI Frontend offers an experience similar to what you will find in ChatGPT or Bard, as well as an intuitive interface for interacting with Assistants and LLMs. Additionally, it allows you to create your own prompts and adjust settings in the User Menu to have full control over your customized experience. The frontend screen is divided into three sections: 1. Assistants Panel and User Menu\n2. Chat Panel\n3. Contextual Panel ![Image](./images/72.png) Below is a detailed description of the steps for establishing conversations with Assistants and LLMs, together with information on the operation of each Panel. ## 1. Assistants Panel and User Menu This Panel contains: - **In the upper part:** the Assistants section.\n- **In the lower part:** the User Menu. ### Assistants Section In the upper part of the Panel there is a list of Assistants, including those defined by default and those defined by your organization. The default Assistants include \"Search with Google\", \"Chat with LLMs\", and \"Create Images\". ![Image](./images/73.png) In this section, you may perform the following actions: - Generate a New Chat by selecting the Assistant with which you wish to interact, or click on \u201c+ New chat\u201d.\n- Create New Folders by clicking on the option available next to \u201c+ New Chat\u201d (whose image contains a folder and \u201c+\u201d).\n- Select, search for, and organize Chats. Once you have created folders and chats, you may select them, make searches, and even organize your chats. For example, you may drag your chats to the corresponding folder to keep everything organized and accessible.\n- Rename your conversations. To do so, click on the name of the chat, select the pencil image, and then edit based on your preferences. Lastly, confirm the change by pressing Enter or by clicking on the check mark.\n- Delete a specific chat when you no longer need it. To do this, you must select the chat to be deleted, click on the Trash icon, and end by confirming with a click on the check mark. Bear in mind that the chat history is stored in the local storage of the browser you are using; therefore, if you change your browser, you will not view the same data. ### User Menu The User Menu is in the lower part of the Panel, with additional options to manage and customize your experience. ![Image](./images/74.png) It includes the following options: - **Clear Conversations:** it deletes all conversations.\n- **Settings:** it adjusts the Panel's appearance by changing between light and dark modes.\n- **Log out:** to exit the system. ## 2. Chat Panel The Chat Panel is the main interface for interacting with Assistants and LLMs. Here, you will be able to view your queries and the responses generated. You may use the Chat Box to enter your queries and enjoy fluent communications. The upper part of the Chat Panel shows the name of the Assistant selected for chatting. The following image shows that the assistant selected is \"Search with Google\". ![Image](./images/75.png) You will find arrows on both sides of the selected Assistant name: - Left arrow: collapses the Assistants Panel.\n- Right arrow: collapses the Prompt Panel. Below is a detail of the steps you should follow to: - Chat with Assistants.\n- Chat with LLMs. ### Chat with Assistants 1. **Select Assistant:** select the Assistant you wish to interact with in the Assistants Panel.\n2. **Write Message:** write your message in the Chat Box of the Chat Panel in accordance with the Assistant\u2019s guidelines.\n3. **Send Message:** press Enter or click on the arrow that will send your message to the Assistant.\n4. **Receive Reply:** the Assistant's reply will appear in the Chat Panel. ### Chat with LLMs 1. **Select Assistant:** select the LLM Assistant with which you wish to interact in the Assistants Panel. ![Image](./images/76.png) 2. **Customize the LLM:** in the Chat Panel, you may select the desired LLM Assistant (the default value is GPT-3.5) and adjust the conversation style using the scroll bar.\n3. **Write Message:** write your message in the Chat Box of the Chat Panel.\n4. **Send Message:** press Enter or click on the arrow that will send your message.\n5. **Receive Reply:** the reply will appear in the Chat Panel. ## Contextual Panel The Contextual Panel allows you to simplify daily or repetitive tasks by creating prompts. The prompts are stored locally in your computer to facilitate the execution of actions with just one click. ![Image](./images/77.png) The Prompt Panel allows the following actions: - **Create a New Prompt:** click on \"+ New prompt\" and complete the data requested on this screen: ![Image](./images/79.png) - **Create New Folders:** as in the Chat Panel, you may organize your Prompts in customized folders. To create a new folder, click on the option appearing beside \u201c+ New prompt\u201d (with an image of a folder and \u201c+\u201d).\n- **Select, Search, and Organize Prompts:** once you have created folders and prompts you may select, do searches, and even organize your prompts. Simplify your experience by dragging your prompts to the corresponding folders. To use a defined prompt go to the Chat Panel, write a slash (/) in the Chat Box, and select the desired prompt: ![Image](./images/80.png)",
          "evaluationResultRowContextDocId": "23ee2522-aa47-4ddb-b9ff-fb9f7b9ed494",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "8a4be602-9561-4f53-992d-ea51e020a9ba",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?82",
          "evaluationResultRowContextName": "82",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# File support for multimodal models Multimodal models allow interactions that go beyond traditional text-based prompts. These models can understand and process information from a variety of sources, including text, file documents, images, audio, and video. This means they can analyze, process, and generate content based on a richer and more complete set of inputs. Globant Enterprise AI implements multimodal models, allowing you to interact with models such as Gemini and OpenAI in a completely new way. By combining text with files, a world of possibilities opens up for analyzing, processing, and generating content. These models can directly receive files alongside your prompts, eliminating the need to manually extract and inject file content. This direct file integration offers several advantages. - Simplifies file handling. You no longer need to extract and inject file content into the prompt.\n- Files are sent directly to the model, streamlining the process and enabling direct model interaction.\n- Empowers you to interact directly with files within your prompts, making it easier to analyze, process, and generate content based on their contents. This eliminates the need for manual extraction and injection, simplifying your workflow.\n- Leverages the information contained in files to provide more comprehensive and insightful responses, enhancing their capabilities. ## File References in Prompts You can reference files in your prompts using a specific format: `{file:filename}`. For example, to reference a file named \"report.pdf\" you would write: `{file:report}`. > **Note**: This reference format only works for files that have already been uploaded to your project. If you're uploading a file for the first time, you'll need to use the standard file upload mechanism within the platform. ## How Files are Managed in Your Project 1. **File Storage**: All files uploaded to your project are stored securely and can be accessed by your prompts.\n2. **File Access**: When you reference a file in your prompt, the system automatically retrieves it and sends it to the multimodal model.\n3. **File Types**: The system supports a wide range of file types, including text documents, images, audio files, and more. ## Sample Suppose you have a document called \"product_specs.txt\" loaded in your project. You can use it in your prompt as follows: ```plaintext\n\"Please summarize the key features of the product described in {file:product_specs}\".\n``` The multimodal model will then access the \"product_specs.txt\" file and use its contents to generate a summary of the product features. ## See Also [Supported models and file formats](406.html)",
          "evaluationResultRowContextDocId": "381b4a5d-cb99-40b8-a58e-e67f82082418",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "97dc153b-66d9-4ae9-83ad-265d6ed1d10e",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?345",
          "evaluationResultRowContextName": "345",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# File Support in Assistants File Support in Assistants allows you to work with various file types, including documents, images, videos, and audio, to perform tasks like summarizing documents, extracting information, comparing files, and generating text based on file content. This opens up a world of possibilities for using assistants in new and innovative ways. ## Scenarios The end user should be able to interact with assistants using files in the following ways: - Summarize a document: Provide a concise summary of the document's content.\n- Extract specific information: Extract specific data points from a document, such as the amount, client name, and deadline from a contract.\n- Compare documents: Compare the information in two documents, identifying matching and differing clauses.\n- Generate text based on document content: Create new text based on the content of a document.\n- Ask questions based on uploaded documents: Similar to [RAG Assistants](44.html), but with the ability to upload new documents from the [Playground Interface](349.html) to add to the assistant's context. ## Functionality File Support in Assistants provides a robust set of features for managing and using files: - File Management: You can manage files associated with each project using CRUD operations (create, read, update, delete). A project-level API is available for file management, using the project identifier as the key. You can also configure a storage limit per project to manage storage space effectively.\n- File Referencing: You can reference files within assistant prompts using variables like `{file:filename}` or `{file:*}`. For example, you can use the prompt: \"Compare `{file:id1}` with `{file:id2}`\". The system seamlessly handles file uploads, context window size, and summarization, regardless of whether the model is multimodal or not.\n- File Types: File support extends beyond documents to include images and videos, using a similar syntax: `{file:image1}` or `{file:video1}`. For example, you can use the prompt: \"Extract the text from image `{file:id1}` and summarize its content\".\n- Frontend Upload: End users can easily select existing files or upload new ones when interacting with file-handling assistants. ## How to Use File Support in Assistants 1. Choose your assistant: Select the assistant that best suits your needs.\n2. Upload your file: Click on the \u201cUpload File\u201d button and select the file you want to use. The assistant will recognize the file type and process it accordingly.\n3. Write your prompt: Enter your instructions for the assistant in the prompt box. Be clear and specific about what you want the assistant to do.\n4. Use file references: To reference the uploaded file in your prompt, use the following syntax: - Documents: `{file:filename}` (for example, {file:contract}) - Images: `{file:image1}` - Videos: `{file:video1}` - Audio: `{file:audio1}`\n5. Submit your prompt: Click on the arrow icon (or press Enter on your keyboard) to send your prompt to the assistant.\n6. Review the results: The assistant will process your request and display the results. ## Samples ### Document: - Prompt: \u201cSummarize the key points of the document `{file:contract}`\u201d.\n- Response: \u201cThe contract outlines the terms of service for a software subscription, including pricing, payment terms, and support options\u201d. ### Image: - Prompt: \u201cDescribe the image `{file:product_image}`\u201d.\n- Response: \u201cThe image shows a red sports car with black leather interior\u201d. ### Video: - Prompt: \u201cSummarize the main points of the video `{file:tutorial}`\u201d.\n- Response: \u201cThe video demonstrates how to use the new software feature for creating custom reports\u201d. ### Audio: - Prompt: \u201cTranscribe the audio file `{file:meeting_recording}`\u201d.\n- Response: The assistant will provide a text transcript of the audio recording.",
          "evaluationResultRowContextDocId": "97680bd3-a82d-4d62-87ac-fba252965c5b",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "d2103ed4-dda7-4db1-8a6e-2a3b7a09d014",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?346",
          "evaluationResultRowContextName": "346",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.8,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "ceec6d24-da62-4167-ad42-e55157f83f4d",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0033128200",
      "evaluationResultRowEndDate": "2025-05-13T11:16:10",
      "evaluationResultRowOutput": "To reference a file in a prompt within Globant Enterprise AI, you use the syntax:\n\n```\n{file:filename}\n```\n\nFor example, if you have a file named \"report.pdf\" uploaded to your project, you reference it in the prompt as `{file:report}` (without the file extension).\n\n**Key points:**\n- The file must already be uploaded to your project for this reference format to work.\n- This method works for different file types including documents, images, videos, and audio.\n- You can reference multiple files by using their respective names, e.g., `{file:image1}`, `{file:video1}`, `{file:audio1}`, etc.\n\n**Sample prompt:**\n\n```\nPlease summarize the key features of the product described in {file:product_specs}.\n```\n\nThis will make the assistant access the content of the file \"product_specs.txt\" and generate a summary based on it.\n\nThis referencing method applies to both general multimodal models and assistants supporting file uploads.\n\nFor more details, see the documentation on File Support in Assistants here:  \nhttps://wiki.genexus.com/enterprise-ai/wiki?346  \nand File support for multimodal models here:  \nhttps://wiki.genexus.com/enterprise-ai/wiki?345\n\n[3](https://wiki.genexus.com/enterprise-ai/wiki?345)  \n[4](https://wiki.genexus.com/enterprise-ai/wiki?346)",
      "evaluationResultRowStartDate": "2025-05-13T11:16:04",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "7552"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# Managing Roles and Permissions in Globant Enterprise AI The management of roles and permissions is a fundamental part of security management. It allows defining which actions and functionalities users can perform according to their role. Instead of assigning permissions directly to users, permissions are managed through roles. These roles, assigned to users, control their access to actions in the [Backoffice](42.html) and also determine access to specific assistants from the [Frontend](82.html). When creating a project, three default [roles](245.html) are generated that allow different levels of access and functions: - #### [Project Member:](#Project+Member%3A) This role grants access to the Backoffice for working with the project, including creating assistants, viewing requests, viewing and creating API tokens, and more. - #### [Project Administrator:](#Project+Administrator%3A) This role includes the \"Project Member\" role; in addition, it allows working with the list of members and defining roles within the project. - #### [Access all Project Assistants:](#Access+all+Project+Assistants%3A) This role grants Frontend access to all active project assistants. Each project user is assigned one or more roles, and the management of users and permissions is carried out in the [Members section](253.html). Here you can view the list of users with access to the project, modify the assigned roles, or revoke access completely. In addition, new members can be added by entering their email address and selecting the corresponding roles. Invitations are processed automatically if the user is logged in to Globant Enterprise AI. Keep in mind that the invitation is sent by email and is valid for 72 hours if the user has never logged in; after that, it expires and a new one must be generated. The Invitation History records all actions related to the assignment and revoked roles, including the person responsible for each action. > **Note**: The management of Roles and Members is only available for users with 'Organization member' and 'Project Administrator' roles. ### [See Also](#See+Also) [Migration to the new Roles and Permissions Management System](264.html)",
          "evaluationResultRowContextDocId": "452af474-b7bc-419b-b0ce-da8ddf6fcbea",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "79a2d230-1b5f-4abb-a0d0-767583de5048",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?242",
          "evaluationResultRowContextName": "242",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Migration to the new Roles and Permissions Management System This document describes how to migrate to the new [Roles and Permissions Management System](242.html). This is a one-time process that transfers data from existing users and projects to the new role and permission structures. ### [Prerequisites](#Prerequisites) Before you begin the manual migration process, make sure that the latest version of the \"Security Configuration\" module is installed. This module is typically installed/updated automatically during the initial deployment of the environment (the first access to the console login). To verify it, navigate to SUPERADMIN OPTIONS > System > System Modules and make sure that the \"Security Configuration\" module does not show any available updates. ## [Manually Migrating to the New System](#Manually+Migrating+to+the+New+System) If the migration process is not automated, you can manually migrate to the new system by following these steps: 1. Navigate to SUPERADMIN OPTIONS > System > System Modules. ![Image](./images/265.png)\n2. Find the \"System\" module and click on **Install/Update**.\n3. The module will begin the migration process in the background. While the module will indicate that it has been updated, you need to check System > Operation Logs for a summary of the migration. This summary will indicate whether the migration was successful. ![Image](./images/266.png)\n4. Review the Migration Log: ![Image](./images/268.png) - **Start of Process:** When the migration process begins, a log entry will appear indicating this. For example, in the \"Data\" column of the log, you will see a message similar to \"The migration process is about to start...\". - **End of Process:** When the process is complete, another log entry will appear. Verify that the beginning of the \"Data\" description indicates that the execution was successful. For example, the message could be \"OVERALL EXECUTION: SUCCESS = true\". - **Viewing Details:** You can view the details of the log entry to see all the steps that were executed during the migration. The log will detail the steps and results, such as \"Populating OrganizationRole table...\".\n5. If the migration process fails, the \"Log level\" will be set to \"ERROR\". The details of the log will indicate where the process failed. In the event of a failure, no changes will be committed to the database, and the system will continue to function normally without the migrated schema. You will need to analyze the log to determine the cause of the failure.\n6. If the process is completed successfully, no further action is required. ### [Using the Old Role-Permissions Structures](#Using+the+Old+Role-Permissions+Structures) If you need to revert to the old role-permission structures after successfully migrating, you can set the following system parameter: 1. Navigate to SUPERADMIN OPTIONS > System > Parameters. ![Image](./images/269.png)\n2. Configure the following parameter: **Name:** USE_OLD_ROLES_SCHEMA **Description:** Enables the active use of the old Role-Permissions structures **Type:** String **Value S:** 1 ## [Migrated Projects](#Migrated+Projects) When migrating a GeneXus Enterprise AI environment to use the new Roles and Permissions Management System, the behavior for all existing projects is preserved. However, this is not optional for new projects. ### [Activating the New Scheme](#Activating+the+New+Scheme) To activate the new scheme in a previously defined project, follow these steps: 1. **Organization Member Role:** Make sure you have the \"Organization Member\" role.\n2. **Project Options:** Navigate to the \"Project\" section under \"ORGANIZATION OPTIONS\". ![Image](./images/243.png) As seen in the image, the projects MyNew and Default have a red exclamation mark (!). This red warning symbol indicates that these projects have not been migrated to the new roles and permissions scheme.\n3. **Update:** Click on \"UPDATE\".\n4. **Check User Permissions:** Select the checkbox labeled \"Check user permissions by API?\". ![Image](./images/244.png) **Notes:** - Existing projects do not have a \"Project Administrator\" role. An \"Organization Member\" must assign this role to Backend users who need to manage project members and roles.\n- Once checked and confirmed, this cannot be reversed for that project.",
          "evaluationResultRowContextDocId": "107ebd9d-01d0-47c5-af7e-1873ab45709c",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "96b05369-6f86-4f75-aadc-98d84f0732b1",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?264",
          "evaluationResultRowContextName": "264",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Managing Roles and Permissions in Globant Enterprise AI: Roles Roles are fundamental to managing security and permissions within Globant Enterprise AI. They define which actions and functionalities users can perform based on their responsibilities within a project. This section focuses on managing these roles, which control frontend access to assistants. ## Project Roles Overview **Location:** Roles are created and edited within each Project. To achieve this, you must enter the [Globant Enterprise AI Backoffice](42.html). On the left side of the screen, you can find the Backoffice menu. In this menu, click on Roles below \"PROJECT OPTIONS\". ![Image](./images/246.png) **Role Description:** Each project has four default roles that allow different levels of access and functions: [Project Member](242.html), [Project Administrator](242.html), [Access all Project Assistants](242.html), and [Access for Project Assistants without Membership](242.html). **Role Origin:** It takes the value System for the roles that are defined by default when creating the project. If you create a new role, the value will be user-defined. **Role Type:** Roles are categorized as \"Backend\" or \"Frontend\". Backend roles implicitly grant Frontend access. **Permissions:** Allows defining which assistants are available to each role with the 'Frontend' value in the Role Type column, ensuring that members who are assigned these roles can access only the configured assistants, from the Frontend interface. Available only for the \"Access for Project Assistants without Membership\" role and for user-defined roles. **User Reference:** Each role displays a quick reference to the users assigned to it. Clicking on the \"USERS\" column reveals the list of assigned users. ![Image](./images/247.png) **System-Defined Roles:** Backend roles are system-defined. All roles you create will be \"Frontend\" roles. ## Creating Project Roles **Default Roles:** Every new project starts with four system-defined roles. **Creating New Roles:** To create a new role, follow these steps: 1. **Select Project:** In the Project Dynamic Combo Box, select the project you want to work with.\n2. **Navigate to Roles:** Navigate to the \"Roles\" section below \"PROJECT OPTIONS\".\n3. **Create New:** Click on the \"CREATE NEW\" button. ![Image](./images/248.png) **Role Details:** When creating a new role, provide a description (which also acts as the name) and optionally an \"External Id\". The \"External Id\" defaults to the internal identifier of the role but can be used to map the role to a role in another IDP (like Microsoft Entra ID). You can update the \"External Id\" later by editing the role. ![Image](./images/249.png) **Editing and Deleting Roles:** User-defined roles can be edited (name/description, external ID) and deleted, but only if the role is not assigned to any user. ![Image](./images/250.png) ## Assigning Assistant Permissions to Roles **List of assistants:** By clicking on \"PERMISSIONS\", a window opens where you can edit the permissions of a role, selecting from the list the assistants you wish to include. ![Image](./images/251.png) **Confirmation:** To confirm, click on the ADD button. Once you have confirmed the permissions, the list of assistants included in the role is displayed. ![Image](./images/252.png) **Deleting assistants:** To remove an assistant from the list, use the \"DELETE\" action at the end of each item. ## Frontend Access with Project Roles **Assistant Visibility:** Frontend users with a specific role assigned will only see the assistants included in that role when accessing the Frontend. **Multiple Roles:** If a user has multiple Frontend roles assigned, they will see the combined list of assistants included in all their assigned roles, even if assistants are repeated across roles. **Backend role:** Users who have a Backend role can see all the project's assistants in the Frontend. This is because Backend roles imply full access to the Frontend.",
          "evaluationResultRowContextDocId": "4b5cb192-a002-4bbf-8ea7-bf0d2c52dc67",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "d49082c4-c217-4a3d-89cb-bbfd5c5661cb",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?245",
          "evaluationResultRowContextName": "245",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "- New Average Request Time metric added in the Project Dashboard. ### January 8th - The option formerly known as 'Search Documents' has been improved and renamed to RAG Assistant (Retrieval Augmented Generation) to provide an optimized experience when searching and generating information. #### Frontend - Feedback is provided during conversations with RAG Assistants, indicating where you are in the process.\n- 'Response streaming' support for RAG Assistants.\n- Settings are hidden when selecting an assistant, except when 'Chat with LLMs' is selected. ## 2023 ### December 19th - Fixed: Too Many Redirects when accessing Playground using a browser in Spanish language. ### December 6th - New backoffice design.\n- Access to the Playground from the backoffice to chat with the assistants defined in the project.\n- Upload images for analysis with GPT-4 Vision.\n- Google Analytics support at the frontend.\n- Keep a conversation thread when chatting with documents.\n- An email notification is sent automatically when a new member is invited to join the organization or project. ### November 28th - First version officially released!! ### November 6th - The following OpenAI models are supported: GTP-4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), and GPT-4 Vision (gpt-4-vision-preview). ### October 18th - [Manage quotas per project](71.html): It is possible to set quota limits to control project expenses or usage. ### October 11th - [AI-Driven Load Balancing](32.html): The platform automatically manages the Load Balancing process when you work with generative AI providers, efficiently addressing the limits imposed by LLM platforms.\n\n- In the [Organization API](22.html), the ability to set and manage usage limits on projects through the [POST /project](22.html) and [GET /project/{id}](22.html) endpoints has been added.\n- [Quota Limit](71.html) now includes improvements such as highlighting the active quota in green, offering options to cancel active quotas, among others. ### October 17th - Rebranding to Globant Enteprise AI\n- Improvements in RAG - New option not to [compact the conversation history](297.html) - Support for Redis, MongoDB Atlas and PG Vector [vectorstores](267.html) - New endpoint to get an embedding. Check details here: [Embeddings API](422.html)\n- Playground improvements\n- File management improvements\n- New [LLMs supported](200.html) - NVIDIA provider with new models supported - nvidia.nemotron-mini-4b-instruct - meta.llama-3.1-8b-instruct - meta.llama-3.1-70b-instruct - meta.llama-3.1-405b-instruct - meta.llama-3.2-3b-instruct - Groq provider supported - groq/llama-3.1-70b-versatile - groq/llama-3.2-11b-vision-preview - groq/llama-3.2-3b-previewgroq/llama-3.2-1b-preview\n- New [embeddings models](447.html) added - Vertex AI: - vertex_ai/textembedding-gecko - vertex_ai/text-embedding-004 - vertex_ai/textembedding-gecko-multilingual - Nvidia: - nvidia/nvclip - nvidia/nv-embed-v1 - nvidia/baai.bge-m3 - nvidia/snowflake.arctic-embed-l - nvidia/nv-embedqa-mistral-7b-v2 - nvidia/embed-qa-4 - nvidia/nv-embedqa-e5-v5 ### September 25th - [Support for file](346.html) processing with prompt-based assistants. This will enable many scenarios, such as uploading documents and summarizing, extracting, and checking information, etc. Also, depending on the model used by the assistant, it will be able to process audio, video, or images.\n- [Support for multi-modal](345.html) LLMs allow processing docs, audio, video, and images in models like GPT-4o or Gemini Pro.\n- Chat with data assistants - The model used to build the queries was updated with GPT-4o, which improves the quality of the generated query. - Configure the query builder server by organization and project. This means you can connect with different DBMS from each project when building Chat with data assistants. - Show an explanation of how the query was built.\n- New [Playground Interface](349.html) design - New design - Upload documents from the front end to chat with them.\n- [Flows builder](321.html) - There will be two types of Flows, one more oriented to build a conversational UI and the other to build assistant flows. Access to these flows will only be available through [Chat API](34.html) or through the [channels](384.html) offered by Flows.\n- New models hosted in AWS Bedrock added: - Amazon Titan Express v1 - Amazon Titan Lite v1 - Anthropic Claude 3 Haiku - Anthropic Claude 3 Sonnet - Anthropic Claude 3.5 Sonnet - Cohere Command - Meta Llama 3 8B - Meta LLama 3 70B\n- It is now possible to provide clear guidance on the assistant's capabilities, allowing you to add information such as descriptions, features, and example prompts. This configuration can be done from the Backoffice, [Start Page](408.html), or WelcomeData section of the [Assistant API](35.html) and [RAG Assistants API](29.html) endpoints.\n- RAG Assistants - New endpoint in [RAG Assistants API](29.html): using [DELETE/profile/{name}/documents](29.html), all documents associated with the specified RAG Assistant are deleted. ### August 9th - Support of new models - GPT-4o mini\n- RAG Assistants - New option called [CLEANUP](60.html) allows to delete the documents associated to a RAG Assistant. - When creating a new assistant, the following defaults are updated: - [Chunk Count](69.html) updated from 2 to 5. - [Model Name](68.html) from gpt-3.5-turbo to gpt-4o-mini.\n- Data Analyst Assistant - New [Conversational Module](291.html) for Data Analyst Assistant.\n- Considerations - **Important**: [max_tokens integer below minimum value](290.html) error may occur if you are using the _max_tokens_ value as -1; those Assistants must be updated to use a correct positive value. ### July 4th - Permissions by assistants in role-based access control. A new [role and permissions management system](242.html) has been implemented to provide greater flexibility and control. Please read [Migration to the new Roles and Permissions Management System](264.html).\n- Dashboard performance optimizations. Please read [Using the Request Summary Table for Improved Dashboard Performance](273.html).\n- RAG Improvements - New option to customize the [context](205.html). - Support for [Max Marginal Relevance](206.html) - Support for _similarity_hybrid_ and _semantic_hybrid_ for [Azure AISearch](207.html) - Advanced options for SelfQuery retriever prompt case. See [Self Query Use Case](150.html) - Support for Parent Document retrieval option. See [Parent Document Retrieval Strategy](204.html) ### June 10th - [Enterprise AI Proxy](19.html) is deprecated. Use [Chat API](34.html) instead.\n- Support for new LLMs - OpenAI new model GPT-4o - Models in Google Vertex - Gemini 1.0 Pro - Gemini 1.5 Flash preview-0514 - Gemini 1.5 Pro preview-0514 - Claude 3 Haiku - Claude 3 Opus - Claude 3 Sonnet\n- RAG Improvements - New option to initialize RAG Assistant based on another when creating a new RAG Assistant. - New option to export document list in View Documents over a RAG Assistant. - Added filter options when browsing Documents. - SelfQuery RAG retriever partial support for a [customized Prompt](150.html). - Support for text-embedding-004 in Google models to generate the embeddings.\n- Deprecated [Assistant API](35.html) endpoints. - /assistant/text/begin - /assistant/text\n- Support to deploy in Google Cloud Platform. ### May 8th - New [Chat with Data Assistant](159.html).\n- New [Ingestion SDK](172.html) to automate document ingestion in RAG assistants.\n- New models hosted in NVIDIA platform supported. See [Supported Chat Models](200.html) for more details.\n- New option to export information about projects and members available for the organization administrator.\n- New API to extend dataset for [Data Analyst Assistant 1.0](89.html).\n- New filter by user email in Requests.\n- Update default to use _text-embedding-3-small_ OpenAI Embeddings for new RAG assistants.\n- Support for gemini-1.5-pro-preview-0409 model added. ### April 3rd - New [API Assistant](110.html).\n- New [Data Analyst Assistant 1.0](89.html).\n- The user Dashboard displays the email to identify the most active user or the one who generates the most costs. ### March 11th - GeneXus Identity Provider is implemented, expanding the login options in the [Backoffice](42.html) of the production environment. This allows for login not only with Google but also with Apple or GeneXus Account.\n- It is possible to customize the icon for each assistant. ### February 29th - [Frontend](82.html) improvements in UI/UX.\n- Option to get feedback from end users when interacting with [RAG Assistant](44.html).\n- Gemini Pro LLM support.\n- New [Dashboard](562.html) with user metrics.\n\n# Updates Log This log shows the most important fixes or features added to the platform. ## Log ## 2025 ### April 10th - New [Globant Enterprise AI Lab](972.html) - The Lab is designed for defining, managing, and orchestrating autonomous AI agents. It provides a standardized model for representing agents, their capabilities, and their interactions within complex workflows. The core components of the Lab include: - Agents & Tools: This module allows for the definition and management of individual agents and their resources, such as skills and tools. It serves as a central hub for cataloging and managing the agent workforce. - Agentic Processes: This component enables the definition of processes based on tasks executed by Agents. These workflows facilitate collaboration among agents to achieve larger objectives. More details at [How to create an Agentic Process](985.html). - Agent RunTime: This module provides the execution environment for agentic workflows, where agents perform tasks based on their skills and interact with artifacts, driven by events and the flow of knowledge. - The Lab aims to meet the growing demand for intelligent, self-sufficient AI agents capable of collaborating and solving complex problems with minimal human intervention. It offers a flexible and adaptable model, allowing for the creation and management of a diverse range of agents, from co-pilots working alongside humans to fully automated agents executing complex tasks. Implemented as a module of Globant Enterprise AI, the Lab supports the development of intelligent agents that can work autonomously or in collaboration with humans and other agents.\n- New features in Flows - Agent Integration Component: You can now directly integrate agents created with the AI Lab into a Flow. These agents can be exposed through platforms like WhatsApp, Teams, or Slack. - [File Upload Support](1053.html) from WhatsApp: Flows now support receiving file attachments such as documents, images, audio, and video directly from WhatsApp interactions. - Audio and Video Attachment in Web Chat: The Web Chat component now allows users to attach audio and video files, enhancing the interaction experience.\n- New Features for the Data Analyst Agent - Reduced Configuration Requirements: The setup needed to enable the assistant to respond to a wide range of questions has been minimized. - Enhanced Analysis Module: An additional analysis module has been incorporated to complement the responses with relevant business conclusions and interpretations of the obtained data.\n- New metrics to track processed tokens. - A new view has been added to each project dashboard to monitor token-based consumption. More details at [Tokens tab in Dashboard](965.html). - To enable this feature, you must run the initialization process described in [Reprocessing Requests and Loading the Summary Table](1015.html).\n- LLMs: - New Gemini 2.5 Pro (via providers Vertex AI and Gemini): Gemini 2.5 is Google\u2019s latest reasoning model, engineered to tackle increasingly complex challenges. This model is designed for tasks that demand advanced analytical thinking and robust problem-solving capabilities. More details at [LLM API](663.html). - Migration to Gemini 2.0 series (Vertex AI): Based on recommendations from Vertex AI, we have migrated from the legacy Gemini 1.0 and 1.5 models to the more advanced Gemini 2.0 series, offering improved performance, scalability, and integration capabilities. For comprehensive information, please refer to the [Deprecated Models](Deprecated Models). - New Azure OpenAI models (o1, o1-mini and o3-mini): We have expanded our model availability by introducing these models via Azure, providing the same high-quality capabilities as those offered through the OpenAI provider. - DeepSeek-R1 via AWS Bedrock: Recently added through a Serverless API, DeepSeek-R1 offers reliable inference with a substantial 128K token context window and up to 32K maximum output tokens. - OpenAI's new models with built-in web search tool: These specialized models integrate web search capabilities directly into the Chat Completions API, enabling them to both interpret and execute search queries in real time. - openai/gpt-4o-search-preview - openai/gpt-4o-mini-search-preview - New models - Beta only: - gemini/gemma-3-27b-it: Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 has a large, 128K context window, and multilingual support in over 140 languages. - SambaNova: - DeepSeek-R1: This Provider offers the fastest performance for running DeepSeek, processing up to 198 tokens per second per user, with a 16K token context window. The model is hosted in US data centers; privately and securely. - DeepSeek-V3-0324: This model significantly outperforms its predecessor with enhanced reasoning benchmarks, improved code executability and refined web aesthetics, and superior Chinese writing aligned with the R1 style. It also offers better multi-turn interactive rewriting, translation quality, detailed report analysis, and more accurate function calling. - QwQ-32B: SambaNova provides access to QwQ-32B-Preview, the best open source test-time compute model released by Alibaba. - Llama 4 collection by Meta: - Llama 4 Scout: A 17B-parameter multimodal MoE model with 16 experts that excels in text and image understanding. The model is currently in beta and is available via providers Nvidia, Groq and SambaNova. Via Groq, it supports a 128k tokens context window with fast inference at 460 tokens/sec, while SambaNova Cloud runs at 697 tokens/second/user. - Llama 4 Maverick: Available via Nvidia, this 17-billion-parameter model featuring 128 experts supports a 32k-token context window. ### March 10th - New LLMs: - GPT-4.5 - Claude 3.7 Sonnet (Providers Anthropic, Vertex AI and AWS Bedrock) - Updates in Gemini 2.0 series: - vertex_ai/gemini-2.0-flash-lite-preview-02-05 - vertex_ai/gemini-2.0-flash-thinking-exp-01-21\n- RAG Revision #6 - Support for o3-mini, gpt-4.5-preview, claude-3-7-sonnet-20250219, new DeepSeek, Gemini2* and sambanova LLM providers. - New pinecone provider available for [embeddings](447.html) and [rerankers](638.html). - The [CleanUp](60.html) action message has been corrected to clearly specify that it will permanently delete the RAG Assistant files and update the information in the RDS. - Added [usage](712.html) element on every response. - Improvements when changing the [LLM](69.html)/[Embeddings](68.html) settings; all models and providers are normalized to be selected from standard combo-box items; use the [override mechanism](209.html) if you need other options. - Support for [guardrails](840.html). - New [documentAggregation](298.html) property to decide how sources are grouped and returned.\n- It is possible to [provide feedback](170.html) on the response of the Chat with Data Assistant in the Frontend.\n\n- The new [Evaluation APIs](893.html) introduce key functionalities through three interconnected APIs: [DataSet API](839.html), [Evaluation Plan API](849.html), and [Evaluation Result API](856.html). This version is primarily designed for users with a data science profile and is mainly accessed via APIs, complemented by a series of Jupyter notebooks that demonstrate their use. For a comprehensive guide on how to use these APIs, you can refer to [How to evaluate an AI Assistant](881.html) and the [EvaluationAPITutorial.ipynb](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?887,,) notebook, which provide practical examples and code for working through the evaluation process.\n- File attachment support in Flows (version 0.9).\n- Support for Full Story integration in the Workspace/Playground to generate user access statistics in Full Story.\n- In the [LLM API](663.html), for models that have descriptions in the specified languages, the `descriptions` property is included in the Response, which contains the descriptions in the available languages, such as `Spanish`, `English` and `Japanese`.\n- [Data Analyst Assistant 2.0](886.html) version presents important improvements, simplifying the interaction with the data by reducing the main components to just two: **Dispatcher** and **Thinker**. In addition, the metadata structure is automatically generated when loading the datasets, streamlining the setup process. For more information, see [How to create a Data Analyst Assistant](105.html).\n- The option to consult version-specific documentation is now available. Articles with versions show the option \u201cOther document versions\u201d in the header. Clicking on \u201cOther document versions\u201d brings up a menu that allows you to choose between the most recent version (\u201cLatest\u201d) or earlier versions (e.g. \u201c2025-02 or prior\u201d). If you select a version other than \u201cLatest\u201d, a message appears: \u201cThis is not the latest version of this document; to access the latest version, click here\u201d. This message provides a direct link to the most up-to-date documentation. [Components Version Update](822.html) ### February 10th - New documentation with details about [Supported Chart Types](731.html).\n- New [Usage Limits API](802.html).\n- Flows - Logs associated with conversations are accessible from the [conversation history](378.html).\n- RAG Revision #5 - Ingestion supports new [file formats](137.html). For on-premises installations, update the _FILE_TYPES_ parameter. - New optional _cleanProcessing_ parameter when using [Parent Document Retrieval Strategy](204.html). - [Embeddings API](422.html) cache support; disabled by default from a [RAG assistant](68.html). - New [mode](68.html) property for using multimodal embeddings and creating a [Multimodal RAG](783.html).\n- New endpoint [GET /accessControl/apitoken/validate](22.html) returns information about the organization and project associated with the provided apitoken.\n- New LLMs: - Already in production - **OpenAI**: o3-mini - Already in Beta - **DeepSeek**: - deepseek/deepseek-reasoner - deepseek/deepseek-chat - azure/deepseek-r1 - nvidia/deepseek-ai-deepseek-r1 - groq/deepseek-r1-distill-llama-70b - sambanova/DeepSeek-R1-Distill-Llama-70B - Updates in Gemini 2.0 series: - gemini-2.0-flash-thinking-exp-01-21 (Via Providers Gemini and Vertex AI) - gemini/gemini-2.0-flash-lite-preview - gemini/gemini-2.0-pro-exp - vertex_ai/gemini-2.0-flash-001 - sambanova/Llama-3.1-Tulu-3-405B [Components Version Update](822.html) ### January 13th - Internationalization, [Backoffice](42.html), and frontend support for Japanese.\n- Invitations now include information about the organization and project in the subject.\n- New LLMs - Already in Production - **OpenAI**: o1 (2024-12-17 version) - Already in Beta - New Providers: **Cohere** - **Cohere**: Cohere-r\n- [Guardrails](682.html) configured by assistant.\n- [Rerank API](625.html) to semantically order a list of document chunks given a query.\n- New optional RAG [Retrieve and Rerank](620.html) adds an extra layer of precision to ensure that only the most relevant information reaches the model used in the generation step. ## 2024 ### December 12th - [Automatic Creation of Default Assistant](640.html)\n- [Organization Usage Limits](666.html): It is possible to set quota limits to control organization expenses or usage.\n- Chat with Data Assistant - [Show details](170.html) about the generated query in the Playground. - Support in [Chat API](34.html) to interact with Chat with Data Assistant.\n- Flows - Support for markdown when showing the response on the different channels supported by Flows (web, Slack, WhatsApp, and Teams). - New component for connecting flows to the agent overflow console (Human-in-the-loop) via B2Chat. Please read [How to connect a Flow to B2Chat](674.html).\n- RAG - [Chroma](537.html), [Elasticsearch](579.html), [OpenSearch](580.html) vectorstore support. - [Improvements](581.html) when processing PDF documents (update to RAG module revision #4 or higher). - Integration with [AWS Knowledge Bases](577.html) at query time.\n- Data Analyst Assistant - Option to update metadata options. - New version by default in new Data Analyst assistants.\n- New LLMs - OpenAI: gpt-4o-2024-11-20 - AWS Bedrock: Anthropic Claude 3.5 Haiku - Amazon Nova models (Micro, Lite, and Pro) - Llama 3.1 405B on Vertex AI - Beta: - Support for providers Cerebras, SambaNova and xAI (Grok models). - All new Gemini Experimental models.\n- Security - It is possible to define a user 'all-members@domain.com' to give access to the Frontend to all users of the domain '@domain.com'. Permissions are enforced based on the roles assigned to this user. Please read [Allowing Frontend Access for Users from Specific Domains](657.html) ### November 12th - Flows execution integrated into the Playground\n- New [LLMs support](200.html) - **OpenAI**: o1-preview and o1-mini - **Claude Sonnet 3.5 v2 - Providers**: Anthropic, Vertex AI, and AWS Bedrock - **Llama 3.2 models - Providers**: Vertex AI and AWS Bedrock\n- Chat with data assistants - Possibility to edit metadata, [entities](170.html), and attribute descriptions. - The Properties tab has been renamed to [Settings](170.html) along with the options that can be configured in it.\n- RAG - New [returnSourceDocuments](298.html) option to disable returning the documents section used to answer the question. - New [step](298.html) option to use the assistant as a retrieval tool. - Support for [custom history](85.html) in conversations using the _chat_history_ variable.\n- Stand-alone Frontend based on the new Playground UI - Options to customize the Frontend to use the client logo, color palette, welcome message, and descriptions. - Feature to collect feedback (thumbs up/down) in each response. - Google Analytics support.\n- Data Analyst Assistant - Support to upload large CSV files.",
          "evaluationResultRowContextDocId": "68ace6a6-8b1b-4736-a466-9ed0a67b97a9",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "ed1a821d-5d8c-4458-8b0d-622f5fb7e32f",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?31",
          "evaluationResultRowContextName": "31",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.9,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "d3cb48b8-fabc-40f0-807d-fdd5bb047767",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0036325400",
      "evaluationResultRowEndDate": "2025-05-13T11:16:20",
      "evaluationResultRowOutput": "The purpose of managing roles and permissions in Globant Enterprise AI is to control and define which actions and functionalities users can perform based on their assigned roles. Rather than assigning permissions directly to individual users, permissions are grouped into roles, and these roles are then assigned to users. This method enables efficient and scalable security management by controlling access to different parts of the system \u2014 like the Backoffice or specific assistants in the Frontend \u2014 according to the responsibilities and needs of each user.\n\nIn summary:\n- Roles allow defining access rights and permissions centrally.\n- Users inherit these permissions by being assigned one or more roles.\n- This approach simplifies security management and ensures users only have appropriate access.\n- It controls access to project actions and assistants both in the Backoffice and Frontend interfaces.\n\nThis also supports different levels of access, such as Project Member, Project Administrator, and access to assistants, creating a structured and flexible permission system.\n\nReference: [Managing Roles and Permissions in Globant Enterprise AI](https://wiki.genexus.com/enterprise-ai/wiki?242) [2](https://wiki.genexus.com/enterprise-ai/wiki?242), and details on roles in [Managing Roles and Permissions in Globant Enterprise AI: Roles](https://wiki.genexus.com/enterprise-ai/wiki?245) [3](https://wiki.genexus.com/enterprise-ai/wiki?245).",
      "evaluationResultRowStartDate": "2025-05-13T11:16:10",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "8405"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# API Assistant API Assistant is a feature of Globant Enterprise AI that allows you to define assistants to chat and interact with any external REST API compliant with the OpenAPI 3.0 specification or higher. This assistant leverages AI models and function calling capabilities to provide a dynamic way to chat with APIs using natural language. ## [Main features](#Main+features) Defining an API Assistant provides a range of advanced features designed to empower interaction with external APIs in an efficient and secure manner. Key features include: - **Compatibility with OpenAPI 3.0:** It is possible to interact with any API that is built following the OpenAPI 3.0 specification or later.\n- **Customizable instructions:** The user can provide specific text instructions to guide the AI assistant in making decisions and accurately handling requests.\n- **Model settings:** It allows you to adjust the AI model settings, such as temperature and the maximum number of tokens generated, to enhance and customize the assistant's answers according to the user's needs.\n- **Security:** It ensures the security of interactions through the use of API tokens. These tokens, which can be of Basic or Bearer type, are used in the Authorization header to authenticate and authorize requests, thus protecting information and data.\n- **Automated function calls:** It returns answers efficiently by making only the necessary API calls. This is achieved by Globant Enterprise AI's intelligent model and automated function calls handling end user requests.\n- **Multiple API calls:** The assistant\u2019s intelligence is shown by making sequential calls to different APIs to obtain complex results. For example, upon receiving a question such as 'What is the weather like in Tokyo and Montevideo?' the assistant identifies the need to query the weather API for each city mentioned. This ability to automatically handle multiple requests ensures a seamless experience for the user, who doesn't need to intervene in the process. ## [Prerequisites](#Prerequisites) Before starting to use API Assistant, it is important to ensure that certain prerequisites are met: - You need to access Globant Enterprise AI. To do so, you can visit the [official website](https://www.genexus.com/en/products/genexus-enterprise-ai), where you will find contact information and details on how to obtain access to the platform.\n- If your API is authenticated with an API Token (either Basic or Bearer), you must configure your requests to include this token in the authorization header, specifying the correct type. For example: `-H \"Authorization: Bearer $OPENAI_API_KEY\"`. In this way, when executing the call, the token will be used for the interaction.\n- Verify that the API Specification is documented according to OpenAPI 3.0 or higher, in JSON format. This is important, as APIs that do not comply with this specification may not be fully supported.\n- Avoid setting Message streaming because it is not supported. To define or update an API Assistant, you can use the [Globant Enterprise AI Backoffice](42.html) by following the steps described in [How to use API Assistant](123.html) or using the API.",
          "evaluationResultRowContextDocId": "b3f2c7ef-2bcc-4b6c-b879-cdeee55f7e13",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "0642df15-f837-44c8-86e5-6f36d64ce94e",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?110",
          "evaluationResultRowContextName": "110",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Globant Enterprise AI API Reference Globant Enterprise AI provides various APIs that, on one hand, allow integration with Large Language Model (LLM) systems and perform actions that modify the platform's metadata. On the other hand, a different set of APIs enables interaction with defined assistants. ## Available APIs - [[34]]\n- [[1180]]\n- [[663]]\n- [[22]]\n- [[29]]\n- [[625]]\n- [[422]]\n- [[1165]]\n- [[35]]\n- [[450]]\n- [[1183]]\n- [[802]]\n- [[407]]\n- [[893]] ## Generic Variables Notice the following properties needed when using the API. | Variable | Description |\n|---|---|\n| `$BASE_URL` | The base URL for your Globant Enterprise AI installation, for example, `https://api.saia.ai` or the value provided to you. |\n| `$SAIA_APITOKEN` | An API token generated for each project. | ## Authentication In order to use the API, you need to authenticate each request using [[564]]. These tokens are managed in [[42]]. To authenticate your requests, you need to provide your token via HTTP Bearer Token. This means that your token is encoded in the username field, and the password field should be left empty. Once authenticated, you will be able to access all endpoints within your API scope. For security purposes, it is strongly recommended that you do not share your API tokens with anyone and revoke them immediately if they are compromised. ## Errors REST API employs the widely accepted practice of using HTTP response codes to convey the status of an API request. The codes in the 2xx range indicate that the request was successful and the server has returned the expected data. On the other hand, the codes in the 4xx range signify that the request failed because of a client-side error, such as missing or invalid parameters, unauthorized access, or any other fault in the request. The codes in the 5xx range suggest that there's an error on the server side, and the request couldn't be completed due to a server malfunction or connectivity issue. Such errors, fortunately, are infrequent in the service. By following these HTTP response codes, users can easily understand whether their API requests have succeeded or failed, and the probable causes of failure if there are any. If there is an error during the execution, all APIs return a list of errors and a status code `400*`: ```\n{ \"errors\": [ { \"id\": \"integer\", \"description\": \"string\" }, ... ]\n}\n``` ## Versioning The API versioning strategy is designed to minimize disruptions to your application when backwards-incompatible changes are introduced. Whenever changes are made to the API, a new version is released. This approach allows you to continue using the previous version of the API until you are ready to upgrade to the latest version. It is strongly recommended that you always specify the version number when making API requests to ensure the correct behavior of your application. You can find the latest version number in the documentation or by contacting the support team. By using versioning, you ensure that your application remains stable and functional, while still providing access to the latest API features and functionality. ## Cache control Globant Enterprise AI has a cache control that is enabled by default for APIs ([[34]] and [[29|RAG Assistants API]]), but disabled when accessing from the Frontend. In the case of APIs, to take advantage of cached content with CacheHit, the query must be identical in terms of assistantname, version, llmsettings, and prompt. Any modification to these parameters will result in cache invalidation. To control the state of the cache in APIs, the HTTP header is used in requests: - To enable caching: X-Saia-Cache-Enabled=true\n- To disable caching: X-Saia-Cache-Enabled=false It is important to note that the cache expires after 7 days. {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "1bed5590-a8e6-46b8-a194-c2a8ae6d8e63",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "0d79eb1e-0f72-4dbd-8bf6-7f7fff5af224",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?20",
          "evaluationResultRowContextName": "20",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Globant Enterprise AI Contact Resources This document describes how to contact the technical support provided by Globant Enterprise AI, outlining the procedure for reporting incidents and the corresponding response times. ## Reporting Incidents to the Support Team You can report incidents to the Globant Enterprise AI Support Team through the following channels: 1. **Email:** You can send your inquiries to enterpriseai(at)genexus.com. When sending an email query, be sure to do so using your **GeneXus Account**.\n2. **Tracking System:** You can directly access the [https://frontend.support.genexus.com/frontend.querystep.aspx|Tracking System]. This method is recommended as it guides you through the process of reporting the inquiry. This ensures that the support team receives all the necessary information to respond as quickly as possible. It also allows you to track your inquiries. For detailed guidance on effectively reporting technical issues and navigating the Tracking System, read [[868]]. The support service is available from Monday to Friday (except holidays) during GMT-3 business hours. ### Tracking Your Inquiries You can easily track the status and progress of your support tickets through the [https://frontend.support.genexus.com/frontend.workwith_support.aspx|Support Portal]. ### Inquiry Priority When reporting an incident, you need to define its priority. This is important to address your request with the appropriate urgency. To determine the priority level, you can follow these guidelines: * **Level 1/High: Production down:** This is the most critical case, where an application in production is down.\n* **Level 2/Medium: Development down:** This level refers to a problem that makes it impossible to continue development work.\n* **Level 3/Low:** This level includes general inquiries, suggestions, or non-critical situations. When submitting an inquiry by email, you should include the priority level in the subject line. For example: \"Level 2: I get an error when running Globant Enterprise AI\". Assigning the most appropriate priority level to each inquiry is essential to maintain the quality standards of the service. ### Ticket Number In all cases, when an inquiry is submitted, it enters the system and a ticket number is assigned. With this number, you can query the status of the ticket at any time. You can also add additional information or comments. If the support staff needs more information about a ticket to be able to solve it, you will receive an email with the request. This email will contain a link to answer the request from the Web application. You can also reply directly to the email sent by the support staff. In both cases, the ticket will be updated in the system. All communication related to a ticket between the client and the support staff is recorded in the system, allowing for full traceability. ### Additional Resources * **FAQs:** Find answers to frequently asked questions about Globant Enterprise AI at [https://www.genexus.com/en/products/globant-enterprise-ai/faqs|Globant Enterprise AI FAQs].\n{{{12}}}{{{17}}}\n##MARKDOWN##",
          "evaluationResultRowContextDocId": "8a649f4a-185e-4213-a146-955f6c87a0f2",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "2e480014-ea5d-458d-a521-1a1a7b8dbcdf",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?274",
          "evaluationResultRowContextName": "274",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Help Pages belonging to this category are part of the official documentation of Globant Enterprise AI. --- **Pages** - [.custom File Format](201.md)\n- [.web File Format](202.md)\n- [AI-Driven Load Balancing](32.md)\n- [API Assistant](110.md)\n- [Assistant API](35.md)\n- [Azure AISearch Search Options](207.md)\n- [Azure AISearch Vector Store Provider](271.md)\n- [Azure AISearch Vector Store Provider Troubleshooting](149.md)\n- [Chat API](34.md)\n- [Chat with Data Assistant](159.md)\n- [Chat with Data Assistant Use Guidelines](177.md)\n- [Chat with Documents API](33.md)\n- [Coming Next](30.md)\n- [Configuration - General Information](68.md)\n- [Configuration - Prompt](69.md)\n- [Configuration - Retrieval](70.md)\n- [Context Prompt Template](205.md)\n- [Conversational Module](291.md)\n- [Data Analyst Assistant](89.md)\n- [Embeddings API](422.md)\n- [Embeddings Troubleshooting](140.md)\n- [Error Codes](23.md)\n- [Feedback API](407.md)\n- [File API](450.md)\n- [File support for multimodal models](345.md)\n- [File Support in Assistants](346.md)\n- [File Upload Troubleshooting](199.md)\n- [Filter Operators](179.md)\n- [Flows Configuration](384.md)\n- [Flows Conversation History](378.md)\n- [Flows in Globant Enterprise AI](321.md)\n- [Flows Section](389.md)\n- [GeneXus Enterprise AI Proxy](19.md)\n- [Globant Enterprise AI API Reference](20.md)\n- [Globant Enterprise AI Backoffice](42.md)\n- [Globant Enterprise AI Frontend](82.md)\n- [Globant Enterprise AI Overview](15.md)\n- [Graph Retrieval](203.md)\n- [History Prompt Usage](297.md)\n- [How to change the output format and filter results in Chat with Data Assistant](181.md)\n- [How to create a Flow](322.md)\n- [How to create a Flow with Conditional component](363.md)\n- [How to create and configure a RAG Assistant](60.md)\n- [How to expose a Flow through Teams](391.md)\n- [How to set the language in Data Analyst Assistant](464.md)\n- [How to upload and manage Files via API](454.md)\n- [How to use API Assistant](123.md)\n- [How to use Chat with Data Assistant](170.md)\n- [How to use Data Analyst Assistant](105.md)\n- [Ingestion SDK](172.md)\n- [LLM Troubleshooting](290.md)\n- [Managing quotas per project](71.md)\n- [Managing Roles and Permissions in Globant Enterprise AI](242.md)\n- [Managing Roles and Permissions in Globant Enterprise AI: Members](253.md)\n- [Managing Roles and Permissions in Globant Enterprise AI: Roles](245.md)\n- [Message Metrics in Flows](382.md)\n- [Migration to the new Roles and Permissions Management System](264.md)\n- [MongoDB Atlas Vector Search](438.md)\n- [On-premises installations](441.md)\n- [Organization API](22.md)\n- [Parent Document Retrieval Strategy](204.md)\n- [pgvector postgreSQL extension](439.md)\n- [Pinecone Vector Store Provider](270.md)\n- [Pinecone Vector Store Provider Troubleshooting](307.md)\n- [Playground Interface](349.md)\n- [RAG Advanced configuration](209.md)\n- [RAG Assistants API](29.md)\n- [RAG Assistants configuration](64.md)\n- [RAG Assistants Introduction](44.md)\n- [RAG Assistants Use Guidelines](61.md)\n- [RAG Custom File Format sample](138.md)\n- [RAG File Formats](137.md)\n- [RAG Index Configuration](145.md)\n- [RAG Profile Metadata configuration](298.md)\n- [Redis Search Vector Store Provider](437.md)\n- [Self Query Use Case](150.md)\n- [Start Page](408.md)\n- [Supported Chat Models (v2)](200.md)\n- [Supported Embeddings Models](447.md)\n- [Supported LLMs](153.md)\n- [Supported models and file formats](406.md)\n- [Supported Vector Store Provider](267.md)\n- [Tokens](293.md)\n- [Troubleshooting Flows with RAG when Action is Not Taken if Responses are Not Found](445.md)\n- [Updates Log](31.md)\n- [User Metrics in Flows](380.md)\n- [User's Guide for Globant Enterprise AI Flow Builder](371.md)\n- [Using the Request Summary Table for Improved Dashboard Performance](273.md)\n- [Using variables in Prompts](85.md)\n- [Vector Store Provider Troubleshooting](308.md)\n- [VectorStore Search Options](206.md)",
          "evaluationResultRowContextDocId": "ac17ebe8-86f9-4bd8-bcc2-f75f1d342865",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "3adbeb5f-ec06-4e34-a7a8-7664e014fe0b",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?17",
          "evaluationResultRowContextName": "17",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Globant Enterprise AI Overview [Globant Enterprise AI](https://www.genexus.com/en/products/genexus-enterprise-ai) is a business platform designed to facilitate the implementation of AI assistants tailored to your specific needs and areas of expertise. You can create AI Assistants that can integrate and interact with your current operations, processes, systems, and documents, creating new paths of innovation and productivity to explore. One of the great benefits of using Globant Enterprise AI is the ability to select a Large Language Model (LLM) and later switch to another without changing your definitions. Thus, Globant Enterprise AI acts as a secure bridge, connecting enterprise applications to LLMs while providing a variety of tools and features to boost productivity and innovation. Since Globant Enterprise AI is an intermediate layer, anyone who works with it is protected because the data will not be made public or used by the LLMs. To facilitate the integration of LLMs into your workflow, Globant Enterprise AI offers features that enable you to: - Monitor access to LLMs through customized authentication and authorization protocols.\n- Provide a Web interface with a look and feel that is familiar to LLM users, branded and managed by your organization.\n- Supervise the costs and interactions associated with each LLM for [streamlined analysis and control](32.html).\n- [Manage quotas](71.html) per solution to keep your spending in check. In addition, for those looking to integrate AI capabilities into their custom corporate software, Globant Enterprise AI provides the ability to: - Automatically generate [APIs](20.html) and version the assistants and [prompts](85.html) you create using the Chat Assistant.\n- Optimize your AI exploration by effortlessly switching between different LLMs.\n- Reduce dependency between the AI applications you develop and their underlying LLMs.\n- Leverage pre-built AI functions like 'Chat with your Documents' and 'Chat with your Data' to accelerate your AI adoption journey. These AI applications, designed to be integrated into production environments, require a set of essential non-functional and functional characteristics for business applications. ### Non-functional Characteristics - Observability\n- Maintainability, Future Proofing\n- Security\n- Scalability\n- Accountability ### Functional Characteristics - Retrieval Augmented Generation (RAG) for private documentation\n- Intent-based navigation for existing user interfaces\n- Assistants for power autocomplete\n- Definition and chaining of assistants and actuators\n- Autonomous or semi-autonomous execution agents From a process perspective, it is also crucial to consider development elements such as versioning and testing assistants, and deploying them. Globant Enterprise AI was designed using an architecture of multiple logical layers that can be accessed independently and incrementally. This means you can start by just consuming a particular model, then conceptualize those accesses as assistants, and finally as a use case. ![Globant Enterprise AI Architecture](./images/18.png) ### What problems does Globant Enterprise AI solve with a unique time to market? - Centralized data and cost observability\n- Development of POCs (Proof Of Concept) or products that quickly \u201ccognify\u201d business, reducing costs or time for a specific business area\n- Time to market for AI solutions in a safe and scalable manner\n- Canonical use cases for AI solutions: - Intent-based navigation using [heterogeneous APIs](110.html) and [relational databases](159.html) - [RAG](44.html) - [Data Analyst](886.html) - Autocomplete - Business assistant-based processing conceptualization ### Observability It is essential to understand the information lifecycle and be able to measure various business indicators regarding the use and costs of the AI models being utilized. Business applications need to understand which business domains are using which AI models; check the [backoffice section](42.html). ### Maintainability, Future Proofing AI models are evolving rapidly. Can businesses keep up? Can your interfaces fluctuate and test each model without breaking connection interfaces? How can you become independent of these decisions, often tied to cost, privacy, or other factors? Globant Enterprise AI allows for the independence of created interfaces from the accessed models, enabling agents to evolve with a business perspective, separate from the underlying AI models' evolution. ### Security Businesses require access security, data security, data governance, and alerts for potential rule breaches. Globant Enterprise AI provides everything needed to achieve control over what happens with data within the company, ensuring that data travels securely and adheres to business-imposed rules. ### Scalability Performance should not be degraded by business success. Having infrastructure designed for secure and scalable communication with models is essential when deploying products in production. Using Globant Enterprise AI ensures that if the business succeeds, scalability concerns are addressed. ### Built-in Patterns Solved Globant Enterprise AI provides all the services and abstractions necessary for implementing emerging UX patterns in an [integrated frontend](82.html). It offers the following services: - Chat with any document set, essentially building a private ChatGPT for unstructured company documents (RAG Assistant)\n- Assistant creation and chaining for various business objectives\n- Autocomplete assistants\n- Summarization assistants\n\n- Private instance to chat with LLMs ### Testing your assistants Globant Enterprise AI allows you to version all your assistants; this is a great feature that allows you to evolve assistants without breaking things. You need to be aware that programming with non-deterministic agents could be a challenge without help. With Globant Enterprise AI, you can declare different versions of your assistants and start enabling different access to them depending on the consumer. You can create a complete regression test suite with the new version of a new assistant before entering production. Actually, you can match your current staging planning in order to evolve your solution with confidence. ### Access to Globant Enterprise AI Access to the platform's various layers is generally programmable using any programming language. Access is granted via access tokens per project or organization. In addition to programmatic interfaces, you can quickly create Playgrounds to demonstrate AI technology capabilities. If you are already using an integration with OpenAI API with minimal changes, simply update the base URL and Authorization header in your existing OpenAI SDK to set up the route request through [Chat API](34.html). Check the [Ingestion SDK](172.html) if you need to connect to different data-sources. ### See Also [Updates Log](31.html) [Coming Next](30.html) ### Videos ![Globant Enterprise AI Course](./images/197.png) [Globant Enterprise AI Course](https://training.genexus.com/en/learning/courses/artificial-intelligence/globant-enterprise-ai/introduction-to-the-course)",
          "evaluationResultRowContextDocId": "b6f9138e-f5c7-4b62-8756-6a0795c0bf3f",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "5bd0b6d7-e4cf-42b1-ba82-f185855f7678",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?15",
          "evaluationResultRowContextName": "15",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Globant Enterprise AI Lab An AI Agent is a system designed to execute specific tasks with some degree of autonomy. Unlike Large Language Models (LLMs), which primarily respond to specific instructions, AI Agents are designed to perceive their environment, process information, make autonomous decisions and take proactive actions to achieve specific goals. In addition, they are able to dynamically adapt to unforeseen changes or challenges, continuously learning from their experiences. The autonomy of AI Agents is based on three fundamental characteristics: * **Proactive Planning and Execution:** AI Agents do not simply react to isolated instructions; they set clear objectives and plan how to achieve them, breaking down complex tasks into manageable steps, anticipating obstacles and adjusting their approach in real time. * **Advanced Reasoning and Continuous Learning:** They use sophisticated reasoning strategies, such as \u201cChain of Thought\u201d, to analyze situations, infer conclusions and decide on the most effective actions. They incorporate different types of memory (declarative, procedural and working) that allow them to learn from previous experiences, generalize knowledge, and continuously improve their performance. * **Integration with Tools and Data Sources:** AI Agents integrate effectively with external tools, APIs and various data sources, enabling them to access and process information from multiple systems and environments, thus extending their range of practical application. ## AI Agents in Globant Enterprise AI Recognizing the transformative potential of AI Agents, [[15|Globant Enterprise AI]] introduces **Globant Enterprise AI Lab**: a platform specialized in simplifying the creation, configuration and management of these intelligent systems. **Globant Enterprise AI Lab** not only facilitates the creation of Agents, but also drives new, future-oriented ways of working. **Globant Enterprise AI Lab** allows you to define the type of Agent required through a simple and conversational interaction. Through this platform, it is possible to configure in detail: * The specific tasks to be solved by the Agent.\n* The most appropriate reasoning model for each task.\n* The thinking strategies to be used by the Agent. Each configured Agent is clearly represented in JSON format, thus facilitating its evaluation and direct testing on the platform. Once created and validated, **Globant Enterprise AI Lab** also makes it possible to publish the Agent for widespread use, integrating it into automated business processes and workflows that require human interaction. This allows to effectively combine automated tasks performed by autonomous Agents with those activities executed by people, creating agile, collaborative and future-oriented work dynamics. ##Availability Since April 2025 release. {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "466fe5e4-c067-4107-9e9a-56883dea6cd8",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "88db7684-83ab-4e14-b5cc-7c6ce8cb2b9b",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?972",
          "evaluationResultRowContextName": "972",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.5,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "e082685d-e58d-4d3b-8bfc-a04a8bf70d86",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0025191600",
      "evaluationResultRowEndDate": "2025-05-13T11:16:27",
      "evaluationResultRowOutput": "Globant Enterprise AI dispone de varias APIs. Seg\u00fan la documentaci\u00f3n, las APIs disponibles incluyen, entre otras:\n\n- APIs para integraci\u00f3n con sistemas de Large Language Models (LLM).\n- APIs para modificar metadatos de la plataforma.\n- APIs para la interacci\u00f3n con asistentes definidos.\n\nLos identificadores de estas APIs mencionados en la documentaci\u00f3n son:  \n`[[34]]`, `[[1180]]`, `[[663]]`, `[[22]]`, `[[29]]`, `[[625]]`, `[[422]]`, `[[1165]]`, `[[35]]`, `[[450]]`, `[[1183]]`, `[[802]]`, `[[407]]`, `[[893]]`.\n\nPara detalles m\u00e1s espec\u00edficos sobre cada API, su funcionalidad y uso, puedes consultar la documentaci\u00f3n completa en la referencia de APIs de Globant Enterprise AI.\n\nReferencia: [1](https://wiki.genexus.com/enterprise-ai/wiki?20)",
      "evaluationResultRowStartDate": "2025-05-13T11:16:20",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "5867"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "is defined; for example, \"lastUserInput\". This variable contains the user's most recent input and will be used as the basis for the query on the selected documents or tags. Similarly, in the Output section, there is also a Variable field that specifies the variable in which the output generated by that node will be stored. In this case, we are interested in saving the RAG Assistant\u2019s response in a new variable called lastResponse. Therefore, the Use as response option is disabled. Clicking on the pencil icon to the right of the node opens the \"State Configuration\" menu where you can modify the following items: - **Acceptable confidence level**: The default value is 0.2, which indicates the level of confidence in the match of the chunks that the RAG obtains after searching the vector database. Only responses that meet or exceed this confidence level will be considered valid.\n\n- **Include conversation history**: Enabled by default. It ensures that the conversation history is included in the context of the interaction, allowing the assistant to take previous messages into account when generating responses.\n- **Include context variables**: Enabled by default. It allows context variables stored during the conversation to be sent to the assistants so that they can be used within the prompts.\n- **Show sources**: Enabled by default. It allows the assistant to show the sources of the information used to generate the response, which helps to provide transparency and to validate the accuracy of the response.\n- **Handle empty response**: Disabled by default. It handles situations where the assistant is unable to generate a valid response. When enabled, it opens a new flow branch (as shown in the image), allowing you to configure specific actions to follow in case no response is available. In this new branch, you can add any of the components available in the Interactions menu on the left to continue the interaction as needed. ![Handle Empty Response](./images/337.png) In this case, if the RAG Assistant finds the answer in the documents, you want the flow to call an assistant to translate the response to Japanese and store it in the same input variable (lastResponse). If no answer is found, the flow goes to a second RAG Assistant configured with documents used in Globant Enterprise AI courses. The variable that goes into the RAG Assistant is lastUserInput, which contains the user's last input. The response generated by this RAG Assistant is stored in the lastResponse variable. ![RAG Assistant Configuration](./images/338.png) ### Go To Node Finally, the Go To node is used to connect different flows within the interaction. This node allows you to select the flow to which you want to redirect the end user based on their current interaction. ![Go To Node](./images/339.png) Clicking on the blank bar (corresponding to the drop-down menu) displays a list of available flows: ![Available Flows](./images/340.png) Here you can select and define the flow to which you want to redirect the user. To review or modify any of these flows, you can access the Flows menu on the left side of the screen. In addition, \"Pick from canvas\" allows you to select a node directly from the canvas, facilitating navigation and connection between different parts of the Flow. The 'Go' icon next to the node allows you to navigate directly to the node to which it is connected. In this case, there are two 'Go To' nodes: - The first 'Go To': Returns to the 'User Input' node. Since the previous node (which translates to Japanese) does not store the response in a new variable and simply redirects to the 'User Input', the response received by the end user is the answer to their question translated to Japanese.\n- The second 'Go To': Sends the response to the 'Assistant' node, which is responsible for translating the generated response to Japanese using the lastResponse variable. This variable contains the response obtained from either of the two RAG Assistants previously executed in the flow. In this way, the flow ensures that the end user receives the response in Japanese, regardless of whether the response was generated by the first RAG Assistant or the second. ![Flow Diagram](./images/341.png) ## Step 3: Test flow Once you have configured the flow, make sure to click on the 'Save' button located at the top left of the screen. To test the flow you have created, you have three options available from the same design window: ![Test Flow Options](./images/342.png) 1. Click on the arrow icon (similar to the play button) to run the flow ('Run Flow').\n2. Click on the message icon to open the testing assistant (\u201cOpen Testing Assistant\u201d). ![Testing Assistant](./images/343.png)\n3. Click on the icon to open in a new window (the box with an arrow) to access the demo page (\u201cGo to Demo Page\u201d). ![Demo Page](./images/344.png) The first two options will open a window on the right side of the screen, while the third option will open a new window in the browser.\n\n# How to create a Flow This is a step-by-step guide to create and test an Assistants [Flow](321.html). ## Step 1: Create New First, log in to the [Globant Enterprise AI Backoffice](42.html). In the Project Dynamic combo box, select the project you want to work with (in this case, Default is used). Next, on the left side of the screen, you will find the backoffice menu. In this menu, click on Flows. ![Flow Menu](./images/324.png) By clicking on Flows in the Globant Enterprise AI Backoffice, a new window opens in the browser with the Flow Builder, where the Flows associated with the selected Globant Enterprise AI project are created and managed. When accessing Globant Enterprise AI\u2019s Flow Builder for the first time, a welcome screen like the one shown in the image below is displayed. From this screen, you can start creating a new flow by clicking on the \"Create\" button. ![Welcome Screen](./images/325.png) When you click on \"Create\", a pop-up window opens in which you must fill in the following information: 1. **Name**: Descriptive name for the flow, allowing it to be easily identified.\n2. **Description**: This field is optional, but it is recommended to add a brief description of the flow to clarify its purpose or content.\n3. **Type**: Allows you to select the type of flow to create, with the options \"Chat Flow\" or \"Assistant Flow\". Selecting one of these types will determine specific configurations and characteristics of the flow. In this case, Assistant Flow is selected.\n\n4. **Select languages**: Sets the language in which the flow will be configured, and allows you to define the language of the hard-coded messages. Multiple languages can be selected, so the same message can be available in different languages. Once you have completed these fields, you can click on the \"Create\" button. ![Create Flow](./images/326.png) ## Step 2: Manage the Start flow Once you have clicked on \"Create,\" you will return to the welcome screen. There, click on the \"Manage\" button of the newly created flow to add the assistants you want and customize their behavior. ![Manage Flow](./images/327.png) Clicking on the \"Manage\" button displays the first flow, which is created automatically. This initial flow, called \"start,\" establishes a basic configuration that you can customize, adding or modifying interactions, messages and assistants according to your needs. ![Start Flow](./images/328.png) ### Start flow The initial flow, called \"start,\" is automatically generated with the ID input.welcome. ![Start Flow ID](./images/329.png) ### Start (Intent: input.welcome) Node The Start (Intent: input.welcome) node marks the beginning of the conversation flow. This node is activated every time the flow is triggered, and its main purpose is to start the flow and, from there, the other nodes that manage the interaction with the user are connected. To rename the start node, go to the left menu, find Flows > start, click on the three dots next to it, select \"Rename Flow\" and, after typing the new name, press Enter to save the change. ![Rename Flow](./images/330.png) ### Message Node The next node, Message, is in charge of sending the welcome message or any other initial message configured. In this case, the message is: \"Hello {name}, how can I help you?\". Here, {name} represents a dynamic variable that is filled with the user's name. The message displayed in this node is customizable, and the text can be formatted by selecting different styles, such as Normal, Heading 1, among others, from the drop-down menu that appears where it says \"Normal\". This allows you to adjust the text style according to your needs. ![Message Node](./images/331.png) On the top bar of the node, you can see several formatting buttons such as bold, italic, underline, among others, to customize the text. In addition, there are icons that allow you to add lists, links and emojis. The last button, which has the shape of a small drawer, allows you to insert dynamic variables in the message. Clicking on this button displays a list of the variables available in the system, and it is also possible to define new custom variables. This Message node can be deleted by clicking on the trash can icon to the right of the node. It is also possible to add a variant by clicking on the \"plus\" (+) button at the top right. ### User Input Node The flow then proceeds to capture the user input through the User Input node, storing it in the lastUserInput variable for later use. ### Assistant Node In the Assistant node, you must configure the assistant to perform specific actions. This node uses the information stored in the lastUserInput variable, which contains the text entered by the end user in the User Input node. ![Assistant Node](./images/332.png) To configure the assistant, click on the \"Select an assistant\" field. By default, \"No assistant selected\" will be displayed. From the drop-down menu, choose the assistant you wish to use. **Note**: The assistants available are those that have been defined in the associated project, which was configured in the first step. The assistants can be any of those defined in Globant Enterprise AI, including [Data Analyst Assistant](886.html), [API Assistant](110.html), [Chat with Data Assistant](159.html). Once selected, the Prompt field will display the message that defines the task of the assistant. This message is read-only and can\u2019t be edited. The assistant's response can be stored in the lastUserInput variable or in a new variable, depending on your needs. By default, the Use as response option is enabled, which causes the assistant's response to be used directly in the lastUserInput variable. If you prefer to store the response in a new variable, disable Use as response. This will enable the Output field, where you can select or create a new variable. ![Assistant Output](./images/333.png) In addition, you can enable the option to read the assistant's response in JSON format. By enabling this option, you will have the ability to map the fields of that JSON to variables previously defined in your configuration. This allows for more precise integration of the assistant's responses in more complex workflows or in systems that require a specific format for data handling. The pencil icon to the right of the node corresponds to the Edit button. By clicking on it, you can enable or disable the Include conversation history and Include context variables options. By default, Include conversation history is enabled. It allows the assistant to access the conversation history to generate more coherent and contextual responses. The Include context variables option is also enabled by default. This option sends to the assistant variables defined in the flow, such as previously captured data or configured values, so that they can be used within prompts. This node can be deleted by clicking on the trash can icon to the right of the node. In this case, the Assistant node is deleted: ![Delete Assistant Node](./images/334.png) ### Insert RAG Assistant or Assistant Node On the left menu, below the Interactions category, you will find the different components that can be added to the flow. In particular, in the GENERATIVE section, you can choose between adding an Assistant or a RAG Assistant. ![Insert RAG Assistant](./images/335.png) In this case, you need to add a RAG Assistant. To do so, click on the RAG Assistant option in the GENERATIVE section. Next, drag the component to the desired location in the flow. In this case, you can insert it between the User input and Go To nodes. When you add a new RAG node, you must click on the \"Select an assistant\" field. By default, the option \"No assistant selected\" will appear. From the drop-down menu, choose the RAG assistant you wish to use. Once you select the RAG assistant, the next field, \"Select a document\", allows you to choose a specific document that will act as the source for responding to the end user's request. If you select a document, the assistant will only use that document as a reference. ![Select Document](./images/336.png) Alternatively, you can use the \"Indicates the documents' tags\" field. Here you can add the tags defined in [Step 2: Upload RAG documents](60.html), which are used to filter the documents. This allows the assistant to search only in the documents that match the indicated tags to respond to the end user's request. In the Input section, a field called Variable is displayed. There, the variable to be used as input for that node",
          "evaluationResultRowContextDocId": "09ebe65a-fb9d-4f45-8146-084558fef786",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "0ff1758e-4680-4166-942f-d832fe1117ca",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?322",
          "evaluationResultRowContextName": "322",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "``` ## Step 3: Creating the Flow Follow the steps described in [[322]] to create the Flow called Mall Chat Assistant with the following structure: [[364]] To add variables to the Flow, click on the Variable option in the LOGICAL section of the [[371##Flows menu|Flows Menu]]. Next, drag the component below the User input node into the flow. In this case, a variable has been added: today. **Today variable:** It is defined by clicking on the down arrow inside the variables node and selecting the option + Add new variable. Next, enter the name today and press Enter. Deactivate the Clear option. To set the value of this variable, click on the edit icon to the right of the node. In the left menu, click on the Open in new window icon, and in the window that opens (Value) add `$today`. ## Step 4: Implementing the Derivation Logic Every end user query must be evaluated to determine which assistant is the most appropriate to answer. This is implemented with IF conditions in the flow, where the end user's intentions are compared with the available assistants. To add the Conditional component, go to the Interactions section and, below LOGICAL in the Flow menu, drag the component and drop it between the Assistant and Go to nodes. [[365]] Dragging the Conditional node generates a branch with two paths: one labeled If and one labeled Else. This structure allows you to evaluate a condition and direct the flow according to the result. To add an additional condition, click on the Add condition option to the right of the first node of the branch: [[366]] After clicking on Add condition in the first node of the branch, an additional node of Expression type is added, just like the first one. These Expression nodes allow evaluating different conditional expressions. [[367]] To add the variable to be evaluated, click on the word Variable, located at the bottom of each node. In the first two nodes of Expression type, you can see how each condition evaluates the `orchestrationResponse` variable. Depending on the value of this variable, the flow is directed to different assistants: either to the Movie Ticket Assistant or to the Mall Shops Catalog. If neither of these conditions is met, the flow continues through the Else branch, where by default there is a Go to node. A Message node has been added before the Go to node so that the flow can display a specific message to the end user. Then, the corresponding nodes are added to handle the responses according to the evaluated condition. [[368]] A node of API Assistant type that is configured for the Movie Ticket Assistant and handles queries related to the movie theater in the shopping mall is placed below the first node of Expression type (which checks if `orchestrationResponse` is equal to \"Movie Ticket Assistant\"). A node of RAG Assistant type that is configured for the Mall Shop Catalog Assistant and provides information about the stores in the mall is added below the second node of Expression type (which checks if `orchestrationResponse` is equal to \"Mall Shops Catalog\"). Finally, below each node a Go to is added, which redirects back to the User Input node. Once the assistant has processed and answered the query, this allows the flow to return to the entry point to receive new user interactions, ensuring a continuous and fluid conversation. [[369]] ## Step 5: Testing and Adjusting the Mall Chat Assistant Once you have configured the flow, make sure to click on the 'Save' button located at the top left of the screen. This step is essential to save all changes made to the flow. To test the flow you have created, you have three options available from the same design window: 1. Click on the arrow icon (similar to the play button) to run the flow ('Run Flow').\n2. Click on the message icon to open the testing assistant ('Open Testing Assistant').\n3. Click on the open in new window icon (the box with an arrow) to access the demo page ('Go to Demo Page'). The first two options will open a window on the right side of the screen, while the third option will open a new window in the browser, allowing you to see how the flow runs in a demo environment. During testing, you can run various queries to make sure that the flow responds correctly. For example, you can ask \"Where can I purchase books?\" and the flow should correctly identify the query and direct it to the Mall Shop Catalog, where the end user will be informed about the availability of stores that sell books. Then, you can ask \"What movies can I watch?\" and verify that the flow correctly redirects to the Movie Ticket Assistant to get the list of available movies. If the answer is ambiguous, as in \"I would like to watch the last one\", the orchestrator should use the context of the conversation to identify the last movie mentioned and continue the interaction with the Movie Ticket Assistant. In situations where the query does not correspond to any assistant\u2014such as \"What is Linux?\"\u2014the orchestrator should handle the response using a fallback and provide the information directly from the model, so that the flow covers all possible end user interactions. These tests will allow you to adjust the flow as needed, ensuring that the Mall Chat Assistant runs efficiently and delivers a smooth and accurate experience for end users. {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "57a2c916-1a55-4579-abb9-419226e4e2c1",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "7f90e172-340e-4368-b341-dc99ea769760",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?363",
          "evaluationResultRowContextName": "363",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#How to create a Flow with internet search This is a step-by-step guide to create a conversational assistant with internet search using [[321]]. This Flow manages two assistants: a Chat Assistant that answers questions using internet search results, and another Chat Assistant that rephrases follow-up questions (based on the conversation history) as standalone questions. In addition, it includes a Search API component. ## Step 1: Configuring the Work Environment To get started, first, make sure you have access to the [[42]] and are familiar with the [[371|interface for creating Flows]]. ## Step 2: Creating Chat Assistants In this step, two [[566##Chat Assistant creation|Chat Assistants]] are created. They are called 'Response from Search' and 'Follow-up Rephrase'. ### 'Response from Search' Chat Assistant This Chat Assistant generates answers for the end user based solely on the results of an internet search, without using prior knowledge or inventing information. It takes the provided search results and uses them to answer the end user's question in the best possible way. If necessary, the answer can include rich text in HTML format, but Markdown should not be used to format the answer. **LLM Settings:** - AI Provider: Google VertexAI\n- Model: gemini-1.5-pro\n- Temperature: 0.10\n- Max Output Tokens: 8192 **Prompt:** ```\nINSTRUCTIONS\n---\nYour task is to generate a response for the user based on the results of an internet search.\nDO NOT USE PRIOR KNOWLEDGE AND DO NOT MAKE UP INFORMATION USE THE SEARCH RESULTS.\nGiven the following results from an internet search and NOT PRIOR KNOWLEDGE answer the user question the best you can. SEARCH RESULTS\n---\n{searchResults} RESPONSE FORMAT\n---\nFor the response you can generate RICH TEXT USING HTML if it is needed.\nDO NOT USE MARKDOWN TO FORMAT YOUR RESPONSE.\nFOR RICH TEXT DO NOT USE MARKDOWN USE HTML INSTEAD.\n``` ### 'Follow-up Rephrase' Chat Assistant This Chat Assistant rephrases a follow-up question based on the conversation history, ensuring it becomes a standalone question if needed. If the follow-up question already works as a standalone question, it simply repeats the question without any changes. **LLM Settings:** - AI Provider: Google VertexAI\n- Model: gemini-1.5-pro\n- Temperature: 0.10\n- Max Output Tokens: 8192 **Prompt:** ```\nGiven the conversation history and not prior knowledge, and a follow-up question, rephrase the follow-up question to be a standalone question IF IT IS NEEDED. If the question works as a standalone question, just repeat the question. RESPONSE ONLY WITH THE STANDALONE QUESTION AND NOTHING MORE; DO NOT INCLUDE ANY OTHER TEXT.\n``` ## Step 3: Creating the Flow Follow the steps described in [[322]] to create the Flow called 'Search Assistant' with the following structure: [[689]] ## Step 4: Implementing the API component To add the API component, go to the [[494|Interactions section]] and, below INTEGRATIONS in the Flow menu, drag the component between the Assistants nodes. [[690]] ## Step 5: Setting Up the Search API Go to [https://developers.google.com/custom-search/v1/introduction?hl=es-419|Google Search API documentation] and click on Get API key button. [[691]] In the pop-up window that opens, select or create a project from the Select or create project menu. [[692]] Select the Yes option to agree that the use of the services and related APIs is subject to compliance with the applicable Terms of Service. Then click on NEXT to continue with the setup process. In the next window, click on the SHOW KEY button to reveal the API Key. Copy this API key, as it will be required to configure the API node in your flow. [[693]] ## Step 6: Creating a Programmable Search Engine Go to [https://programmablesearchengine.google.com/controlpanel/all|Programmable Search Engine] and click on the Add button. [[694]] In the pop-up window that opens, fill in the necessary fields to create the search engine: [[695]] - **Name of your search engine:** In the \"Search engine name\" field, enter a name for the search engine, such as Search Assistant.\n- **What to search for:** Select the \"Search the entire web\" option to have the engine search the entire web. If you prefer to search only specific sites, you can select the \"Search specific sites or pages\" option and add the necessary URLs.\n- **Search settings:** If you wish, you can adjust additional settings such as image search or SafeSearch filter.\n- **Captcha:** Select the \"I am not a robot\" checkbox.\n- **Create:** At the end of the window, click on the Create button to finalize the creation of the search engine. In the window that appears, you must click on the Customize button. [[696]] This will take you to the Overview section of the search engine you have just created. In this section, you should find and copy the Search engine ID, which is a unique value associated with your search engine. This ID is required to configure the API node in your flow. [[697]] ## Step 7: Configuring the API Node With the Search engine ID and API key, you can now configure the API node. First, you must click on the Edit button that appears when you position the mouse over the API node. [[698]] Once you have clicked on Edit, the State Configuration menu will be displayed on the left side. [[699]] Within this menu, in the URL section, you must configure the URL with the following value: ```\nhttps://www.googleapis.com/customsearch/v1?key=<API key>&cx=<Search engine ID>&q=<query>\n``` Be sure to replace `<API key>` and `<Search engine ID>` with the previously obtained values. In the API node, go to the **Parameters** section and click on **+ Add new** to add a new parameter. Then, configure the parameter with the following details: - **Parameter name:** q.\n- **Value:** lastResponse (the value obtained from the previous response). Next, in the State Configuration section, you must make sure to mark q as **Query params**. [[700]] In the **Responses** section, make sure to map the items property to the **searchResults** variable. If searchResults is not available, you can click on Add new variable to create it. [[701]] ## Step 8: Configuring the Assistant Node After you have configured the API node, go to the Assistant node, which is located just below the API node in your flow. In this node, find the Variable section and change the input variable to searchResults. [[702]] ## Step 9: Testing the Flow Finally, save the changes and test the flow. You can use the test window or the Playground to make sure everything works correctly, verifying that the Assistant responds properly using the search results. | [[704]] | [[705]] | {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "526b3969-e9ac-4188-a986-6b0013b11c4d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "899a94a5-0d37-4ef5-8dea-1349bea4c0d6",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?688",
          "evaluationResultRowContextName": "688",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# User's Guide for Globant Enterprise AI Flow Builder Globant Enterprise AI Flow Builder provides a visual tool that allows building and configuring conversational flows. ![Image](./images/372.png) ## Top Toolbar The Top Toolbar in the Globant Enterprise AI Flow Builder offers several options that allow you to manage and navigate between flows, as well as access key functionalities when building conversational flows. ![Image](./images/373.png) ### Current Flow Name This drop-down menu displays the name of the flow you are working with. In this case, the current flow is Documentation Assistant. From this menu, you can: - Quickly access other flows created.\n- Return to the main Flows window by clicking on the \"See All\" option. ### Language Change It allows you to change the language of the Flow Builder. In this case, it appears as \"En\" to indicate English. This option is available only if several languages have been selected in the [Select languages](322.html) option when creating the flow. ### Profile It displays the name of the project you are working on and provides the option to log out. ### Go to Demo Page It opens a new window that allows you to test the flow in real time, making it easier to verify the assistant's behavior and responses before implementing it. ### Open Logs Console It opens the Open Logs Console option, where you can access real-time logs of the assistant execution. This is especially useful for debugging tasks and detailed analysis of flow behavior. ### Open Testing Assistant It opens an interactive chat to test the flow in real time, helping to verify how the assistant responds under different scenarios before final implementation. ## Side Navigation Menu On the left side of the window is the Side Navigation Menu. This menu is a collapsible section that is hidden by clicking on the 'Collapse' option at the bottom. It provides quick access to various options: ![Image](./images/374.png) ### OVERVIEW Provides key information about the flow you are managing. Here you can see: ![Image](./images/375.png) - The name of the flow. In this case, \u201cDocumentation Assistant\u201d.\n- The Bot id and Project id, which are unique identifiers of the related bot and project, useful for internal reference and management.\n- The flow description, which is the text entered at the time of creation to detail its purpose or function. In addition, the section includes options to edit the flow name and add members. ### BUILD This section groups the tools that allow you to build and manage the flows and variables used in the assistants: - Flows: From here you can view, create, edit or manage conversational flows.\n- Variables: Allows you to view and manage the global and local variables that will be used in the flows. ### LOGS/DIAGNOSTICS This section is dedicated to the analysis and diagnosis of the assistant's behavior, providing access to interaction logs: - [Conversations history](378.html): Displays the history of all conversations between users and the assistant. It is useful to review how the interactions went and to make adjustments if necessary. ### ANALYTICS This section provides tools to analyze the performance and effectiveness of the assistant using different metrics: - [User Metrics](380.html): Presents metrics related to users, such as number of interactions and frequency of use.\n- [Message Metrics](382.html): Displays metrics related to the messages exchanged, providing insight into the effectiveness of the assistant's responses. ### CONFIGURATION This section allows you to adjust the general settings of the assistant and its environment: - [Configuration](384.html): Accesses the global system settings, where you can find the options Application keys to manage authentication keys, Integrations to configure connections with other platforms, Bot versioning to manage bot versions, and Danger Zone to export, import or restore bot configurations. ## Flows menu This menu has two main sections: Flows and Interactions. ![Image](./images/376.png) 1. [Flows Section](389.html): This section lists all available flows and appears by default when selecting a flow from the welcome screen. However, it is only displayed when the Flows option is selected in the Side Navigation Menu (the leftmost menu on the screen). If another option is selected in the Side Navigation Menu, this section is hidden or other options relevant to the selection are displayed.\n2. Interactions section: This area provides a list of components that can be used to build the flow. Components include user input, conditions, variables, language changes, history restart, among others. ## Flow Creation Area This section shows the flow under construction, allowing you to design the interaction sequence of the different assistants. ![Image](./images/377.png) In the upper right part of the area, you will find several key tools: - Save: This button saves the progress made when building the flow.\n- Run Flow: It allows running the flow to test how it would work in a test environment.\n- Code View (icon {}): This icon allows you to switch to a code view, where you can view and edit the flow in code format. To navigate the flow, you can click and drag the diagram to the creation area. Zooming is controlled with the mouse scroll wheel or pinch gesture on the touchpad, adjusting the zoom level without affecting the vertical position within the flow.",
          "evaluationResultRowContextDocId": "2708af94-91ad-4cc5-8095-f02fe76415ba",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "90deb142-baa3-40c1-b9d8-d72658ee6465",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?371",
          "evaluationResultRowContextName": "371",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Flows in Globant Enterprise AI Flows is a Globant Enterprise AI solution that allows creating conversational solutions tailored to the specific needs of each company by leveraging the capabilities of generative and deterministic artificial intelligence. With Flows, companies can significantly improve interaction with their end users. The solution not only enables the development of chatbots that respond accurately and appropriately, but also facilitates the integration of specialized assistants such as [Data Analyst Assistant](886.html), [API Assistant](110.html), [Chat with Data Assistant](159.html), and [RAG Assistant](44.html). This means that a conversational flow can simultaneously draw on data analysis, integration with external APIs, conversation with documents, and generation of contextual responses. Rather than being limited to a single type of interaction, Flows allows you to seamlessly combine these functions, enriching the conversation and providing more complete and accurate responses to end users.",
          "evaluationResultRowContextDocId": "2b2be084-a211-4463-bc6c-162c331a549e",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "ca10e53e-1f71-4166-bf31-a05819310be2",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?321",
          "evaluationResultRowContextName": "321",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.9,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "e0a19e61-29bc-46eb-9a08-37e76776f3c2",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0031696200",
      "evaluationResultRowEndDate": "2025-05-13T11:16:32",
      "evaluationResultRowOutput": "Flows in Globant Enterprise AI allow integration of different types of assistants to create rich and versatile conversational solutions. The types of assistants that can be integrated include:\n\n- **Data Analyst Assistant**\n- **API Assistant**\n- **Chat with Data Assistant**\n- **RAG Assistant**\n\nThis means that within a single Flow, you can combine data analysis, API integrations, document-based conversations, and contextual AI-generated responses to provide more complete and accurate interactions with end users.\n\nReference: This information can be found in the context about \"Flows in Globant Enterprise AI,\" which describes the solution and the assistants supported in flows. You can read more here: [Flows in Globant Enterprise AI](https://wiki.genexus.com/enterprise-ai/wiki?321) [3].",
      "evaluationResultRowStartDate": "2025-05-13T11:16:27",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "7595"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#Deprecated Models Detail of deprecated models. | **Provider** | **Deprecated Model** | **Fallback to** |\n|---|---|---|\n| openai | gpt-3.5 family | gpt-4o-mini |\n| openai | gpt-4 family | gpt-4o |\n| openai | o1-preview (1) | o3 |\n| openai | o1-mini (1) | o4-mini |\n| openai | text-embedding-ada-002-v2 | text-embedding-ada-002 |\n| vertex_ai | gemini-1.0-pro | gemini-2.0-flash |\n| vertex_ai | gemini-1.5-flash | gemini-2.0-flash-lite |\n| vertex_ai | gemini-1.5-flash-002 | gemini-2.0-flash-lite |\n| vertex_ai | gemini-2.0-flash-lite-preview-02-05 | gemini-2.0-flash-lite |\n| vertex_ai | textembedding-gecko | text-embedding-005 |\n| vertex_ai | textembedding-gecko-multilingual | text-multilingual-embedding-002 |\n| vertex_ai | claude-3-sonnet-20240229 | claude-3-5-sonnet-v2-20241022 |\n| anthropic | claude-3-sonnet-20240229 | claude-3-5-sonnet-20241022 |\n| awsbedrock | anthropic.claude-3-sonnet | anthropic.claude-3.5-sonnet-v2 |\n| groq | llama-3.1-70b-versatile | - |\n| azure | gpt-35-turbo-16k | - |\n| azure | gpt-4 (1) | gpt-4o |\n| sambanova | Llama-3.1-Tulu-3-405B | DeepSeek-V3-0324 |\n| sambanova | Meta-Llama-3.1-70B-Instruct | Llama-4-Scout-17B-16E-Instruct |\n| sambanova | Qwen2.5-72B-Instruct | QwQ-32B |\n| sambanova | Qwen2.5-Coder-32B-Instruct | QwQ-32B |\n| sambanova | Llama-3.2-11B-Vision-Instruct | - |\n| sambanova | Llama-3.2-90B-Vision-Instruct | - | </br>\n(1) Deprecation since May 2025 release. {{{12}}} {{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "7f724003-55a7-4013-97eb-2944173111dd",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "16c33611-59c0-4103-aa80-ef600e6fdad3",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?843",
          "evaluationResultRowContextName": "843",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Supported models and file formats This table provides a detailed overview of the multimodal models supported by Globant Enterprise AI and their compatibility with different file formats such as `pdf`<sup>(1)</sup>, `txt`, `png`, `jpg`, `jpeg`, `gif`, `mp3` and `mp4`. The size of the supported files will depend on the context window of each model. <div style=\"background-color: rgb(242,242,242);border-left: 6.0px solid rgb(128,128,128);margin-bottom: 15.0px;padding: 4.0px 12.0px;\"><b>Note</b>:All non-multimodal models (not included in the table) accept text (`txt`) and PDF (`pdf`) files.</div> | **Provider** | **Model** | **Supported formats** | **Context window (tokens)**|\n|---|---|---|---|\n| anthropic | claude-3-5-sonnet-20241022 | txt, pdf, jpeg, jpg, png, gif | 200000 |\n| anthropic | claude-3-haiku-20240307 | txt, pdf, png, gif | 200000 |\n| anthropic | claude-3-opus-20240229 | txt, pdf, png, gif | 200000 |\n| anthropic | claude-3-5-sonnet-20240620 | txt, pdf, png, gif | 200000 |\n| anthropic | claude-3-7-sonnet-latest | txt, pdf, png, gif, jpeg | 200000 |\n| awsbedrock | anthropic.claude-3-opus | txt, pdf | 200000 |\n| awsbedrock | meta.llama3-2-11b | txt, pdf | 128000 |\n| awsbedrock | meta.llama3-2-90b | txt, pdf | 128000 |\n| awsbedrock | anthropic.claude-3-7-sonnet | txt, pdf, png, gif, jpeg | 200000 |\n| awsbedrock | anthropic.claude-3.5-sonnet-v2 | txt, pdf, jpeg, jpg, png, gif | 200000 |\n| awsbedrock | anthropic.claude-3-haiku | txt, pdf, png, gif | 200000 |\n| awsbedrock | anthropic.claude-3.5-sonnet | txt, pdf, png, gif | 200000 |\n| awsbedrock | amazon.nova-lite-v1:0 | txt, pdf, png, gif | 300000 |\n| awsbedrock | amazon.nova-pro-v1:0 | txt, pdf, png, gif | 300000 |\n| azure | gpt-4o | txt, pdf, gif, jpg, png | 128000 |\n| azure | gpt-4o-mini | txt, pdf, gif, jpg, png | 128000 |\n| azure | o1 | txt, pdf, jpg, png, gif | 200000 |\n| gemini | gemini-2.0-flash-thinking-exp-01-21 | txt, pdf, png, mp3 | 1048576 |\n| gemini | gemini-1.5-pro-latest | txt, pdf, jpg | 2097152 |\n| gemini | gemini-exp-1206 | txt, pdf, jpg | 2097152 |\n| gemini | gemini-2.0-pro-exp | txt, pdf, jpg | 2097152 |\n| gemini | gemini-1.5-flash-8b-exp-0827 | txt, pdf, jpg, png, gif, mp3 | 1048576 |\n| gemini | gemini-1.5-flash-latest | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| gemini | gemini-2.0-flash-exp | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| gemini | gemini-2.0-flash-lite-preview | txt, pdf, jpg, png, mp3 | 1048576 |\n| gemini | gemma-3-27b-it | txt, pdf | 131072 |\n| gemini | gemini-2.5-pro-exp-03-25 | txt, pdf, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| groq | meta-llama-4-scout-17b-16e-instruct | txt, pdf, jpg, jpeg, png | 131072 |\n| groq | meta-llama-4-maverick-17b-128e-instruct | txt, pdf, jpg, jpeg, png | 131072 |\n| groq | llama-3.2-11b-vision-preview | txt, pdf | 128000 |\n| groq | llama-3.2-90b-vision-preview | txt, pdf | 128000 |\n| nvidia | meta-llama-4-scout-17b-16e-instruct | txt, pdf, png, jpeg, jpg, gif | 131072 |\n| nvidia | meta-llama-4-maverick-17b-128e-instruct | txt, pdf, png, jpeg, jpg, gif | 32768 |\n| openai | chatgpt-4o-latest | txt, pdf, jpg, png, gif | 128000 |\n| openai | o1 | txt, pdf, jpg, png, gif | 200000 |\n| openai | o1-pro | pdf, jpg, jpeg, png, gif, webp | 200000 |\n| openai | o3 | txt, pdf, jpg, jpeg, png, gif | 200000 |\n| openai | o4-mini | txt, pdf, jpg, jpeg, png, gif | 200000 |\n| openai | gpt-4.1 | txt, pdf, gif, jpg, png, jpeg | 1047576 |\n| openai | gpt-4.1-mini | txt, pdf, gif, jpg, png, jpeg | 1047576 |\n| openai | gpt-4.1-nano | txt, pdf, gif, jpg, png, jpeg | 1047576 |\n| openai | gpt-4.5-preview | txt, pdf, gif, jpg, png, jpeg | 128000 |\n| openai | gpt-4o | txt, pdf, gif, jpg, png | 128000 |\n| openai | gpt-4o-mini | txt, pdf, gif, jpg, png | 128000 |\n| openai | gpt-4o-2024-11-20 | txt, pdf, gif, jpg, png | 128000 |\n| openai | gpt-4o-search-preview | txt, pdf | 128000 |\n| openai | gpt-4o-mini-search-preview | txt, pdf | 128000 |\n| openrouter | [https://openrouter.ai/openrouter/auto|auto] | txt, pdf, gif, jpg, jpeg, png | depends on the selected model |\n| sambanova | Llama-3.2-11B-Vision-Instruct | txt, pdf, png, jpg | 4096 |\n| sambanova | Llama-3.2-90B-Vision-Instruct | txt, pdf | 4096 |\n| vertex_ai | claude-3-7-sonnet-20250219 | txt, pdf, png, gif, jpeg | 200000 |\n| vertex_ai | claude-3-5-sonnet-20240620 | txt, pdf, pdf, png, gif | 200000 |\n| vertex_ai | claude-3-haiku-20240307 | txt, pdf, png, gif | 200000 |\n| vertex_ai | claude-3-opus-20240229 | txt, pdf, png, gif | 200000 |\n| vertex_ai | meta.llama-3.2-90b-vision-instruct-maas | txt, pdf | 128000 |\n| vertex_ai | meta-llama-4-scout-17b-16e-instruct-maas | txt, pdf, jpeg, jpg, png, gif | 10000000 |\n| vertex_ai | meta-llama-4-maverick-17b-128e-instruct-maas | txt, pdf, jpeg, jpg, png, gif | 1000000 |\n| vertex_ai | claude-3-5-sonnet-v2-20241022 | txt, pdf, jpeg, jpg, png, gif | 200000 |\n| vertex_ai | gemini-2.0-flash-thinking-exp-01-21 | txt, pdf, jpg, png, mp3, mp4 | 32000 |\n| vertex_ai | gemini-1.5-pro | txt, pdf, jpg, png, gif, mp3, mp4, ogg | 2097152 |\n| vertex_ai | gemini-1.5-pro-002 | txt, pdf, jpg, png, gif, mp3, mp4, ogg | 2097152 |\n| vertex_ai | gemini-2.0-flash-exp | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| vertex_ai | gemini-2.0-flash-001 | txt, pdf, jpg, png, gif, mp3, mp4 | 1048576 |\n| vertex_ai | gemini-2.0-flash-lite | txt, pdf, jpg, jpeg, png, mp3, mp4, ogg | 1048576 |\n| vertex_ai | gemini-2.0-flash | txt, pdf, jpg, jpeg, png, mp3, mp4, ogg | 1048576 |\n| vertex_ai | gemini-2.5-pro-exp-03-25 | txt, pdf, jpg, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| vertex_ai | gemini-2.5-flash-preview-04-17 | txt, pdf, jpg, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| vertex_ai | gemini-2.5-pro-preview-05-06 | txt, pdf, jpg, jpeg, png, mp3, ogg, mp4 | 1048576 |\n| xai | grok-vision-beta | txt, pdf, png | 8192 |\n| xai | grok-2-vision-1212 | txt, pdf, png | 32768 | <sup>(1)</sup> - Encrypted files are not supported. ## See Also [[200]] [[843]] {{{12}}} {{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "2021f0ec-69d2-4e24-a755-4628261a3c84",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "2388efed-6502-4c39-86ad-d750d7873a2f",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?406",
          "evaluationResultRowContextName": "406",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "| | | - openrouter/qwen3-8b:free<br>- openrouter/qwen3-14b:free<br>- openrouter/qwen3-30b-a3b:free<br>- openrouter/qwen3-32b:free<br>- openrouter/qwen3-235b-a22b:free<br>- openrouter/qwen3-32b<br>- openrouter/qwen3-235b-a22b | \u2705 | | | </div> <sup>(1)</sup> - To use these models the temperature must be set to 1, check [Reasoning models](https://platform.openai.com/docs/guides/reasoning/quickstart). <sup>(2)</sup> - These models do not support the Temperature parameter in the request body. <sup>(3)</sup> - This model is only available via API. Please note that it does not support the \"system\" role in the request body. <sup>(4)</sup> - This model is only available via [Responses API](1180.html). <sup>(5)</sup> - Deprecation since May 2025 release. ### [Globant Enterprise AI LLM consumption limits in SaaS mode](#Globant+Enterprise+AI+LLM+consumption+limits+in+SaaS+mode) When using Globant Enterprise AI in SaaS mode, you have a monthly limit of 11,000 requests for the following LLMs: - **OpenAI:** - GPT-4.5 - GPT-4.1 - GPT-4o - o1 - o3 - o3-mini - o4-mini - **Google Vertex AI:** - Gemini Pro 2.5 - Gemini Pro 1.5 - Claude 3.7 Sonnet - **AWS Bedrock:** - Claude 3.7 Sonnet - Claude 3.5 Sonnet (v1 and v2) - Claude 3 Opus - Llama 3.1 Instruct (405b) For any other LLMs or models, pricing and usage limits will be evaluated on a case-by-case basis. ### [See Also](#See+Also) [LLM Troubleshooting](290.html) [Deprecated Models](843.html)\n\n# Supported Chat Models List of supported models [via /chat Api](34.html) <style type=\"text/css\">\ntd, th { border: 1px solid silver;\n}\n</style> <div class=\"ResponsiveTable\"> | Module | Provider | Model Full Name | Chat Support | Function Calling support | Environment support |\n|---|---|---|---|---|---|\n| saia.models.openai | OpenAI | - openai/gpt-4o<br>- openai/gpt-4o-mini<br>- openai/gpt-4o-2024-11-20<br>- openai/gpt-4.5-preview<br>- openai/gpt-4.1<br>- openai/gpt-4.1-mini<br>- openai/gpt-4.1-nano<br>- openai/o1<sup>(1)</sup><br>- openai/o3<sup>(1)</sup><br>- openai/o3-mini<sup>(1)</sup><br>- openai/o4-mini<sup>(1)</sup><br>openai/* (any openai model) | \u2705 | \u2705 | - Beta<br>- qa<br>- Production |\n| | | - openai/o1-preview<sup>(1)(5)</sup><br>- openai/o1-mini<sup>(1)(5)</sup><br>- openai/chatgpt-4o-latest<br>- openai/gpt-4o-search-preview<sup>(2)</sup><br>- openai/gpt-4o-mini-search-preview<sup>(2)</sup> | \u2705 | | |\n| | | - openai/o1-pro<sup>(1)(4)</sup> | \u2705 | \u2705 | - Beta<br>- qa |\n| saia.models.googlevertexai | Google VertexAI | - vertex_ai/gemini-2.5-pro-preview-05-06<br>- vertex_ai/gemini-2.5-pro-exp-03-25<br>- vertex_ai/gemini-2.5-flash-preview-04-17<br>- vertex_ai/gemini-2.0-flash<br>- vertex_ai/gemini-2.0-flash-lite<br>- vertex_ai/gemini-2.0-flash-001<br>- vertex_ai/gemini-2.0-flash-exp<br>- vertex_ai/gemini-1.5-pro<br>- vertex_ai/gemini-1.5-pro-002<br>- vertex_ai/claude-3-7-sonnet-20250219<br>- vertex_ai/claude-3-5-sonnet-20240620<br>- vertex_ai/claude-3-sonnet-20240229<br>- vertex_ai/claude-3-opus-20240229<br>- vertex_ai/claude-3-haiku-20240307<br>- vertex_ai/claude-3-5-sonnet-v2-20241022<br>- vertex_ai/claude-3-5-haiku-20241022<br>- vertex_ai/meta.llama-3.1-405b-instruct-maas<br>- vertex_ai/meta-llama-4-maverick-17b-128e-instruct-maas<br>- vertex_ai/meta-llama-4-scout-17b-16e-instruct-maas | \u2705 | \u2705 | - Beta<br>- qa<br>- Production |\n| | | - vertex_ai/gemini-2.0-flash-thinking-exp-01-21<br>- vertex_ai/codestral-2501 | \u2705 | | |\n| | | - vertex_ai/meta.llama-3.2-90b-vision-instruct-maas | \u2705 | \u2705 | - Beta |\n| saia.models.azure | Azure OpenAI | - azure/gpt-4<sup>(5)</sup><br>- azure/gpt-4o<br>- azure/gpt-4o-mini<br>- azure/o1<br>- azure/o3-mini | \u2705 | \u2705 | - Beta<br>- qa<br>- Production |\n| | | - azure/o1-mini | \u2705 | | |\n| | Azure AI Foundry | - azure/deepseek-r1 | \u2705 | | - Beta |\n| saia.models.anthropic | Anthropic | - anthropic/claude-3-sonnet-20240229<br>- anthropic/claude-3-opus-20240229<br>- anthropic/claude-3-haiku-20240307<br>- anthropic/claude-3-5-sonnet-20240620<br>- anthropic/claude-3-5-sonnet-20241022<br>- anthropic/claude-3-5-haiku-20241022<br>- anthropic/claude-3-7-sonnet-latest | \u2705 | \u2705 | - Beta<br>- qa<br>- Production |\n| saia.models.awsbedrock | AWS Bedrock | - awsbedrock/anthropic.claude-3-7-sonnet<br>- awsbedrock/anthropic.claude-3-haiku<br>- awsbedrock/anthropic.claude-3-sonnet<br>- awsbedrock/anthropic.claude-3.5-sonnet<br>- awsbedrock/anthropic.claude-3-opus<br>- awsbedrock/meta.llama3-8b<br>- awsbedrock/meta.llama3-70b<br>- awsbedrock/amazon.titan-lite-v1<br>- awsbedrock/amazon.titan-express-v1<br>- awsbedrock/cohere.command<br>- awsbedrock/meta.llama3-1-70b<br>- awsbedrock/meta.llama3-1-405b<br>- awsbedrock/anthropic.claude-3.5-sonnet-v2<br>- awsbedrock/anthropic.claude-3.5-haiku<br>- awsbedrock/amazon.nova-pro-v1:0<br>- awsbedrock/amazon.nova-lite-v1:0<br>- awsbedrock/amazon.nova-micro-v1:0 | \u2705 | \u2705 | - Beta<br>- qa<br>- Production |\n| | | - awsbedrock/us.deepseek.r1-v1:0 | \u2705 | | |\n| | | - awsbedrock/meta.llama3-2-1b<br>- awsbedrock/meta.llama3-2-3b<br>- awsbedrock/meta.llama3-2-11b<br>- awsbedrock/meta.llama3-2-90b | \u2705 | \u2705 | - Beta |\n| saia.models.gemini | Gemini | - gemini/gemini-1.5-flash-latest<br>- gemini/gemini-1.5-flash-exp-0827<br>- gemini/gemini-1.5-flash-8b-exp-0827<br>- gemini/gemini-1.5-pro-latest<br>- gemini/gemini-1.5-pro-exp-0801<br>- gemini/gemini-1.5-pro-exp-0827<br>- gemini/gemini-exp-1206<br>- gemini/gemini-2.0-flash-exp<br>- gemini/gemini-2.0-pro-exp<br>- gemini/gemini-2.0-flash-lite-preview<br>- gemini/gemini-2.5-pro-exp-03-25 | \u2705 | \u2705 | - Beta |\n| | | - gemini/gemini-2.0-flash-thinking-exp-1219<br>- gemini/gemini-2.0-flash-thinking-exp-01-21<br>- gemini/gemma-3-27b-it<sup>(3)</sup> | \u2705 | | |\n| saia.models.groq | Groq | - groq/llama-3.3-70b-versatile<br>- groq/llama-3.2-3b-preview<br>- groq/llama-3.2-1b-preview<br>- groq/llama-3.1-8b-instant<br>- groq/mixtral-8x7b-32768<br>- groq/meta-llama-4-scout-17b-16e-instruct<br>- groq/meta-llama-4-maverick-17b-128e-instruct<br>- groq/qwen-qwq-32b | \u2705 | \u2705 | - Beta<br>- qa |\n| | | - groq/llama-3.2-11b-vision-preview<br>- groq/llama-3.2-90b-vision-preview | \u2705 | \u2705 | - Beta |\n| | | - groq/deepseek-r1-distill-llama-70b | \u2705 | | |\n| saia.models.deepseek | DeepSeek | - deepseek/deepseek-chat | \u2705 | \u2705 | - Beta |\n| | | - deepseek/deepseek-reasoner | \u2705 | | |\n| saia.models.nvidia | NVidia | - nvidia/nvidia.nemotron-mini-4b-instruct<br>- nvidia/meta.llama-3.1-8b-instruct<br>- nvidia/meta.llama-3.1-70b-instruct<br>- nvidia/meta.llama-3.1-405b-instruct<br>- nvidia/meta.llama-3.2-3b-instruct<br>- nvidia/meta-llama-4-scout-17b-16e-instruct<br>- nvidia/llama-3.3-nemotron-super-49b-v1 | \u2705 | \u2705 | - Beta<br>- qa |\n| | | - nvidia/meta.llama-3.2-1b-instruct<br>- nvidia/llama-3.1-nemotron-ultra-253b-v1<br>- nvidia/meta-llama-4-maverick-17b-128e-instruct | \u2705 | | |\n| | | - nvidia/llama-3.1-nemotron-70b-instruct | \u2705 | \u2705 | - Beta |\n| | | - nvidia/deepseek-ai-deepseek-r1 | \u2705 | | |\n| saia.models.sambanova | SambaNova | - sambanova/Llama-4-Scout-17B-16E-Instruct<br>- sambanova/DeepSeek-R1-Distill-Llama-70B<br>- sambanova/Meta-Llama-3.1-8B-Instruct<br>- sambanova/Meta-Llama-3.3-70B-Instruct | \u2705 | \u2705 | - Beta |\n| | | - sambanova/DeepSeek-V3-0324<br>- sambanova/DeepSeek-R1<br>- sambanova/QwQ-32B<br>- sambanova/Meta-Llama-3.1-405B-Instruct<br>- sambanova/Meta-Llama-3.2-1B-Instruct<br>- sambanova/Meta-Llama-3.2-3B-Instruct<br>- sambanova/Llama-4-Maverick-17B-128E-Instruct | \u2705 | | |\n| saia.models.cerebras | Cerebras | - cerebras/llama3.1-8b<br>- cerebras/llama3.1-70b<br>- cerebras/llama-3.3-70b | \u2705 | \u2705 | - Beta |\n| | | - cerebras/llama-4-scout-17b-16e-instruct | \u2705 | | |\n| saia.models.cohere | Cohere | - command-r<br>- command-r-08-2024<br>- cohere/command-r-plus<br>- cohere/command-r-plus-08-2024<br>- cohere/command-r7b-12-2024 | \u2705 | \u2705 | - Beta |\n| saia.models.xai | xAI | - xai/grok-3-beta<br>- xai/grok-3-fast-beta<br>- xai/grok-3-mini-beta<br>- xai/grok-3-mini-fast-beta<br>- xai/grok-2-vision-1212<br>- xai/grok-2-1212<br>- xai/grok-beta | \u2705 | \u2705 | - Beta |\n| | | - xai/grok-vision-beta | \u2705 | | |\n| saia.models.openrouter | OpenRouter | - [openrouter/auto](https://openrouter.ai/openrouter/auto) | \u2705 | \u2705 | - Beta |",
          "evaluationResultRowContextDocId": "ed388c6f-786b-41f9-b07f-6117e7d6b295",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "2f26aa5b-fedb-4a79-96e3-b68a07234be8",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?200",
          "evaluationResultRowContextName": "200",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#How to use LLMs with built-in web search tools via API This documentation provides guidance on how to use OpenAI and Gemini LLMs with built-in web search capabilities through the `chat/completions` API. ## Supported Models\n### OpenAI Models with Web Search:\n* openai/gpt-4o-search-preview\n* openai/gpt-4o-mini-search-preview\n### Gemini Models with Web Search:\n* vertex_ai/gemini-2.0-flash * vertex_ai/gemini-2.5-pro-exp-03-25 ## Request Format ### OpenAI Example Request\n```\ncurl --request POST \\ --url $BASE_URL/chat/completions \\ --header 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\ --header 'Content-Type: application/json' \\ --data '{ \"model\": \"openai/gpt-4o-search-preview\", \"messages\": [ { \"role\": \"user\", \"content\": \"Who won the last presidential elections in USA?\" } ], \"stream\": false, \"max_completion_tokens\": 1000 }'\n```\n*Note:* For these two models, the OpenAI request body does not support the `temperature` parameter. #### Expected Response\n```\n{ \"created\": 1741876215, \"usage\": { \"completion_tokens\": 432, \"prompt_tokens\": 10, \"total_cost\": 0.004345, \"total_tokens\": 442, \"currency\": \"USD\", \"completion_cost\": 0.00432, \"prompt_cost\": 0.000025 }, \"model\": \"gpt-4o-search-preview-2025-03-11\", \"choices\": [ { \"finish_reason\": \"stop\", \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Donald Trump won the 2024 United States presidential election, defeating Vice President Kamala Harris in a historic political comeback. ([time.com](https://time.com/7173338/donald-trump-wins-election-2024/?utm_source=openai))\" } } ], \"object\": \"chat.completion\"\n}\n``` ### Gemini Example Request\n```\ncurl --request POST \\ --url $BASE_URL/chat/completions \\ --header 'Authorization: Bearer $SAIA_PROJECT_APITOKEN' \\ --header 'Content-Type: application/json' \\ --data '{ \"model\": \"vertex_ai/gemini-2.0-flash\", \"messages\": [ { \"role\": \"user\", \"content\": \"What happened in Bahia Blanca, Argentina during the past days?\" } ], \"tools\": [ { \"googleSearch\": {} } ] }'\n```\nAs shown in the provided sample, it is required to enable the `googleSearch` tool. #### Expected Response\n```\n{ \"created\": 1741876381, \"usage\": { \"completion_tokens\": 417, \"prompt_tokens\": 12, \"total_cost\": 0.000252, \"total_tokens\": 429, \"currency\": \"USD\" }, \"model\": \"gemini-2.0-flash\", \"choices\": [ { \"finish_reason\": \"stop\", \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"In the past few days, specifically around March 7, 2025, Bahia Blanca, Argentina, experienced a devastating extreme rainfall event and severe flooding.\" } } ], \"object\": \"chat.completion\"\n}\n``` ### Additional References * [https://platform.openai.com/docs/models/gpt-4o-search-preview|OpenAI - gpt-4o-search-preview]\n* [https://platform.openai.com/docs/models/gpt-4o-mini-search-preview|OpenAI - gpt-4o-mini-search-preview]\n* [https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-with-google-search#googlegenaisdk_tools_google_search_with_txt-drest|Gemini - Ground with Google Search] ##Availability Since April 2025 release. {{{12}}}{{{17}}} ##MARKDOWN##",
          "evaluationResultRowContextDocId": "e507c5cb-0f7c-494a-a2f9-494e46c7822a",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "57fd8fdc-fb52-4f71-a243-82d54b990ef2",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?1079",
          "evaluationResultRowContextName": "1079",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#How to upload and manage Files via API It is possible to work with previously uploaded files to perform various operations such as summaries or analysis. To perform these operations, first it is necessary to upload the file using the [[450]], confirm the success of the operation, and then reference it in the requests to the models that require the file. Here is a step-by-step guide on how to upload and manage files using the File API. ## Step 1: Uploading a File Using the File API\nThe first step is to upload the file you want to manage. To do this, use the [[450##POST /files|POST /files]] endpoint of the File API. This endpoint allows you to upload a file and receive a confirmation that the file has been successfully uploaded. The name of the file, which will be used to reference it, can be specified in the `fileName` parameter or, if not provided, it will be the name of the uploaded file. ### Getting your projectId and organizationId You can get your `$PROJECT_ID` and `$ORGANIZATION_ID` using the [[22##GET /accessControl/apitoken/validate|GET /accessControl/apitoken/validate]] endpoint of the [[22]]. Keep in mind that you can replace `$SAIA_ORGANIZATION_APITOKEN` with your `$SAIA_PROJECT_APITOKEN` if needed. ### Sample cURL Command\nTo load a file, use the following sample:\n```bash\ncurl -X POST \"$BASE_URL/v1/files\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: multipart/form-data\" \\ -H \"fileName: $FILE_NAME\" \\ -H \"folder: testFolder\" \\ -H \"organizationId: $ORGANIZATION_ID\" \\ -H \"projectId: $PROJECT_ID\" \\ -F \"file=@/path/to/your_file.mp4\" ``` If you include the `fileName` parameter, that will be the name you will use to reference the file later. If you don't include it, you will need to use the original name of the uploaded file. For example, if you upload `\"file=@/path/to/your_file.mp4\"`, the reference name will be exampleFile (without the extension). ## Step 2: Validating the File Upload\nOnce the file has been successfully uploaded, the response will include information confirming the success of the operation. You don't need to use a dataFileId, but you can check the response to ensure the file was uploaded correctly. ### Sample response\n```json\n{ \"dataFileId\": \"string\", \"dataFileUrl\": \"string\", \"success\": true\n}\n```\n## Step 3: Create a Chat Assistant (if not created already) After uploading the file, you need to create a [[708]] to process the uploaded content, if you haven't already created one. This is done through the [[35##POST/assistant|POST /assistant]] endpoint of the [[35]]. ### Sample cURL Command ```bash\ncurl -X POST \"$BASE_URL/v1/assistant\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"type\": \"chat\", \"name\": \"vertex_ai/gemini-2.0-flash-exp -Test File Upload\", \"prompt\": \"You are a helpful assistant\", \"llmSettings\": { \"providerName\": \"vertex_ai\", \"modelName\": \"gemini-2.0-flash-exp\", \"temperature\": 0.10, \"maxTokens\": 8192, \"uploadFiles\": true } }'\n``` ## Step 4: Referencing the File Once the Chat Assistant is set up, you can reference the file in a request to the Assistant for analysis or summary, using the [[34]] and specifying the file name in the request body. ### Sample cURL Command to Summarize a File: ```bash\ncurl -X POST \"$BASE_URL/chat\" \\ -H \"Authorization: Bearer $SAIA_PROJECT_APITOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"saia:assistant:vertex_ai/gemini-2.0-flash-exp -Test File Upload\", /* Using the assistant created in Step 3 named 'vertex_ai/gemini-2.0-flash-exp -Test File Upload' */ \"messages\": ~~[~~ { \"role\": \"user\", \"content\": \"Provide the content {file:$FILE_NAME}\" } ~~]~~, \"stream\": false }'\n``` ## Alternative: Inquire About Files - Interact Directly with LLMs You can also interact with LLMs directly, asking questions about files like images or videos. [regions]\n[region|cURL]\nSample 1: Ask GPT-4o About an Image\n```bash\ncurl --location '&lt;BASE_URL&gt;/chat' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;SAIA_PROJECT_APITOKEN&gt;' \\\n--data '{ \"model\": \"openai/gpt-4o\", \"max_tokens\": 1024, \"temperature\": 0.1, \"messages\": ~~[~~ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": ~~[~~ { \"text\": \"Tell me what this is about?\", \"type\": \"text\" }, { \"image_url\": { \"url\": \"&lt;IMAGE_FILE_URL&gt;\" }, \"type\": \"image_url\" } ~~]~~ } ~~]~~\n}'\n```\n[/region]\n[region|Python]\nSample 2: Python Code - Sending Gemini a Video File for Analysis\n```python\nimport requests\nimport json\nimport base64 # Define the API endpoint and authentication headers\nurl = \"&lt;BASE_URL&gt;/chat/completions\"\nheaders = { 'Content-Type': 'application/json', 'Authorization': 'Bearer &lt;SAIA_PROJECT_APITOKEN&gt;'\n} # Function to encode a file in Base64 format\ndef encode_file(file_path): \"\"\"Encodes a file in Base64 format for API submission.\"\"\" with open(file_path, 'rb') as file: return base64.b64encode(file.read()).decode('utf-8') # Replace 'path_to_your_file.mp4' with the actual file path\nfile_path = \"path_to_your_file.mp4\"\nencoded_file = encode_file(file_path) # Construct the API request payload\npayload = { \"model\": \"vertex_ai/gemini-1.5-pro-002\", \"messages\": ~~[~~ { \"role\": \"user\", \"content\": ~~[~~ { \"type\": \"text\", \"text\": \"What's this file about?\" }, { \"type\": \"image_url\", \"image_url\": f\"data:video/mp4;base64,{encoded_file}\" } ~~]~~ } ~~]~~\n} # Send the request to the API\nresponse = requests.post(url, headers=headers, json=payload) # Print response details\nprint(\"Status Code:\", response.status_code)\nprint(\"\\nResponse:\")\nprint(json.dumps(response.json(), indent=2))\n```\n[/region]\n[/regions]\n{{{12}}}{{{17}}} ##MARKDOWN##",
          "evaluationResultRowContextDocId": "94fd1b65-0ca6-4b3c-8acf-eed9148e7023",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "b43984c1-79f2-46d5-b127-b45af5cb515b",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?454",
          "evaluationResultRowContextName": "454",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.2,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.0,
          "systemMetricId": "9871bd02-4da0-4c08-bd0b-977270bc3f74"
        }
      ],
      "dataSetRowId": "ec853802-39f5-4a7e-aeeb-d32bb12de99f",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0037328000",
      "evaluationResultRowEndDate": "2025-05-13T11:16:36",
      "evaluationResultRowOutput": "El modelo **vertex_ai/gemini-1.5-flash-latest </B>soporta los siguientes tipos de archivo:\n\n- txt\n- pdf\n- jpg\n- png\n- gif\n- mp3\n- mp4\n\nAdem\u00e1s, tiene una ventana de contexto de 1,048,576 tokens.\n\nEsta informaci\u00f3n est\u00e1 disponible en la tabla de modelos soportados y formatos de archivo compatibles. [1](https://wiki.genexus.com/enterprise-ai/wiki?406)",
      "evaluationResultRowStartDate": "2025-05-13T11:16:32",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "9211"
    }
  ]
}