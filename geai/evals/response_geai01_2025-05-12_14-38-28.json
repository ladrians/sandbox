{
  "dataSetId": "fbaa7af6-1f11-4e7b-b24d-fd5071759fc4",
  "evaluationPlanId": "65234838-9390-44af-ac14-0b7f98e33008",
  "evaluationPlanProfileName": "geai",
  "evaluationResultChunckCount": 7,
  "evaluationResultChunckSize": "7000",
  "evaluationResultCost": "0.0148325400",
  "evaluationResultDuration": "58",
  "evaluationResultEndDate": "2025-05-12T17:00:50",
  "evaluationResultId": "64af8dab-5033-49bf-baa2-592fa4122510",
  "evaluationResultMaxTokens": "1503",
  "evaluationResultModelName": "gpt-4.1-mini",
  "evaluationResultProviderName": "openai",
  "evaluationResultRetrievalMethod": "multiQuery",
  "evaluationResultScoreAverage": 0.0,
  "evaluationResultStartDate": "2025-05-12T16:59:52",
  "evaluationResultStatus": "F",
  "evaluationResultTemperature": 0.0,
  "evaluationResultUploadFiles": false,
  "rows": [
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#Globant Enterprise AI API Reference Globant Enterprise AI provides various APIs that, on one hand, allow integration with Large Language Model (LLM) systems and perform actions that modify the platform's metadata. On the other hand, a different set of APIs enables interaction with defined assistants. ## Available APIs - [[34]]\n- [[1180]]\n- [[663]]\n- [[22]]\n- [[29]]\n- [[625]]\n- [[422]]\n- [[1165]]\n- [[35]]\n- [[450]]\n- [[1183]]\n- [[802]]\n- [[407]]\n- [[893]] ## Generic Variables Notice the following properties needed when using the API. | Variable | Description |\n|---|---|\n| `$BASE_URL` | The base URL for your Globant Enterprise AI installation, for example, `https://api.saia.ai` or the value provided to you. |\n| `$SAIA_APITOKEN` | An API token generated for each project. | ## Authentication In order to use the API, you need to authenticate each request using [[564]]. These tokens are managed in [[42]]. To authenticate your requests, you need to provide your token via HTTP Bearer Token. This means that your token is encoded in the username field, and the password field should be left empty. Once authenticated, you will be able to access all endpoints within your API scope. For security purposes, it is strongly recommended that you do not share your API tokens with anyone and revoke them immediately if they are compromised. ## Errors REST API employs the widely accepted practice of using HTTP response codes to convey the status of an API request. The codes in the 2xx range indicate that the request was successful and the server has returned the expected data. On the other hand, the codes in the 4xx range signify that the request failed because of a client-side error, such as missing or invalid parameters, unauthorized access, or any other fault in the request. The codes in the 5xx range suggest that there's an error on the server side, and the request couldn't be completed due to a server malfunction or connectivity issue. Such errors, fortunately, are infrequent in the service. By following these HTTP response codes, users can easily understand whether their API requests have succeeded or failed, and the probable causes of failure if there are any. If there is an error during the execution, all APIs return a list of errors and a status code `400*`: ```\n{ \"errors\": [ { \"id\": \"integer\", \"description\": \"string\" }, ... ]\n}\n``` ## Versioning The API versioning strategy is designed to minimize disruptions to your application when backwards-incompatible changes are introduced. Whenever changes are made to the API, a new version is released. This approach allows you to continue using the previous version of the API until you are ready to upgrade to the latest version. It is strongly recommended that you always specify the version number when making API requests to ensure the correct behavior of your application. You can find the latest version number in the documentation or by contacting the support team. By using versioning, you ensure that your application remains stable and functional, while still providing access to the latest API features and functionality. ## Cache control Globant Enterprise AI has a cache control that is enabled by default for APIs ([[34]] and [[29|RAG Assistants API]]), but disabled when accessing from the Frontend. In the case of APIs, to take advantage of cached content with CacheHit, the query must be identical in terms of assistantname, version, llmsettings, and prompt. Any modification to these parameters will result in cache invalidation. To control the state of the cache in APIs, the HTTP header is used in requests: - To enable caching: X-Saia-Cache-Enabled=true\n- To disable caching: X-Saia-Cache-Enabled=false It is important to note that the cache expires after 7 days. {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "1bed5590-a8e6-46b8-a194-c2a8ae6d8e63",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "3e5fcd45-094b-4bfe-bcc2-51c7bdd9bb13",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?20",
          "evaluationResultRowContextName": "20",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# RAG Custom File Format sample Suppose the following manually generated *sample.json.custom* file; using the *country* and *type* metadata elements ```json\n[ { \"pageContent\": \"Globant Enterprise AI is a business platform designed to facilitate the implementation of AI-Assistants tailored to your specific needs and areas of expertise. You can create AI-Assistants that can integrate and interact with your current operations, processes, systems, and documents, creating new paths of innovation and productivity to explore. One of the great benefits of using Globant Enterprise AI is the ability to select a Large Language Model (LLM) and later switch to another without changing your definitions. Thus, Globant Enterprise AI acts as a secure bridge, connecting enterprise applications to LLMs while providing a variety of tools and features to boost productivity and innovation. Since Globant Enterprise AI is an intermediate layer, anyone who works with it is protected because the data will not be made public or used by the LLMs.\", \"metadata\": { \"name\": \"name\", \"description\": \"GEAI intro chunk 01\", \"source\": \"https://www.genexus.com\", \"country\": \"UY\", \"type\": \"article\" } }, { \"pageContent\": \"Related to gxeai and from a process perspective, it is also crucial to consider development elements such as versioning assistants, testing assistants, and deploying them. Globant Enterprise AI was designed with an architecture of multiple logical layers which can be accessed independently and incrementally. So you can just start by just consuming some particular model, but then conceptualize those accesses as assistants and then as a use case.\", \"metadata\": { \"name\": \"name\", \"description\": \"GEAI perspective chunk 02\", \"source\": \"https://www.genexus.com\", \"country\": \"AR\", \"type\": \"item\" } }, { \"pageContent\": \"Access to the platform's various layers is generally programmable using any programming language. Access is granted via access tokens per project or organization. In addition to programmatic interfaces, you can quickly create Playgrounds for demonstrating AI technology capabilities. If you already are using an integration with OpenAI API with minimal changes, simply update the base URL and Authorization header in your existing OpenAI SDK to setup the route request through Globant Enterprise AI Proxy\", \"metadata\": { \"name\": \"name\", \"description\": \"GEAI access chunk 03\", \"source\": \"https://www.genexus.com\", \"country\": \"BR\", \"type\": \"summary\" } }\n]\n``` And using the playground to ask questions, the *Sources* section will use the *description* and source *metadata* elements as follows: ![Image](./images/139.png)",
          "evaluationResultRowContextDocId": "cc35a1db-3ea1-4fd3-8f52-6cc1bd4f0bba",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "7d750657-b10e-44c5-b18e-652a42635b8b",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?138",
          "evaluationResultRowContextName": "138",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Globant Enterprise AI Lab An AI Agent is a system designed to execute specific tasks with some degree of autonomy. Unlike Large Language Models (LLMs), which primarily respond to specific instructions, AI Agents are designed to perceive their environment, process information, make autonomous decisions and take proactive actions to achieve specific goals. In addition, they are able to dynamically adapt to unforeseen changes or challenges, continuously learning from their experiences. The autonomy of AI Agents is based on three fundamental characteristics: * **Proactive Planning and Execution:** AI Agents do not simply react to isolated instructions; they set clear objectives and plan how to achieve them, breaking down complex tasks into manageable steps, anticipating obstacles and adjusting their approach in real time. * **Advanced Reasoning and Continuous Learning:** They use sophisticated reasoning strategies, such as \u201cChain of Thought\u201d, to analyze situations, infer conclusions and decide on the most effective actions. They incorporate different types of memory (declarative, procedural and working) that allow them to learn from previous experiences, generalize knowledge, and continuously improve their performance. * **Integration with Tools and Data Sources:** AI Agents integrate effectively with external tools, APIs and various data sources, enabling them to access and process information from multiple systems and environments, thus extending their range of practical application. ## AI Agents in Globant Enterprise AI Recognizing the transformative potential of AI Agents, [[15|Globant Enterprise AI]] introduces **Globant Enterprise AI Lab**: a platform specialized in simplifying the creation, configuration and management of these intelligent systems. **Globant Enterprise AI Lab** not only facilitates the creation of Agents, but also drives new, future-oriented ways of working. **Globant Enterprise AI Lab** allows you to define the type of Agent required through a simple and conversational interaction. Through this platform, it is possible to configure in detail: * The specific tasks to be solved by the Agent.\n* The most appropriate reasoning model for each task.\n* The thinking strategies to be used by the Agent. Each configured Agent is clearly represented in JSON format, thus facilitating its evaluation and direct testing on the platform. Once created and validated, **Globant Enterprise AI Lab** also makes it possible to publish the Agent for widespread use, integrating it into automated business processes and workflows that require human interaction. This allows to effectively combine automated tasks performed by autonomous Agents with those activities executed by people, creating agile, collaborative and future-oriented work dynamics. ##Availability Since April 2025 release. {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "466fe5e4-c067-4107-9e9a-56883dea6cd8",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "859a3004-54da-4432-ba78-ebef348f8280",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?972",
          "evaluationResultRowContextName": "972",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Globant Enterprise AI Overview [Globant Enterprise AI](https://www.genexus.com/en/products/genexus-enterprise-ai) is a business platform designed to facilitate the implementation of AI assistants tailored to your specific needs and areas of expertise. You can create AI Assistants that can integrate and interact with your current operations, processes, systems, and documents, creating new paths of innovation and productivity to explore. One of the great benefits of using Globant Enterprise AI is the ability to select a Large Language Model (LLM) and later switch to another without changing your definitions. Thus, Globant Enterprise AI acts as a secure bridge, connecting enterprise applications to LLMs while providing a variety of tools and features to boost productivity and innovation. Since Globant Enterprise AI is an intermediate layer, anyone who works with it is protected because the data will not be made public or used by the LLMs. To facilitate the integration of LLMs into your workflow, Globant Enterprise AI offers features that enable you to: - Monitor access to LLMs through customized authentication and authorization protocols.\n- Provide a Web interface with a look and feel that is familiar to LLM users, branded and managed by your organization.\n- Supervise the costs and interactions associated with each LLM for [streamlined analysis and control](32.html).\n- [Manage quotas](71.html) per solution to keep your spending in check. In addition, for those looking to integrate AI capabilities into their custom corporate software, Globant Enterprise AI provides the ability to: - Automatically generate [APIs](20.html) and version the assistants and [prompts](85.html) you create using the Chat Assistant.\n- Optimize your AI exploration by effortlessly switching between different LLMs.\n- Reduce dependency between the AI applications you develop and their underlying LLMs.\n- Leverage pre-built AI functions like 'Chat with your Documents' and 'Chat with your Data' to accelerate your AI adoption journey. These AI applications, designed to be integrated into production environments, require a set of essential non-functional and functional characteristics for business applications. ### Non-functional Characteristics - Observability\n- Maintainability, Future Proofing\n- Security\n- Scalability\n- Accountability ### Functional Characteristics - Retrieval Augmented Generation (RAG) for private documentation\n- Intent-based navigation for existing user interfaces\n- Assistants for power autocomplete\n- Definition and chaining of assistants and actuators\n- Autonomous or semi-autonomous execution agents From a process perspective, it is also crucial to consider development elements such as versioning and testing assistants, and deploying them. Globant Enterprise AI was designed using an architecture of multiple logical layers that can be accessed independently and incrementally. This means you can start by just consuming a particular model, then conceptualize those accesses as assistants, and finally as a use case. ![Globant Enterprise AI Architecture](./images/18.png) ### What problems does Globant Enterprise AI solve with a unique time to market? - Centralized data and cost observability\n- Development of POCs (Proof Of Concept) or products that quickly \u201ccognify\u201d business, reducing costs or time for a specific business area\n- Time to market for AI solutions in a safe and scalable manner\n- Canonical use cases for AI solutions: - Intent-based navigation using [heterogeneous APIs](110.html) and [relational databases](159.html) - [RAG](44.html) - [Data Analyst](886.html) - Autocomplete - Business assistant-based processing conceptualization ### Observability It is essential to understand the information lifecycle and be able to measure various business indicators regarding the use and costs of the AI models being utilized. Business applications need to understand which business domains are using which AI models; check the [backoffice section](42.html). ### Maintainability, Future Proofing AI models are evolving rapidly. Can businesses keep up? Can your interfaces fluctuate and test each model without breaking connection interfaces? How can you become independent of these decisions, often tied to cost, privacy, or other factors? Globant Enterprise AI allows for the independence of created interfaces from the accessed models, enabling agents to evolve with a business perspective, separate from the underlying AI models' evolution. ### Security Businesses require access security, data security, data governance, and alerts for potential rule breaches. Globant Enterprise AI provides everything needed to achieve control over what happens with data within the company, ensuring that data travels securely and adheres to business-imposed rules. ### Scalability Performance should not be degraded by business success. Having infrastructure designed for secure and scalable communication with models is essential when deploying products in production. Using Globant Enterprise AI ensures that if the business succeeds, scalability concerns are addressed. ### Built-in Patterns Solved Globant Enterprise AI provides all the services and abstractions necessary for implementing emerging UX patterns in an [integrated frontend](82.html). It offers the following services: - Chat with any document set, essentially building a private ChatGPT for unstructured company documents (RAG Assistant)\n- Assistant creation and chaining for various business objectives\n- Autocomplete assistants\n- Summarization assistants\n\n- Private instance to chat with LLMs ### Testing your assistants Globant Enterprise AI allows you to version all your assistants; this is a great feature that allows you to evolve assistants without breaking things. You need to be aware that programming with non-deterministic agents could be a challenge without help. With Globant Enterprise AI, you can declare different versions of your assistants and start enabling different access to them depending on the consumer. You can create a complete regression test suite with the new version of a new assistant before entering production. Actually, you can match your current staging planning in order to evolve your solution with confidence. ### Access to Globant Enterprise AI Access to the platform's various layers is generally programmable using any programming language. Access is granted via access tokens per project or organization. In addition to programmatic interfaces, you can quickly create Playgrounds to demonstrate AI technology capabilities. If you are already using an integration with OpenAI API with minimal changes, simply update the base URL and Authorization header in your existing OpenAI SDK to set up the route request through [Chat API](34.html). Check the [Ingestion SDK](172.html) if you need to connect to different data-sources. ### See Also [Updates Log](31.html) [Coming Next](30.html) ### Videos ![Globant Enterprise AI Course](./images/197.png) [Globant Enterprise AI Course](https://training.genexus.com/en/learning/courses/artificial-intelligence/globant-enterprise-ai/introduction-to-the-course)",
          "evaluationResultRowContextDocId": "b6f9138e-f5c7-4b62-8756-6a0795c0bf3f",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "ea71402c-d218-4bce-8d1e-467b4a7b5499",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?15",
          "evaluationResultRowContextName": "15",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# API Assistant API Assistant is a feature of Globant Enterprise AI that allows you to define assistants to chat and interact with any external REST API compliant with the OpenAPI 3.0 specification or higher. This assistant leverages AI models and function calling capabilities to provide a dynamic way to chat with APIs using natural language. ## [Main features](#Main+features) Defining an API Assistant provides a range of advanced features designed to empower interaction with external APIs in an efficient and secure manner. Key features include: - **Compatibility with OpenAPI 3.0:** It is possible to interact with any API that is built following the OpenAPI 3.0 specification or later.\n- **Customizable instructions:** The user can provide specific text instructions to guide the AI assistant in making decisions and accurately handling requests.\n- **Model settings:** It allows you to adjust the AI model settings, such as temperature and the maximum number of tokens generated, to enhance and customize the assistant's answers according to the user's needs.\n- **Security:** It ensures the security of interactions through the use of API tokens. These tokens, which can be of Basic or Bearer type, are used in the Authorization header to authenticate and authorize requests, thus protecting information and data.\n- **Automated function calls:** It returns answers efficiently by making only the necessary API calls. This is achieved by Globant Enterprise AI's intelligent model and automated function calls handling end user requests.\n- **Multiple API calls:** The assistant\u2019s intelligence is shown by making sequential calls to different APIs to obtain complex results. For example, upon receiving a question such as 'What is the weather like in Tokyo and Montevideo?' the assistant identifies the need to query the weather API for each city mentioned. This ability to automatically handle multiple requests ensures a seamless experience for the user, who doesn't need to intervene in the process. ## [Prerequisites](#Prerequisites) Before starting to use API Assistant, it is important to ensure that certain prerequisites are met: - You need to access Globant Enterprise AI. To do so, you can visit the [official website](https://www.genexus.com/en/products/genexus-enterprise-ai), where you will find contact information and details on how to obtain access to the platform.\n- If your API is authenticated with an API Token (either Basic or Bearer), you must configure your requests to include this token in the authorization header, specifying the correct type. For example: `-H \"Authorization: Bearer $OPENAI_API_KEY\"`. In this way, when executing the call, the token will be used for the interaction.\n- Verify that the API Specification is documented according to OpenAPI 3.0 or higher, in JSON format. This is important, as APIs that do not comply with this specification may not be fully supported.\n- Avoid setting Message streaming because it is not supported. To define or update an API Assistant, you can use the [Globant Enterprise AI Backoffice](42.html) by following the steps described in [How to use API Assistant](123.html) or using the API.",
          "evaluationResultRowContextDocId": "b3f2c7ef-2bcc-4b6c-b879-cdeee55f7e13",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "f0390bdf-73c2-486b-9fe1-bc5bbcd38b12",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?110",
          "evaluationResultRowContextName": "110",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Globant Enterprise AI Contact Resources This document describes how to contact the technical support provided by Globant Enterprise AI, outlining the procedure for reporting incidents and the corresponding response times. ## Reporting Incidents to the Support Team You can report incidents to the Globant Enterprise AI Support Team through the following channels: 1. **Email:** You can send your inquiries to enterpriseai(at)genexus.com. When sending an email query, be sure to do so using your **GeneXus Account**.\n2. **Tracking System:** You can directly access the [https://frontend.support.genexus.com/frontend.querystep.aspx|Tracking System]. This method is recommended as it guides you through the process of reporting the inquiry. This ensures that the support team receives all the necessary information to respond as quickly as possible. It also allows you to track your inquiries. For detailed guidance on effectively reporting technical issues and navigating the Tracking System, read [[868]]. The support service is available from Monday to Friday (except holidays) during GMT-3 business hours. ### Tracking Your Inquiries You can easily track the status and progress of your support tickets through the [https://frontend.support.genexus.com/frontend.workwith_support.aspx|Support Portal]. ### Inquiry Priority When reporting an incident, you need to define its priority. This is important to address your request with the appropriate urgency. To determine the priority level, you can follow these guidelines: * **Level 1/High: Production down:** This is the most critical case, where an application in production is down.\n* **Level 2/Medium: Development down:** This level refers to a problem that makes it impossible to continue development work.\n* **Level 3/Low:** This level includes general inquiries, suggestions, or non-critical situations. When submitting an inquiry by email, you should include the priority level in the subject line. For example: \"Level 2: I get an error when running Globant Enterprise AI\". Assigning the most appropriate priority level to each inquiry is essential to maintain the quality standards of the service. ### Ticket Number In all cases, when an inquiry is submitted, it enters the system and a ticket number is assigned. With this number, you can query the status of the ticket at any time. You can also add additional information or comments. If the support staff needs more information about a ticket to be able to solve it, you will receive an email with the request. This email will contain a link to answer the request from the Web application. You can also reply directly to the email sent by the support staff. In both cases, the ticket will be updated in the system. All communication related to a ticket between the client and the support staff is recorded in the system, allowing for full traceability. ### Additional Resources * **FAQs:** Find answers to frequently asked questions about Globant Enterprise AI at [https://www.genexus.com/en/products/globant-enterprise-ai/faqs|Globant Enterprise AI FAQs].\n{{{12}}}{{{17}}}\n##MARKDOWN##",
          "evaluationResultRowContextDocId": "8a649f4a-185e-4213-a146-955f6c87a0f2",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "fa191f08-5d27-44e1-acc9-2ec7c80a4810",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?274",
          "evaluationResultRowContextName": "274",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.8,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.8,
          "systemMetricId": "8fd20b5c-68a7-42fa-afad-45d17350172f"
        }
      ],
      "dataSetRowId": "138b9984-3523-4eb3-8315-2b1100804629",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0024737000",
      "evaluationResultRowEndDate": "2025-05-12T16:59:59",
      "evaluationResultRowOutput": "Globant Enterprise AI is a business platform designed to facilitate the implementation of AI assistants tailored to specific needs and areas of expertise. It enables the creation of AI assistants that can integrate and interact with existing operations, processes, systems, and documents to create innovative and productive solutions. \n\nKey features include:\n- Acting as a secure bridge connecting enterprise applications to Large Language Models (LLMs) while protecting data privacy and security.\n- Allowing users to select and switch between different LLMs without changing their assistant definitions.\n- Providing observability for monitoring access to LLMs, supervising costs, and managing quotas.\n- Offering tools to automatically generate and version APIs, assistants, and prompts.\n- Supporting built-in AI patterns like Retrieval Augmented Generation (RAG) for private documents, intent-based navigation, autocomplete assistants, and autonomous agents.\n- Ensuring non-functional characteristics such as observability, maintainability, security, scalability, and accountability.\n- Supporting development processes with versioning, testing, and deployment of assistants.\n\nOverall, it serves to reduce time to market for AI solutions in a safe, scalable, and maintainable way, enabling businesses to innovate with AI efficiently and securely.\n\nMore detailed information can be found here: [Globant Enterprise AI Overview](https://wiki.genexus.com/enterprise-ai/wiki?15) [1](https://wiki.genexus.com/enterprise-ai/wiki?15)",
      "evaluationResultRowStartDate": "2025-05-12T16:59:52",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "5456"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# Chat with Data Assistant Chat with Data Assistant is a key component of [Globant Enterprise AI](15.html), designed to facilitate the interaction between end users and the database. This innovative assistant uses artificial intelligence to simplify the data query and retrieval process from any relational database that is in third normal form. End users can send queries in natural language directly from the [Frontend](82.html), without the need to access the database directly. This significantly streamlines the information search process, offering a more intuitive and efficient experience. One of the outstanding features of Chat with Data Assistant is its ability to create, edit, and delete queries directly from the **Frontend**. This provides end users with quick and easy access to the information stored in the database, without the need for additional tools. # [Creation and configuration of the assistant](#Creation+and+configuration+of+the+assistant) To create and configure the Chat with Data Assistant, access the [Globant Enterprise AI Backoffice](42.html), and upload the necessary metadata in a file with an **.export** extension. Next, configure the database connection options. It is important to select from the DBMS supported by Globant Enterprise AI. Currently, this list includes SQL Server, MySQL, PostgreSQL, Oracle, and SAP Hana (DB2 for iSeries, DB2 Universal Database, and Informix coming soon). Once the assistant is configured, you can go to the **Frontend** and select the Chat with Data Assistant that you have just created. From now on, you will be able to make queries using natural language, which significantly speeds up the data search and retrieval process. > **Note**: For the proper use of this assistant in a SaaS solution, the database must be accessible via the internet, and the required ADO.NET drivers must be installed to establish the connection. ### [See Also](#See+Also) [How to create a Chat with Data Assistant](170.html)",
          "evaluationResultRowContextDocId": "84e52739-9fd0-4e65-b9e5-ef39770245a0",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "23261b2f-0fcf-49ea-830d-d42d072f0d33",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?159",
          "evaluationResultRowContextName": "159",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Chat with Data Assistant Use Guidelines This document is intended to provide guidelines on how to ask questions to get information that was ingested in a particular [Chat with Data Assistant](159.html). ## General Considerations ### Use Natural Language Type your question naturally just like you would ask a colleague for information. It doesn't matter if you make spelling mistakes or use different synonyms. It is important to convey your intention to the assistant correctly. For example, suppose you have a database with information about continents/countries and want to get the list of continents together with the number of countries in each one. In this case, your query can be formulated in one of the following ways: - Number of countries per continent.\n- How many countries are there in each continent?\n- Give me the list of continents with the number of countries in each continent. ### Iterate and refine your questions It is not necessary to repeat all the previous information, as this is inferred from the conversation. For example, if you previously asked \"How many countries are there in each continent?\", you can follow up with \"Only those with more than 50 countries\". In this way, the assistant will understand that you want the list of continents and the number of countries for each one, but limited to those with more than 50 countries in total. Remember that with each new question, you can add more details or change the focus as needed. ## Questions that the Assistant can answer The assistant is designed to provide answers from the database and also from the database metadata. When a question is asked, the assistant first determines whether it is necessary to access the database to return the answer. If the question is about metadata (and therefore it isn't necessary to access the database to look for data), it can be answered using natural language. On the other hand, if the question requires access to the database, the assistant builds a Query and the possible [Output Formats](181.html) are Table, PivotTable, Chart or Card (no free text). ### Examples of metadata queries - What can I ask about?\n- What entities are available?\n- Can you suggest some questions I could ask? ### Examples of data queries - Show the top 10 countries by GDP per capita.\n- Number of patients by continent.\n- List the top 10 countries with the highest population. ## Questions that the Assistant can't answer There are certain types of queries that the assistant will not be able to answer, or whose answer may not be as expected. ### Information not contained in the database If the requested information is not in the database, the assistant will not be able to provide an answer. For example, if the database is only about continents/countries: **Question:** Total exports in the last year. **Answer:** The answer is not found in the entities provided because there is no entity or attribute related to export data or economic transactions. ### Concepts not represented in the database The assistant does not have information about the concepts handled in the organization but not included in the database. **Do:** Number of students promoted on the 05/01/2024 exam. **Don't:** Number of students with a grade higher than 6 on the 05/01/2024 exam. ### Calculations or Formulas It is not possible to request calculations based on known formulas referenced by name. For example, requesting the Human Development Index for a country will not work, even if the elements to calculate it are in the database. ### Predictions The assistant can only provide information based on the current data contained in the database. It cannot make predictions unless they are specifically recorded in a database table.",
          "evaluationResultRowContextDocId": "23e8cce5-2512-4ec0-8972-673baf964b70",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "a1908cc1-8231-46c1-ad2a-b98a09325e9b",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?177",
          "evaluationResultRowContextName": "177",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Enterprise AI For GeneXus Documentation, please visit [wiki.genexus.com](https://wiki.genexus.com/) <div style=\"background:#f8f8f8; padding:100px 128px\"> <span style=\"display: block; color: #004742; font-size: 46px; font-weight: 700; margin-bottom: 16px; line-height: 54px;\">Globant Enterprise AI Documentation</span> Delve into all the documentation of Globant Enterprise AI and stay ahead with insights into upcoming features. [Start reading](https://wiki.genexus.com/enterprise-ai/wiki?15,GeneXus+Enterprise+AI+Overview) [Learn with training videos](https://training.genexus.com/es/aprendiendo/cursos/inteligencia-artificial/genexus-enterprise-ai/introduccion-al-curso-6105673) --- - [Overview](15.html)\n- [API Reference](20.html) - [Chat API](34.html) - [Organization API](22.html) - [RAG Assistant API](29.html) - [Chat with Documents API](33.html) - [Embeddings API](422.html) - [Assistant API](35.html) - [File API](450.html) - [Feedback API](407.html) - [Error Codes](23.html)\n- [Using variables in Prompts](85.html)\n- [Tokens](293.html)\n- [Backoffice](42.html)\n- [Frontend](82.html)\n- [Playground](349.html)\n- [Start Page](408.html)\n- [Data Analyst Assistant](89.html) - [How to use Data Analyst Assistant](105.html) - [How to set the language in Data Analyst Assistant](464.html) - [Conversational Module](291.html)\n- [API Assistant](110.html) - [How to use API Assistant](123.html)\n- [Chat with Data Assistant](159.html) - [How to use Chat with Data Assistant](170.html) - [How to generate metadata for the Chat with Data Assistant](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?178,,) - [Chat with Data Assistant Use Guidelines](177.html) - [How to change the output format and filter results in Chat with Data Assistant](181.html)\n- [RAG Assistant](44.html) - [How to create and configure a RAG Assistant](60.html) - [File Formats](137.html) - [Use Guidelines](61.html) - [Update Configuration](64.html) - [Configuration - General Information](68.html) - [Configuration - Prompt](69.html) - [Configuration - Retrieval](70.html) - [Index Configuration](145.html) - [Supported Vector Store Provider](267.html) - [Advanced configuration](209.html) - [Context Prompt Template](205.html) - [VectorStore Search Options](206.html) - [Self Query Use Case](150.html) - [Parent Document Retrieval](204.html) - [Azure AISearch Search Options](207.html) - [Profile Metadata configuration](298.html)\n- [Flows](321.html) - [How to create a Flow](322.html) - [How to create a Flow with Conditional component](363.html) - [How to expose a Flow through Teams](391.html) - [How to expose a Flow through Slack](477.html) - [How to expose a Flow in a web page](473.html) - [User's Guide for Globant Enterprise AI Flow Builder](371.html) - [Management of Variables in Flows](470.html) - [Flows Conversation History](378.html) - [User Metrics in Flows](380.html) - [Message Metrics in Flows](382.html) - [Flows Configuration](384.html) - [Flows Section](389.html) - [Interactions to Create a Flow](494.html)\n- [User Access Rights Management](242.html) - [Roles](245.html) - [Members](253.html)\n- [File support for multimodal models](345.html) - [How to upload and manage Files via API](454.html) - [Supported models and file formats](406.html)\n- [File Support in Assistants](346.html)\n- [Ingestion SDK](172.html)\n- [AI-Driven Load Balancing](32.html)\n- [Managing quotas per project](71.html)\n- [Supported LLMs](200.html)\n- [Updates Log](31.html)\n- [Coming Next](30.html)\n- Troubleshooting - [File Upload](199.html) - [LLM](290.html) - [Embeddings](140.html) - [Vectorstores](308.html) - [Flows with RAG](445.html)",
          "evaluationResultRowContextDocId": "8b185fd7-17c6-4f63-88f2-154a2b6b3824",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "bf42d467-c4ab-4915-9abb-d445eae0b9ab",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?8",
          "evaluationResultRowContextName": "8",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# How to create a Chat with Data Assistant Here is a step-by-step guide on how to use [Chat with Data Assistant](159.html) and start chatting with the assistant from the [Playground Interface](349.html). ## Step 1: Generate the metadata The first step in configuring the Chat with Data Assistant is to generate the metadata required for its operation. This metadata contains information about the database schema. That is, the list of all tables with their fields, the relationships between them, table and field descriptions, data types, etc. You must ensure that the table and field names are descriptive and representative so that the assistant can generate efficient queries using the metadata information. For example, a table named Table001 with fields named Field001, Field002, etc. will not provide a clear identification that it contains country information, even if this information is present in the table. If your table or field names are not descriptive enough, you can include additional table and field descriptions in the assistant prompt. However, these descriptions must be equally clear and representative to ensure proper understanding of the information. You will have two options available to generate the necessary metadata, depending on whether the database was created with GeneXus or is external. Read more at [How to generate metadata for the Chat with Data Assistant](178.html). ### Database created with GeneXus If the database was created with [GeneXus](https://wiki.genexus.com/commwiki/wiki?1756,Category:GeneXus), this process is carried out using the [GXquery](https://wiki.genexus.com/commwiki/wiki?21971,Table+of+contents:GXquery+4.0,) extension, which generates the corresponding [Knowledge Base](https://wiki.genexus.com/commwiki/wiki?1836,Knowledge+Base) metadata. This extension is obtained directly from the GXquery installation directory. More information at [HowTo: Install the GXquery extension](https://wiki.genexus.com/commwiki/wiki?24482,HowTo:+Installing+the+GXquery+extension). For more information on how to create the metadata, please visit the following links: [HowTo: Create GXquery metadata in GeneXus](https://wiki.genexus.com/commwiki/wiki?17905,HowTo:+Create+a+GXquery+metadata+in+GeneXus), [HowTo: Export metadata in GXquery](https://wiki.genexus.com/commwiki/wiki?24012,HowTo:+Exporting+a+metadata+in+GXquery). ### External database In the case of an external database, you can use a command line tool to generate the necessary metadata. This tool uses reverse engineering to connect to the database and generate the corresponding metadata. When running this tool without parameters, a list of the options that can be configured is displayed. Once the metadata is generated, it is saved as a file with .export extension. ## Step 2: Create Chat with Data Assistant Enter the [Globant Enterprise AI Backoffice](42.html). On the left side of the screen, you can find the Backoffice menu. In this menu, click on Assistants, and then click on CREATE CHAT WITH DATA ASSISTANT. Next, in the Project dynamic combo box, select the project you want to work with (in this case, Default(DocumTeam) is used). ![Image](./images/922.png) In the window that is displayed, select the Add files option by clicking on the + Add files... button and load the **.export** file obtained in the previous step. Then click on SAVE to complete the process. ![Image](./images/161.png) This will open a popup window to enter the Name and Description of the assistant. You can also add a file for Icon by clicking on +Add files... After entering the required information, click on CONFIRM to save all changes. ![Image](./images/162.png) ## Step 3: Update Chat with Data Assistant Once you have created the Assistant, go back to the Assistants page where you can select UPDATE. ![Image](./images/923.png) This option allows you to view the version identifier with which it was saved. In addition, you can change the name and description, configure it as enabled or disabled, or add an icon if you wish. ![Image](./images/164.png) ## Step 4: Edit Chat with Data Assistant To edit the Assistants, go to the Assistants home page and click on CONFIG. ![Image](./images/924.png) A window similar to the one below will open, where you will have the option to make the following four edits: - Add metadata files, start or stop the assistant, and get the Log. This functionality is available in the General tab.\n- Enable or disable entities and attributes, set descriptions, and define the possible values of an attribute in the Entities tab.\n- Configure the database connection for the assistant in the DataStore tab.\n- Configure the metadata to be included in the queries and select the versions of the system assistants in the Settings tab. ![Image](./images/166.png) ### General In the General window, you will find the following information about the assistant: ![Image](./images/167.png) - **Assistant Name:** Displays the name of the assistant, which in this case is \u201cShowCase\u201d.\n- **Metadata ID:** Displays the unique ID for the metadata associated with the assistant.\n- **Metadata status:** Indicates whether the assistant's metadata is \u201cStopped\u201d or \u201cStarted\u201d. To interact with the assistant's metadata, you can use the following buttons: - **+ Add files...:** Allows adding the .export file containing the assistant's metadata.\n- **STOP:** Changes the assistant's metadata status from \u201cStarted\u201d to \u201cStopped\u201d, disabling the assistant. This can be useful when you need to prevent users from continuing to make queries using the assistant while you perform a task that will cause these queries to fail; for example, if you need to take the database offline. When you have finished, you can restart the assistant using the START button.\n- **IMPORT LOG:** Allows you to download a log file (.log) containing detailed information about various actions in the system when importing metadata. This file records events such as inserting metadata, creating Knowledge Bases, generating the design system, and importing objects, among others. ### Entities In the Entities tab, you can view all the entities that are part of the assistant. In this case, the visible entities are City, Continent, Country, CountryCovidData, PivotTableMetadata, and State. Each entity is listed with its name, a description, and whether or not it is enabled for queries. ![Image](./images/466.png) For each entity, you can: - **Edit the description**: this makes it easier for the assistant to better understand what the entity is about.\n\n- **Enable or disable** the entity to control whether the entity will be used in queries. By clicking on the **+** symbol to the left of each entity, the attributes of the entity are displayed and the sign changes to **-**. For example, for the City entity, the visible attributes include CityId, CityLocation, CityName, and CityPopulation, among others. For each attribute, you can perform the following actions: ![Image](./images/467.png) - Edit the attribute description.\n- Enable or disable the attribute for queries.\n- Define the possible values that the attribute can take, if applicable. To do this, click on **EDIT VALUES.** This opens a window where you can define a list of valid values and their descriptions for the corresponding attribute. This list must be entered as a comma-separated sequence, using the following format: `value1=description1, value2=description2, ....` Once you have made the necessary changes to the entities and attributes, you must click on the **CONFIRM** button to save and apply them. ### Datastore In the DataStore tab, the first thing to do is to select the type of database (DBMS) you will use. The values that the DBMS can take are SQL Server, MySQL, PostgreSQL, Oracle, and SAP Hana. It is important to note that the connection type is made through **ADO.NET**, which means that the corresponding libraries must be available to establish the connection. #### Prerequisites 1. **Database Accessibility:** The database must be accessible from Globant Enterprise AI, which implies that both must be on the same network or that the database is accessible via the Internet. 2. **ADO.NET Drivers Installation**: If they are not installed, you will need to install the ADO.NET drivers corresponding to the type of database you are using. The drivers allow GeneXus to connect efficiently with the selected DBMS. #### Configuration 1. **Connection Parameters**: Configure the necessary connection parameters, including: - **Server**: Name or IP of the server. - **Database**: Name of the database you wish to connect to. - **Port**: Database port number (depending on the DBMS, such as 1433 for SQL Server). - **Authentication**: Depending on the DBMS, you can require a username and password or use a built-in authentication. You can set additional options depending on the type of database selected. 2. **Test Connection button**: At the end of the configuration of each DBMS option, you will find a button called \u201cTest Connection\u201d that will allow you to verify the connection before saving the changes. Once you have configured the connection correctly, you can use the assistant to interact with the data stored in the selected database. > **Note**: This process does not require additional installations for Globant Enterprise AI outside of the ADO.NET drivers. The interface is designed to guide you through a simple connection setup. ### Settings In the **Settings** tab, you will find two sections: ![Image](./images/468.png) - Metadata to include in prompt\n- System assistants In the **Metadata to include in prompt** section, you can select which metadata will be included in the assistant prompts when generating queries. This option allows you to check the boxes for **Descriptions**, **Data types**, and **Valid values**. These options are useful if the field names are not descriptive or clear enough, so adding more metadata will help the assistant generate more understandable and accurate queries. By checking these boxes, descriptions, data types, and valid attribute values will be included in the assistant's queries, making it easier to interpret and use the information. In addition, in the **System assistants** section, you can select the versions of the system assistants to be used by the assistant by setting the **Query** parameter. Once you have configured these parameters, remember to click on **CONFIRM** to save the changes. ## Step 5: Test your Chat with Data Assistant Finally, you can test your Chat with Data Assistant via [Chat API](34.html) or by selecting Playground in the left side menu of the Backoffice window: ![Image](./images/916.png) If you have multiple assistants created, select the one you want to interact with. Once there, type the following message in the chat box: *Display the number of patients by continent and country in a pie chart, showing the most representative ones* and press Send. The result of the query will look as follows: ![Image](./images/629.png) The pie chart displays the number of hospitalized patients by continent and country, sorted in descending order by the number of patients. For additional details on how to customize the output format of these queries refer to [How to change the output format and filter results in Chat with Data Assistant](181.html). ### Icons to interact with the query At the bottom right of the screen, there are six icons that allow you to interact with the query and access additional information about the data being viewed. 1. **View query detail**: This icon, in the shape of an eye, allows you to analyze the query that the assistant has just returned. Clicking on this icon displays a pop-up window showing the following two tabs: - **Description:** This tab shows an explanation in natural language that describes the purpose and operation of the query in a more accessible way, helping you to understand where the data displayed in the results comes from. ![Image](./images/630.png) - **Expression:** This tab shows the structure of the executed query which shows how the query was constructed, what elements were used, and how the data was processed. ![Image](./images/632.png) 2. **Copy assistant response**: The second icon allows you to copy the current query in JSON format. This format includes details such as the name of the query, the expression used, and other parameters related to the execution. 3. **Save query**: The third icon opens a window to save the current query as a preset, allowing you to save the configuration and refer to it again in the future. 4. **Regenerate answer**: The fourth icon allows you to update the query by running it again. 5. **Good answer**: This icon lets you mark the query's response as satisfactory. 6. **Bad answer**: This icon allows you to mark the query response as inaccurate or unsatisfactory.",
          "evaluationResultRowContextDocId": "c09d2f5b-11eb-4ed9-934b-8ab39f7fc327",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "f4a70606-9adb-49f9-b035-aa61e6f00bdf",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?170",
          "evaluationResultRowContextName": "170",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# Chat Assistant The Chat Assistant is an AI solution in Globant Enterprise AI that creates conversational assistants with AI Large Language Models, enabling users to process documents, images, audio, and video. ### Chat Assistant creation To create a Chat assistant, follow these steps: 1. Access the [Globant Enterprise AI Backoffice](42.html) interface and log in with your organization administrator credentials.\n2. Go to the \"Assistants\" section. ![Assistants Section](./images/912.png) 3. Click on the option to \"CREATE CHAT ASSISTANT\".\n4. Define the assistant's initial instruction. Start by entering the initial instruction or context in the Prompt field. This will guide the assistant's responses during user interactions.\n5. Configure the assistant according to your needs: - **AI Provider and Model**: Select the AI provider (e.g., OpenAI, Google Vertex AI) and the specific model you want to use for the assistant's interactions. - **Temperature and Max Output Tokens**: Adjust these parameters to customize the creativity of the responses, and define the maximum token limit for the assistant's output. - **File Upload**: Enable these options if you need the assistant to process uploaded files. 6. Test the assistant before saving it. To test the assistant's behavior before saving, use the User Input section to enter sample queries. Once you've filled in the User Input, click on the TEST button to generate a response. Review the assistant's reply in the Response section to ensure it meets your expectations.\n7. Click on \"SAVE\" to proceed. A new window will appear where you must complete the following: - **Name**: Provide a unique name to identify the assistant. - **Description (optional)**: Add a brief explanation of the assistant's purpose. - **Icon (optional)**: Upload an icon to visually identify the assistant in your project list. 8. Click on \"CONFIRM\" to save the first version of your assistant and complete the process. > **Note**: When creating chat assistants, make sure the instructions, prompts, or initial context provided are clear and concise to obtain accurate and meaningful responses from the AI models.",
          "evaluationResultRowContextDocId": "8edcb62f-29ce-42f7-a843-aa7b4fea92fe",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "fc1ceb43-188d-42dd-89ab-8d436143d3db",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?708",
          "evaluationResultRowContextName": "708",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.7,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.8,
          "systemMetricId": "8fd20b5c-68a7-42fa-afad-45d17350172f"
        }
      ],
      "dataSetRowId": "4987c170-6ddd-47d7-8de2-27dacc172e2f",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0032729400",
      "evaluationResultRowEndDate": "2025-05-12T17:00:15",
      "evaluationResultRowOutput": "To get started with the **Chat with Data Assistant**, you can follow these main steps:\n\n### Step 1: Generate the Metadata\n- Metadata contains database schema information such as tables, fields, relationships, descriptions, and data types.\n- The quality of table and field names (descriptive and representative) is important for effective query generation.\n- You have two options to generate metadata depending on your database:\n  - **GeneXus-created databases: </B>Use the GXquery extension to generate metadata.\n  - **External databases: </B>Use a command line tool for reverse engineering to generate metadata.\n- The metadata file is saved with a `.export` extension.\n\n### Step 2: Create the Chat with Data Assistant\n- Access the [Globant Enterprise AI Backoffice](https://wiki.genexus.com/enterprise-ai/wiki?42).\n- In the Backoffice menu, click **Assistants**, then **CREATE CHAT WITH DATA ASSISTANT**.\n- Select your project.\n- Upload the `.export` metadata file.\n- Provide the assistant's Name, Description, and optionally an Icon.\n- Click **CONFIRM </B>to save.\n\n### Step 3: Update Your Assistant\n- From the Assistants page, use the **UPDATE </B>option to edit name, description, icon, or enable/disable the assistant.\n\n### Step 4: Edit Your Assistant Settings\n- Click **CONFIG </B>on the Assistants page.\n- You can:\n  - Add metadata files, start/stop the assistant, and get logs (General tab).\n  - Enable/disable entities and attributes, edit descriptions, define attribute values (Entities tab).\n  - Configure database connection including DBMS type (SQL Server, MySQL, PostgreSQL, Oracle, SAP Hana), server, database, port, authentication (DataStore tab).\n  - Configure included metadata in prompts and system assistant versions (Settings tab).\n\n### Important Prerequisites for Database Connection:\n- Database must be accessible from Globant Enterprise AI (network or Internet access).\n- Relevant ADO.NET drivers must be installed for the selected DBMS.\n\n### Step 5: Test Your Assistant\n- You can test via Chat API or from the Backoffice Playground interface by selecting your assistant and typing natural language queries.\n\nFor full guides, including images and more detailed explanations, refer to the official documentation here:  \n[How to create a Chat with Data Assistant](https://wiki.genexus.com/enterprise-ai/wiki?170) and related sections.\n\nThis summary is based on detailed instructions and screenshots in the documentation to assist you in setting up and interacting with the Chat with Data Assistant effectively.  \n\n[More info here](https://wiki.genexus.com/enterprise-ai/wiki?170) and the general overview [Chat with Data Assistant](https://wiki.genexus.com/enterprise-ai/wiki?159) and [Use Guidelines](https://wiki.genexus.com/enterprise-ai/wiki?177).",
      "evaluationResultRowStartDate": "2025-05-12T16:59:59",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "6559"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "#How to create a Flow with internet search This is a step-by-step guide to create a conversational assistant with internet search using [[321]]. This Flow manages two assistants: a Chat Assistant that answers questions using internet search results, and another Chat Assistant that rephrases follow-up questions (based on the conversation history) as standalone questions. In addition, it includes a Search API component. ## Step 1: Configuring the Work Environment To get started, first, make sure you have access to the [[42]] and are familiar with the [[371|interface for creating Flows]]. ## Step 2: Creating Chat Assistants In this step, two [[566##Chat Assistant creation|Chat Assistants]] are created. They are called 'Response from Search' and 'Follow-up Rephrase'. ### 'Response from Search' Chat Assistant This Chat Assistant generates answers for the end user based solely on the results of an internet search, without using prior knowledge or inventing information. It takes the provided search results and uses them to answer the end user's question in the best possible way. If necessary, the answer can include rich text in HTML format, but Markdown should not be used to format the answer. **LLM Settings:** - AI Provider: Google VertexAI\n- Model: gemini-1.5-pro\n- Temperature: 0.10\n- Max Output Tokens: 8192 **Prompt:** ```\nINSTRUCTIONS\n---\nYour task is to generate a response for the user based on the results of an internet search.\nDO NOT USE PRIOR KNOWLEDGE AND DO NOT MAKE UP INFORMATION USE THE SEARCH RESULTS.\nGiven the following results from an internet search and NOT PRIOR KNOWLEDGE answer the user question the best you can. SEARCH RESULTS\n---\n{searchResults} RESPONSE FORMAT\n---\nFor the response you can generate RICH TEXT USING HTML if it is needed.\nDO NOT USE MARKDOWN TO FORMAT YOUR RESPONSE.\nFOR RICH TEXT DO NOT USE MARKDOWN USE HTML INSTEAD.\n``` ### 'Follow-up Rephrase' Chat Assistant This Chat Assistant rephrases a follow-up question based on the conversation history, ensuring it becomes a standalone question if needed. If the follow-up question already works as a standalone question, it simply repeats the question without any changes. **LLM Settings:** - AI Provider: Google VertexAI\n- Model: gemini-1.5-pro\n- Temperature: 0.10\n- Max Output Tokens: 8192 **Prompt:** ```\nGiven the conversation history and not prior knowledge, and a follow-up question, rephrase the follow-up question to be a standalone question IF IT IS NEEDED. If the question works as a standalone question, just repeat the question. RESPONSE ONLY WITH THE STANDALONE QUESTION AND NOTHING MORE; DO NOT INCLUDE ANY OTHER TEXT.\n``` ## Step 3: Creating the Flow Follow the steps described in [[322]] to create the Flow called 'Search Assistant' with the following structure: [[689]] ## Step 4: Implementing the API component To add the API component, go to the [[494|Interactions section]] and, below INTEGRATIONS in the Flow menu, drag the component between the Assistants nodes. [[690]] ## Step 5: Setting Up the Search API Go to [https://developers.google.com/custom-search/v1/introduction?hl=es-419|Google Search API documentation] and click on Get API key button. [[691]] In the pop-up window that opens, select or create a project from the Select or create project menu. [[692]] Select the Yes option to agree that the use of the services and related APIs is subject to compliance with the applicable Terms of Service. Then click on NEXT to continue with the setup process. In the next window, click on the SHOW KEY button to reveal the API Key. Copy this API key, as it will be required to configure the API node in your flow. [[693]] ## Step 6: Creating a Programmable Search Engine Go to [https://programmablesearchengine.google.com/controlpanel/all|Programmable Search Engine] and click on the Add button. [[694]] In the pop-up window that opens, fill in the necessary fields to create the search engine: [[695]] - **Name of your search engine:** In the \"Search engine name\" field, enter a name for the search engine, such as Search Assistant.\n- **What to search for:** Select the \"Search the entire web\" option to have the engine search the entire web. If you prefer to search only specific sites, you can select the \"Search specific sites or pages\" option and add the necessary URLs.\n- **Search settings:** If you wish, you can adjust additional settings such as image search or SafeSearch filter.\n- **Captcha:** Select the \"I am not a robot\" checkbox.\n- **Create:** At the end of the window, click on the Create button to finalize the creation of the search engine. In the window that appears, you must click on the Customize button. [[696]] This will take you to the Overview section of the search engine you have just created. In this section, you should find and copy the Search engine ID, which is a unique value associated with your search engine. This ID is required to configure the API node in your flow. [[697]] ## Step 7: Configuring the API Node With the Search engine ID and API key, you can now configure the API node. First, you must click on the Edit button that appears when you position the mouse over the API node. [[698]] Once you have clicked on Edit, the State Configuration menu will be displayed on the left side. [[699]] Within this menu, in the URL section, you must configure the URL with the following value: ```\nhttps://www.googleapis.com/customsearch/v1?key=<API key>&cx=<Search engine ID>&q=<query>\n``` Be sure to replace `<API key>` and `<Search engine ID>` with the previously obtained values. In the API node, go to the **Parameters** section and click on **+ Add new** to add a new parameter. Then, configure the parameter with the following details: - **Parameter name:** q.\n- **Value:** lastResponse (the value obtained from the previous response). Next, in the State Configuration section, you must make sure to mark q as **Query params**. [[700]] In the **Responses** section, make sure to map the items property to the **searchResults** variable. If searchResults is not available, you can click on Add new variable to create it. [[701]] ## Step 8: Configuring the Assistant Node After you have configured the API node, go to the Assistant node, which is located just below the API node in your flow. In this node, find the Variable section and change the input variable to searchResults. [[702]] ## Step 9: Testing the Flow Finally, save the changes and test the flow. You can use the test window or the Playground to make sure everything works correctly, verifying that the Assistant responds properly using the search results. | [[704]] | [[705]] | {{{12}}}{{{17}}}##MARKDOWN##",
          "evaluationResultRowContextDocId": "526b3969-e9ac-4188-a986-6b0013b11c4d",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "09c7988c-22e3-49bf-978b-fccd79ef71be",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?688",
          "evaluationResultRowContextName": "688",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "# User's Guide for Globant Enterprise AI Flow Builder Globant Enterprise AI Flow Builder provides a visual tool that allows building and configuring conversational flows. ![Image](./images/372.png) ## Top Toolbar The Top Toolbar in the Globant Enterprise AI Flow Builder offers several options that allow you to manage and navigate between flows, as well as access key functionalities when building conversational flows. ![Image](./images/373.png) ### Current Flow Name This drop-down menu displays the name of the flow you are working with. In this case, the current flow is Documentation Assistant. From this menu, you can: - Quickly access other flows created.\n- Return to the main Flows window by clicking on the \"See All\" option. ### Language Change It allows you to change the language of the Flow Builder. In this case, it appears as \"En\" to indicate English. This option is available only if several languages have been selected in the [Select languages](322.html) option when creating the flow. ### Profile It displays the name of the project you are working on and provides the option to log out. ### Go to Demo Page It opens a new window that allows you to test the flow in real time, making it easier to verify the assistant's behavior and responses before implementing it. ### Open Logs Console It opens the Open Logs Console option, where you can access real-time logs of the assistant execution. This is especially useful for debugging tasks and detailed analysis of flow behavior. ### Open Testing Assistant It opens an interactive chat to test the flow in real time, helping to verify how the assistant responds under different scenarios before final implementation. ## Side Navigation Menu On the left side of the window is the Side Navigation Menu. This menu is a collapsible section that is hidden by clicking on the 'Collapse' option at the bottom. It provides quick access to various options: ![Image](./images/374.png) ### OVERVIEW Provides key information about the flow you are managing. Here you can see: ![Image](./images/375.png) - The name of the flow. In this case, \u201cDocumentation Assistant\u201d.\n- The Bot id and Project id, which are unique identifiers of the related bot and project, useful for internal reference and management.\n- The flow description, which is the text entered at the time of creation to detail its purpose or function. In addition, the section includes options to edit the flow name and add members. ### BUILD This section groups the tools that allow you to build and manage the flows and variables used in the assistants: - Flows: From here you can view, create, edit or manage conversational flows.\n- Variables: Allows you to view and manage the global and local variables that will be used in the flows. ### LOGS/DIAGNOSTICS This section is dedicated to the analysis and diagnosis of the assistant's behavior, providing access to interaction logs: - [Conversations history](378.html): Displays the history of all conversations between users and the assistant. It is useful to review how the interactions went and to make adjustments if necessary. ### ANALYTICS This section provides tools to analyze the performance and effectiveness of the assistant using different metrics: - [User Metrics](380.html): Presents metrics related to users, such as number of interactions and frequency of use.\n- [Message Metrics](382.html): Displays metrics related to the messages exchanged, providing insight into the effectiveness of the assistant's responses. ### CONFIGURATION This section allows you to adjust the general settings of the assistant and its environment: - [Configuration](384.html): Accesses the global system settings, where you can find the options Application keys to manage authentication keys, Integrations to configure connections with other platforms, Bot versioning to manage bot versions, and Danger Zone to export, import or restore bot configurations. ## Flows menu This menu has two main sections: Flows and Interactions. ![Image](./images/376.png) 1. [Flows Section](389.html): This section lists all available flows and appears by default when selecting a flow from the welcome screen. However, it is only displayed when the Flows option is selected in the Side Navigation Menu (the leftmost menu on the screen). If another option is selected in the Side Navigation Menu, this section is hidden or other options relevant to the selection are displayed.\n2. Interactions section: This area provides a list of components that can be used to build the flow. Components include user input, conditions, variables, language changes, history restart, among others. ## Flow Creation Area This section shows the flow under construction, allowing you to design the interaction sequence of the different assistants. ![Image](./images/377.png) In the upper right part of the area, you will find several key tools: - Save: This button saves the progress made when building the flow.\n- Run Flow: It allows running the flow to test how it would work in a test environment.\n- Code View (icon {}): This icon allows you to switch to a code view, where you can view and edit the flow in code format. To navigate the flow, you can click and drag the diagram to the creation area. Zooming is controlled with the mouse scroll wheel or pinch gesture on the touchpad, adjusting the zoom level without affecting the vertical position within the flow.",
          "evaluationResultRowContextDocId": "2708af94-91ad-4cc5-8095-f02fe76415ba",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "1b64c49a-ffdd-45ca-9c86-9a9c46f1beb7",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?371",
          "evaluationResultRowContextName": "371",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "is defined; for example, \"lastUserInput\". This variable contains the user's most recent input and will be used as the basis for the query on the selected documents or tags. Similarly, in the Output section, there is also a Variable field that specifies the variable in which the output generated by that node will be stored. In this case, we are interested in saving the RAG Assistant\u2019s response in a new variable called lastResponse. Therefore, the Use as response option is disabled. Clicking on the pencil icon to the right of the node opens the \"State Configuration\" menu where you can modify the following items: - **Acceptable confidence level**: The default value is 0.2, which indicates the level of confidence in the match of the chunks that the RAG obtains after searching the vector database. Only responses that meet or exceed this confidence level will be considered valid.\n\n- **Include conversation history**: Enabled by default. It ensures that the conversation history is included in the context of the interaction, allowing the assistant to take previous messages into account when generating responses.\n- **Include context variables**: Enabled by default. It allows context variables stored during the conversation to be sent to the assistants so that they can be used within the prompts.\n- **Show sources**: Enabled by default. It allows the assistant to show the sources of the information used to generate the response, which helps to provide transparency and to validate the accuracy of the response.\n- **Handle empty response**: Disabled by default. It handles situations where the assistant is unable to generate a valid response. When enabled, it opens a new flow branch (as shown in the image), allowing you to configure specific actions to follow in case no response is available. In this new branch, you can add any of the components available in the Interactions menu on the left to continue the interaction as needed. ![Handle Empty Response](./images/337.png) In this case, if the RAG Assistant finds the answer in the documents, you want the flow to call an assistant to translate the response to Japanese and store it in the same input variable (lastResponse). If no answer is found, the flow goes to a second RAG Assistant configured with documents used in Globant Enterprise AI courses. The variable that goes into the RAG Assistant is lastUserInput, which contains the user's last input. The response generated by this RAG Assistant is stored in the lastResponse variable. ![RAG Assistant Configuration](./images/338.png) ### Go To Node Finally, the Go To node is used to connect different flows within the interaction. This node allows you to select the flow to which you want to redirect the end user based on their current interaction. ![Go To Node](./images/339.png) Clicking on the blank bar (corresponding to the drop-down menu) displays a list of available flows: ![Available Flows](./images/340.png) Here you can select and define the flow to which you want to redirect the user. To review or modify any of these flows, you can access the Flows menu on the left side of the screen. In addition, \"Pick from canvas\" allows you to select a node directly from the canvas, facilitating navigation and connection between different parts of the Flow. The 'Go' icon next to the node allows you to navigate directly to the node to which it is connected. In this case, there are two 'Go To' nodes: - The first 'Go To': Returns to the 'User Input' node. Since the previous node (which translates to Japanese) does not store the response in a new variable and simply redirects to the 'User Input', the response received by the end user is the answer to their question translated to Japanese.\n- The second 'Go To': Sends the response to the 'Assistant' node, which is responsible for translating the generated response to Japanese using the lastResponse variable. This variable contains the response obtained from either of the two RAG Assistants previously executed in the flow. In this way, the flow ensures that the end user receives the response in Japanese, regardless of whether the response was generated by the first RAG Assistant or the second. ![Flow Diagram](./images/341.png) ## Step 3: Test flow Once you have configured the flow, make sure to click on the 'Save' button located at the top left of the screen. To test the flow you have created, you have three options available from the same design window: ![Test Flow Options](./images/342.png) 1. Click on the arrow icon (similar to the play button) to run the flow ('Run Flow').\n2. Click on the message icon to open the testing assistant (\u201cOpen Testing Assistant\u201d). ![Testing Assistant](./images/343.png)\n3. Click on the icon to open in a new window (the box with an arrow) to access the demo page (\u201cGo to Demo Page\u201d). ![Demo Page](./images/344.png) The first two options will open a window on the right side of the screen, while the third option will open a new window in the browser.\n\n# How to create a Flow This is a step-by-step guide to create and test an Assistants [Flow](321.html). ## Step 1: Create New First, log in to the [Globant Enterprise AI Backoffice](42.html). In the Project Dynamic combo box, select the project you want to work with (in this case, Default is used). Next, on the left side of the screen, you will find the backoffice menu. In this menu, click on Flows. ![Flow Menu](./images/324.png) By clicking on Flows in the Globant Enterprise AI Backoffice, a new window opens in the browser with the Flow Builder, where the Flows associated with the selected Globant Enterprise AI project are created and managed. When accessing Globant Enterprise AI\u2019s Flow Builder for the first time, a welcome screen like the one shown in the image below is displayed. From this screen, you can start creating a new flow by clicking on the \"Create\" button. ![Welcome Screen](./images/325.png) When you click on \"Create\", a pop-up window opens in which you must fill in the following information: 1. **Name**: Descriptive name for the flow, allowing it to be easily identified.\n2. **Description**: This field is optional, but it is recommended to add a brief description of the flow to clarify its purpose or content.\n3. **Type**: Allows you to select the type of flow to create, with the options \"Chat Flow\" or \"Assistant Flow\". Selecting one of these types will determine specific configurations and characteristics of the flow. In this case, Assistant Flow is selected.\n\n4. **Select languages**: Sets the language in which the flow will be configured, and allows you to define the language of the hard-coded messages. Multiple languages can be selected, so the same message can be available in different languages. Once you have completed these fields, you can click on the \"Create\" button. ![Create Flow](./images/326.png) ## Step 2: Manage the Start flow Once you have clicked on \"Create,\" you will return to the welcome screen. There, click on the \"Manage\" button of the newly created flow to add the assistants you want and customize their behavior. ![Manage Flow](./images/327.png) Clicking on the \"Manage\" button displays the first flow, which is created automatically. This initial flow, called \"start,\" establishes a basic configuration that you can customize, adding or modifying interactions, messages and assistants according to your needs. ![Start Flow](./images/328.png) ### Start flow The initial flow, called \"start,\" is automatically generated with the ID input.welcome. ![Start Flow ID](./images/329.png) ### Start (Intent: input.welcome) Node The Start (Intent: input.welcome) node marks the beginning of the conversation flow. This node is activated every time the flow is triggered, and its main purpose is to start the flow and, from there, the other nodes that manage the interaction with the user are connected. To rename the start node, go to the left menu, find Flows > start, click on the three dots next to it, select \"Rename Flow\" and, after typing the new name, press Enter to save the change. ![Rename Flow](./images/330.png) ### Message Node The next node, Message, is in charge of sending the welcome message or any other initial message configured. In this case, the message is: \"Hello {name}, how can I help you?\". Here, {name} represents a dynamic variable that is filled with the user's name. The message displayed in this node is customizable, and the text can be formatted by selecting different styles, such as Normal, Heading 1, among others, from the drop-down menu that appears where it says \"Normal\". This allows you to adjust the text style according to your needs. ![Message Node](./images/331.png) On the top bar of the node, you can see several formatting buttons such as bold, italic, underline, among others, to customize the text. In addition, there are icons that allow you to add lists, links and emojis. The last button, which has the shape of a small drawer, allows you to insert dynamic variables in the message. Clicking on this button displays a list of the variables available in the system, and it is also possible to define new custom variables. This Message node can be deleted by clicking on the trash can icon to the right of the node. It is also possible to add a variant by clicking on the \"plus\" (+) button at the top right. ### User Input Node The flow then proceeds to capture the user input through the User Input node, storing it in the lastUserInput variable for later use. ### Assistant Node In the Assistant node, you must configure the assistant to perform specific actions. This node uses the information stored in the lastUserInput variable, which contains the text entered by the end user in the User Input node. ![Assistant Node](./images/332.png) To configure the assistant, click on the \"Select an assistant\" field. By default, \"No assistant selected\" will be displayed. From the drop-down menu, choose the assistant you wish to use. **Note**: The assistants available are those that have been defined in the associated project, which was configured in the first step. The assistants can be any of those defined in Globant Enterprise AI, including [Data Analyst Assistant](886.html), [API Assistant](110.html), [Chat with Data Assistant](159.html). Once selected, the Prompt field will display the message that defines the task of the assistant. This message is read-only and can\u2019t be edited. The assistant's response can be stored in the lastUserInput variable or in a new variable, depending on your needs. By default, the Use as response option is enabled, which causes the assistant's response to be used directly in the lastUserInput variable. If you prefer to store the response in a new variable, disable Use as response. This will enable the Output field, where you can select or create a new variable. ![Assistant Output](./images/333.png) In addition, you can enable the option to read the assistant's response in JSON format. By enabling this option, you will have the ability to map the fields of that JSON to variables previously defined in your configuration. This allows for more precise integration of the assistant's responses in more complex workflows or in systems that require a specific format for data handling. The pencil icon to the right of the node corresponds to the Edit button. By clicking on it, you can enable or disable the Include conversation history and Include context variables options. By default, Include conversation history is enabled. It allows the assistant to access the conversation history to generate more coherent and contextual responses. The Include context variables option is also enabled by default. This option sends to the assistant variables defined in the flow, such as previously captured data or configured values, so that they can be used within prompts. This node can be deleted by clicking on the trash can icon to the right of the node. In this case, the Assistant node is deleted: ![Delete Assistant Node](./images/334.png) ### Insert RAG Assistant or Assistant Node On the left menu, below the Interactions category, you will find the different components that can be added to the flow. In particular, in the GENERATIVE section, you can choose between adding an Assistant or a RAG Assistant. ![Insert RAG Assistant](./images/335.png) In this case, you need to add a RAG Assistant. To do so, click on the RAG Assistant option in the GENERATIVE section. Next, drag the component to the desired location in the flow. In this case, you can insert it between the User input and Go To nodes. When you add a new RAG node, you must click on the \"Select an assistant\" field. By default, the option \"No assistant selected\" will appear. From the drop-down menu, choose the RAG assistant you wish to use. Once you select the RAG assistant, the next field, \"Select a document\", allows you to choose a specific document that will act as the source for responding to the end user's request. If you select a document, the assistant will only use that document as a reference. ![Select Document](./images/336.png) Alternatively, you can use the \"Indicates the documents' tags\" field. Here you can add the tags defined in [Step 2: Upload RAG documents](60.html), which are used to filter the documents. This allows the assistant to search only in the documents that match the indicated tags to respond to the end user's request. In the Input section, a field called Variable is displayed. There, the variable to be used as input for that node",
          "evaluationResultRowContextDocId": "09ebe65a-fb9d-4f45-8146-084558fef786",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "81a64fd6-f657-47d1-af04-f32f2e0da20e",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?322",
          "evaluationResultRowContextName": "322",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "``` ## Step 3: Creating the Flow Follow the steps described in [[322]] to create the Flow called Mall Chat Assistant with the following structure: [[364]] To add variables to the Flow, click on the Variable option in the LOGICAL section of the [[371##Flows menu|Flows Menu]]. Next, drag the component below the User input node into the flow. In this case, a variable has been added: today. **Today variable:** It is defined by clicking on the down arrow inside the variables node and selecting the option + Add new variable. Next, enter the name today and press Enter. Deactivate the Clear option. To set the value of this variable, click on the edit icon to the right of the node. In the left menu, click on the Open in new window icon, and in the window that opens (Value) add `$today`. ## Step 4: Implementing the Derivation Logic Every end user query must be evaluated to determine which assistant is the most appropriate to answer. This is implemented with IF conditions in the flow, where the end user's intentions are compared with the available assistants. To add the Conditional component, go to the Interactions section and, below LOGICAL in the Flow menu, drag the component and drop it between the Assistant and Go to nodes. [[365]] Dragging the Conditional node generates a branch with two paths: one labeled If and one labeled Else. This structure allows you to evaluate a condition and direct the flow according to the result. To add an additional condition, click on the Add condition option to the right of the first node of the branch: [[366]] After clicking on Add condition in the first node of the branch, an additional node of Expression type is added, just like the first one. These Expression nodes allow evaluating different conditional expressions. [[367]] To add the variable to be evaluated, click on the word Variable, located at the bottom of each node. In the first two nodes of Expression type, you can see how each condition evaluates the `orchestrationResponse` variable. Depending on the value of this variable, the flow is directed to different assistants: either to the Movie Ticket Assistant or to the Mall Shops Catalog. If neither of these conditions is met, the flow continues through the Else branch, where by default there is a Go to node. A Message node has been added before the Go to node so that the flow can display a specific message to the end user. Then, the corresponding nodes are added to handle the responses according to the evaluated condition. [[368]] A node of API Assistant type that is configured for the Movie Ticket Assistant and handles queries related to the movie theater in the shopping mall is placed below the first node of Expression type (which checks if `orchestrationResponse` is equal to \"Movie Ticket Assistant\"). A node of RAG Assistant type that is configured for the Mall Shop Catalog Assistant and provides information about the stores in the mall is added below the second node of Expression type (which checks if `orchestrationResponse` is equal to \"Mall Shops Catalog\"). Finally, below each node a Go to is added, which redirects back to the User Input node. Once the assistant has processed and answered the query, this allows the flow to return to the entry point to receive new user interactions, ensuring a continuous and fluid conversation. [[369]] ## Step 5: Testing and Adjusting the Mall Chat Assistant Once you have configured the flow, make sure to click on the 'Save' button located at the top left of the screen. This step is essential to save all changes made to the flow. To test the flow you have created, you have three options available from the same design window: 1. Click on the arrow icon (similar to the play button) to run the flow ('Run Flow').\n2. Click on the message icon to open the testing assistant ('Open Testing Assistant').\n3. Click on the open in new window icon (the box with an arrow) to access the demo page ('Go to Demo Page'). The first two options will open a window on the right side of the screen, while the third option will open a new window in the browser, allowing you to see how the flow runs in a demo environment. During testing, you can run various queries to make sure that the flow responds correctly. For example, you can ask \"Where can I purchase books?\" and the flow should correctly identify the query and direct it to the Mall Shop Catalog, where the end user will be informed about the availability of stores that sell books. Then, you can ask \"What movies can I watch?\" and verify that the flow correctly redirects to the Movie Ticket Assistant to get the list of available movies. If the answer is ambiguous, as in \"I would like to watch the last one\", the orchestrator should use the context of the conversation to identify the last movie mentioned and continue the interaction with the Movie Ticket Assistant. In situations where the query does not correspond to any assistant\u2014such as \"What is Linux?\"\u2014the orchestrator should handle the response using a fallback and provide the information directly from the model, so that the flow covers all possible end user interactions. These tests will allow you to adjust the flow as needed, ensuring that the Mall Chat Assistant runs efficiently and delivers a smooth and accurate experience for end users. {{{12}}}{{{17}}}##MARKDOWN##\n\n#How to create a Flow with Conditional component This is a step-by-step guide to create a Mall Chat Assistant using [[321|Flows]] in Globant Enterprise AI. This Flow manages multiple assistants, which are specialized in different areas such as querying store catalogs, booking movie tickets, and orchestrating conversations. The steps required to set up and test this assistant are described below. ## Step 1: Configuring the Work Environment To get started, first make sure you have access to the [[42]] and are familiar with the [[371|interface for creating Flows]]. This environment will allow you to create, manage, and orchestrate the various assistants that make up the Mall Chat Assistant. ## Step 2: Creating Assistants ### Mall Shop Catalog Assistant This [[44|RAG Assistant]] is designed to provide information about the stores in the mall. The objective is to allow end users to check the availability of products or services in the different stores of the mall. The assistant is configured to answer questions related to the location, products, and opening hours of the stores. It uses a simple JSON document containing the store name and a short description. ### Movie Ticket Assistant This [[110]] allows end users to search the movie listings available in the mall and book tickets. It is based on the integration with an API that simulates the availability of movies in a movie theater. This assistant must be able to handle questions about schedules and reservations. ### Orchestrator Mall Assistant This [[566##Chat Assistant creation|Chat Assistant]] manages and redirects conversations to the appropriate assistant. The orchestrator is responsible for understanding the end user's intent and redirecting the conversation accordingly. It uses a system of predefined prompts to redirect queries to the corresponding assistant. The orchestrator keeps the context of the conversation, so the interaction is fluid and coherent. It defines variables such as user name and date. Orchestrator Mall Assistant Prompt: ```\nAssistant is a large language model.\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions.\nAdditionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\nAssistant is a helpful Mall Customer Service agent; you are kind but you always follow the rules. CONVERSATION CONTEXT ---\nThe user name is: {name} Today is: {today} OTHER ASSISTANTS\n---\nAssistant can ask the user to use other Assistants to look up information that may be helpful in answering the user\u2019s original question. The other Assistants the human can ask are:\n> Mall Shops Catalog: This Assistant provides information about shops inside the mall.\n> Movie Ticket Assistant: This assistant provides information about the movie theater inside the mall. RESPONSE FORMAT\n---\nWhen responding to me, if you want the human to ask another Assistant JUST ANSWER WITH THE ASSISTANT NAME AND NOTHING ELSE.\nFor any other response, you can create an HTML fragment with RICH TEXT if it's needed.",
          "evaluationResultRowContextDocId": "57a2c916-1a55-4579-abb9-419226e4e2c1",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "f9ec5f42-d72c-46d2-9262-a90d2a9a3edf",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?363",
          "evaluationResultRowContextName": "363",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.8,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.8,
          "systemMetricId": "8fd20b5c-68a7-42fa-afad-45d17350172f"
        }
      ],
      "dataSetRowId": "6af44a97-3be0-4851-bc55-7b219296b0b7",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0044236400",
      "evaluationResultRowEndDate": "2025-05-12T17:00:28",
      "evaluationResultRowOutput": "To use a Flow Assistant in Globant Enterprise AI, you need to create and configure a flow within the Flow Builder. Here's a concise guide on how to use it based on the documentation:\n\n### Steps to Use a Flow Assistant\n\n1. **Create or Open a Flow:**\n   - Log in to the Globant Enterprise AI Backoffice.\n   - Navigate to the *Flows* section in the menu.\n   - Create a new flow by clicking on the \"Create\" button, then filling in details such as Name, Description, Type (choose Assistant Flow), and Languages.\n   - After creation, click \"Manage\" on the flow to open the Flow Builder environment where you can define the flow steps and interactions.  \n   ([See more details here](https://wiki.genexus.com/enterprise-ai/wiki?322))\n\n2. **Add Interaction Nodes:**\n   - Use the components on the left side under the Interactions section to add nodes such as **User Input**, **Message**, **Assistant**, **RAG Assistant**, and **Go To </B>nodes, depending on how you want to design the conversation.\n   - For example, the **User Input </B>node captures the user's input and stores it in a variable like `lastUserInput`.\n   - The **Assistant </B>node can be configured to invoke a specific assistant, e.g., a chatbot or specialized assistant, and the input and output variables are configurable.\n   ([More about nodes configuration](https://wiki.genexus.com/enterprise-ai/wiki?322))\n\n3. **Configure the Assistant Node:**\n   - Click the Assistant node and select the assistant from the dropdown.\n   - Define the input variable for the assistant, often the variable holding the user input (`lastUserInput`).\n   - Decide whether to store the assistant\u2019s output in the same variable or a new one.\n   - Advanced options include enabling conversation history and context variables to improve assistant responses.\n   ([Details on Assistant node configuration](https://wiki.genexus.com/enterprise-ai/wiki?322))\n\n4. **Use Conditional Logic if Needed:**\n   - You can add **Conditional </B>components to direct user queries to different assistants based on logic or variables.\n   - This enables orchestration between multiple assistants within one flow.\n   ([How to use Conditional nodes](https://wiki.genexus.com/enterprise-ai/wiki?363))\n\n5. **Add Go To Nodes for Navigation:**\n   - Use the **Go To </B>node to control the flow direction after a node\u2019s execution, e.g., returning to \"User Input\" for another question.\n   - This keeps the conversation ongoing and smooth.\n   ([Go To node details](https://wiki.genexus.com/enterprise-ai/wiki?322))\n\n6. **Save and Test the Flow:**\n   - Always save changes using the \"Save\" button.\n   - Test your flow using:\n     - The **Run Flow </B>option to simulate the flow.\n     - The **Testing Assistant </B>in a side panel for interactive testing.\n     - The **Demo Page </B>in a new window for full visualization.\n   ([Testing options](https://wiki.genexus.com/enterprise-ai/wiki?322))\n\n### Summary\nUsing a flow assistant involves setting up a flow with nodes representing conversation steps and assistants, configuring input and output variables, applying logic to direct queries, and testing the behavior thoroughly before deploying.\n\nFor more in-depth step-by-step instructions, you can refer to the official documentation on creating and managing flows:\n[How to create a Flow](https://wiki.genexus.com/enterprise-ai/wiki?322)  \n[How to create a Flow with Conditional component](https://wiki.genexus.com/enterprise-ai/wiki?363)  \n[User's Guide for Globant Enterprise AI Flow Builder](https://wiki.genexus.com/enterprise-ai/wiki?371)\n\nIf you want details about a specific part of using a flow assistant, feel free to ask!",
      "evaluationResultRowStartDate": "2025-05-12T17:00:15",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "8816"
    },
    {
      "contexts": [
        {
          "evaluationResultRowContextChunk": "# How to create a Data Analyst Assistant Here is a step-by-step guide to use [Data Analyst Assistant 2.0](886.html), upload a document, and start chatting with the assistant from the playground. ## [Step 1: Create New](#Step+1%3A+Create+New) First, enter the [Globant Enterprise AI Backoffice](42.html). On the left side of the screen, you can find the backoffice menu. In this menu, click on Assistants. Then click on the CREATE DATA ANALYST ASSISTANT. Next, in the Project Dynamic Combo Box, select the project you want to work with (in this case, the Default one is used). ![Image](./images/913.png) ## [Step 2: Add files](#Step+2%3A+Add+files) Upload a dataset file by clicking on the +Add files... button. ![Image](./images/905.png) > **Note**: Use lowercase letters and underscores in your filenames (e.g., `this_notation.csv` instead of `ThisNotation.csv`). ## [Step 3: Define Metadata (JSON)](#Step+3%3A+Define+Metadata+%28JSON%29) Metadata is descriptive information about the data contained in a dataset. It is used to provide context and better understand the available data. Metadata usually includes details such as the name of the dataset, a description of its contents, and the structure of its columns (including data types and possible values). This metadata is used to define how the dataset should be processed and to clarify the dataset to be analyzed. A basic metadata structure is generated based on the datasets uploaded. It\u2019s important to note that this is a placeholder that you must complete with appropriate descriptions and additional considerations (if required). This is necessary for the assistant to know how to interpret the dataset while answering questions. To complete the Metadata (JSON) correctly, you must define a JSON with the following structure: ```json\n{ \"da_version\": \"2.0\", \"dataset_name\": { \"dataframe name\": \"dataset_name\", \"description\": \"contains data regarding different types of revenues, their types, dates, and associated details.\", \"column explanations\": { \"colum1\": \"description column 1. dtype: dtype1\", \"colum2\": \"description column 2. dtype: dtype2\", ... \"columN\": \"description column N. dtype: dtypeN\" }, \"considerations\": { \"coder\": [ \"Describe you consideration here\", ...... \"Example of consideration: use column1 to determine the type\" ], \"interpreter\": [ \"Example 1\", \"Example N\" ], \"dispatcher\": [ \"Describe your dispatcher considerations here.\", \"......\", \"Example of dispatcher consideration: Use the glossary to correctly interpret user-provided terms.\" ], \"thinker\": [ \"Describe your thinker considerations here.\", \"......\", \"Example of thinker consideration: Round monetary values to two decimal places for readability.\" ] } }\n}\n``` Where: - `\"dataframe name\"` must contain the name of the dataset and correspond to the name of one of the .csv files to be uploaded.\n- `\"description\"` must provide a description of the dataset, including the data types of its columns and possible values.\n- `\"column explanations\"` must include a description of each dataset column, specifying its purpose and the type of data it contains. - `\"colum1\", ..., \"columN\"`: Exact names of the columns of the .csv files to be loaded. - These names must follow specific rules: - The number of columns in the .csv files must match the number of columns provided in the metadata. - In addition, it is important to avoid duplication of column names. - Column names must: - Start with a letter. - Use only lowercase letters. - Avoid blanks. - Use underscores (_) as the only separator between words (e.g., example_name). - Exclude accents and special characters. - There cannot be a column named \"id\" within the dataset, as it is already defined by default in the database. - `dtype`: must be one of the following types: - string - float - integer - datetime: must be one of the following formats: - '%Y-%m-%d' - '%d-%m-%Y' - '%Y/%m/%d' - '%d/%m/%Y' - '%m/%d/%Y' - '%Y/%d/%m' - date - bool - biginteger\n- `\"considerations\"` must detail specific considerations for the dispatcher and the thinker. These sections are optional. - `\"coder\"`: It generates the necessary code to process the data. In addition, it uses the Metadata information to guide the process of extracting relevant information from the data. - The coder may include considerations that address consistency in data formats, accuracy and completeness of records, validation of data with external sources, and assessment of the importance of each piece of data during analysis. - `\"interpreter\"`: Its function is to produce the final answer with the information obtained from the data processing performed by the coder. It analyzes the processing results to better understand the data and provide relevant answers. - It may include considerations indicating the understanding of technical terms and abbreviations used in the data, as well as specifying that responses should be returned in the language preferred by the end user or according to the context of use. - `\"dispatcher\"`: Provides guidance for the Dispatcher on how to interpret the data. This replaces the older \"coder\" and \"interpreter\" considerations. For example, you might include considerations that address data consistency, accuracy, completeness, validation against external sources, and the importance of each data point during analysis. This helps the Dispatcher understand the nuances of the data and formulate the correct SQL query. - `\"thinker\"`: Provides guidance for the Thinker on how to process the data and formulate the final answer. This also replaces the older \"coder\" and \"interpreter\" considerations. For example, you might include considerations that clarify technical terms and abbreviations, or specify that responses should be returned in a specific language. This helps the Thinker generate the appropriate Pandas code and present the results in a user-friendly way. > **Note**: The \"coder\" and \"interpreter\" sections are part of [Data Analyst Assistant 1.0](89.html) and will soon be deprecated. ## [Step 4: Define Glossary (JSON)](#Step+4%3A+Define+Glossary+%28JSON%29) Glossary is a JSON that contains a list of terms used in the end user's company or domain, together with their respective definitions. These terms and definitions are relevant for the LLM to understand the questions and provide more accurate answers. For example, you can include abbreviations that are often used in the company and their meanings for better understanding in a format such as the following: ```json\n{ \"glossary\": { \"term1\": \"Definition of term 1\", \"term2\": \"Definition of term 2\", \"term3\": \"Definition of term 3\", \"abbr1\": \"Abbreviation 1 - Meaning of abbreviation 1\", \"abbr2\": \"Abbreviation 2 - Meaning of abbreviation 2\", \"abbr3\": \"Abbreviation 3 - Meaning of abbreviation 3\" }\n}\n\n``` Then click on SAVE to create the assistant. ## [Step 5: Fill in the assistant's details](#Step+5%3A+Fill+in+the+assistant%27s+details) After you click on the SAVE button, a window will appear to enter a Name, Description, and Icon: ![Image](./images/94.png) Next, click on the CONFIRM button. ## [Step 6: Updated Data Analyst Assistants](#Step+6%3A+Updated+Data+Analyst+Assistants) After creating the Data Analyst Assistant, you return to the Assistants page where you can see the current assistant creation status. This status is related to the process of loading the datasets with the Metadata and Glossary information. The possible statuses are FAILED, COMPLETED, PROCESSING (with a progress percentage). In this case, it is being processed with a progress of 11.1%. ![Image](./images/914.png) Once the upload status is complete, you can click on UPDATE, and view the version identifier with which it was saved. In addition, you can change the name and description, configure it as enabled or disabled, or add an icon if you wish. ![Image](./images/97.png) ## [Step 7: Set Data Analyst Assistant](#Step+7%3A+Set+Data+Analyst+Assistant) To edit the Data Analyst Assistants, go to the Assistants home page and click on CONFIG. ![Image](./images/915.png) A window similar to the one below will open, where you will have the option to make one of the following three edits in the General section: - Extend datasets\n- Replace datasets\n- Update prompts\n- Clean cache\n- Clean data ![Image](./images/99.png) ### [Extend datasets](#Extend+datasets) This option allows you to add records to existing datasets in any Data Analyst Assistant. Simply click on + Add files... and then click on the SAVE IN CURRENT REVIEW button. ![Image](./images/100.png) This expands the information available without affecting the original structure of the dataset. When using this function, the files selected for integration must meet the same conditions required when initially defining the assistant, such as having the same number of columns and file names. It addition, the files used for the extension must not contain the same rows that are already loaded in the assistant, as this could generate duplicate keys and cause problems in the execution of the queries. Note that in Extend datasets mode it is not possible to make changes to the metadata or the glossary. This function focuses exclusively on extending existing datasets without altering other aspects of the assistant. ### [Replace datasets](#Replace+datasets) Selecting the Replace datasets option will display a window like the one below: ![Image](./images/101.png) This option allows you to completely replace the data in an existing dataset, as well as modify the associated metadata and glossary. This is useful when you need to completely update the information or make significant changes to the structure and description of the dataset. Selecting this option overwrites all previous data with the new data provided and updates the metadata and glossary as necessary. Note that it is mandatory to select one or more dataset files to be loaded. In addition, already loaded dataset information will be deleted in order to process and save the new one. ### [Update prompts](#Update+prompts) When you select Update prompts, you will see a window similar to the one below, where you will find a list of the default prompts. ![Image](./images/102.png) This function allows you to consult each of the notices individually by clicking on UPDATE. This action requires that you have the Administrator, ProjectRole, or OrganizationRole role. > **Note**: You cannot directly edit prompts. Instead, configure all customizable aspects of the assistant using the `metadata` and `glossary`. When you click on UPDATE, the current contents of the prompt will be displayed. ![Image](./images/103.png) By clicking on the CONFIRM button, you will be returned to the prompts list. By clicking on the CANCEL button, you will also return to the prompts list without making any changes. ## [Step 8: Test your Data Analyst Assistant](#Step+8%3A+Test+your+Data+Analyst+Assistant) Finally, you can test your Data Analyst Assistant by clicking on Playground on the left side menu of the Backoffice window: ![Image](./images/916.png) It is also possible to use the [Chat API](34.html).",
          "evaluationResultRowContextDocId": "3ff322ee-be1c-413e-8e79-9ca353fcef12",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "1f1274f0-3412-486e-ae12-88705b49ca94",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?105",
          "evaluationResultRowContextName": "105",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "- New Average Request Time metric added in the Project Dashboard. ### January 8th - The option formerly known as 'Search Documents' has been improved and renamed to RAG Assistant (Retrieval Augmented Generation) to provide an optimized experience when searching and generating information. #### Frontend - Feedback is provided during conversations with RAG Assistants, indicating where you are in the process.\n- 'Response streaming' support for RAG Assistants.\n- Settings are hidden when selecting an assistant, except when 'Chat with LLMs' is selected. ## 2023 ### December 19th - Fixed: Too Many Redirects when accessing Playground using a browser in Spanish language. ### December 6th - New backoffice design.\n- Access to the Playground from the backoffice to chat with the assistants defined in the project.\n- Upload images for analysis with GPT-4 Vision.\n- Google Analytics support at the frontend.\n- Keep a conversation thread when chatting with documents.\n- An email notification is sent automatically when a new member is invited to join the organization or project. ### November 28th - First version officially released!! ### November 6th - The following OpenAI models are supported: GTP-4 Turbo (gpt-4-1106-preview), GPT-3.5 Turbo (gpt-3.5-turbo-1106), and GPT-4 Vision (gpt-4-vision-preview). ### October 18th - [Manage quotas per project](71.html): It is possible to set quota limits to control project expenses or usage. ### October 11th - [AI-Driven Load Balancing](32.html): The platform automatically manages the Load Balancing process when you work with generative AI providers, efficiently addressing the limits imposed by LLM platforms.\n\n- In the [Organization API](22.html), the ability to set and manage usage limits on projects through the [POST /project](22.html) and [GET /project/{id}](22.html) endpoints has been added.\n- [Quota Limit](71.html) now includes improvements such as highlighting the active quota in green, offering options to cancel active quotas, among others. ### October 17th - Rebranding to Globant Enteprise AI\n- Improvements in RAG - New option not to [compact the conversation history](297.html) - Support for Redis, MongoDB Atlas and PG Vector [vectorstores](267.html) - New endpoint to get an embedding. Check details here: [Embeddings API](422.html)\n- Playground improvements\n- File management improvements\n- New [LLMs supported](200.html) - NVIDIA provider with new models supported - nvidia.nemotron-mini-4b-instruct - meta.llama-3.1-8b-instruct - meta.llama-3.1-70b-instruct - meta.llama-3.1-405b-instruct - meta.llama-3.2-3b-instruct - Groq provider supported - groq/llama-3.1-70b-versatile - groq/llama-3.2-11b-vision-preview - groq/llama-3.2-3b-previewgroq/llama-3.2-1b-preview\n- New [embeddings models](447.html) added - Vertex AI: - vertex_ai/textembedding-gecko - vertex_ai/text-embedding-004 - vertex_ai/textembedding-gecko-multilingual - Nvidia: - nvidia/nvclip - nvidia/nv-embed-v1 - nvidia/baai.bge-m3 - nvidia/snowflake.arctic-embed-l - nvidia/nv-embedqa-mistral-7b-v2 - nvidia/embed-qa-4 - nvidia/nv-embedqa-e5-v5 ### September 25th - [Support for file](346.html) processing with prompt-based assistants. This will enable many scenarios, such as uploading documents and summarizing, extracting, and checking information, etc. Also, depending on the model used by the assistant, it will be able to process audio, video, or images.\n- [Support for multi-modal](345.html) LLMs allow processing docs, audio, video, and images in models like GPT-4o or Gemini Pro.\n- Chat with data assistants - The model used to build the queries was updated with GPT-4o, which improves the quality of the generated query. - Configure the query builder server by organization and project. This means you can connect with different DBMS from each project when building Chat with data assistants. - Show an explanation of how the query was built.\n- New [Playground Interface](349.html) design - New design - Upload documents from the front end to chat with them.\n- [Flows builder](321.html) - There will be two types of Flows, one more oriented to build a conversational UI and the other to build assistant flows. Access to these flows will only be available through [Chat API](34.html) or through the [channels](384.html) offered by Flows.\n- New models hosted in AWS Bedrock added: - Amazon Titan Express v1 - Amazon Titan Lite v1 - Anthropic Claude 3 Haiku - Anthropic Claude 3 Sonnet - Anthropic Claude 3.5 Sonnet - Cohere Command - Meta Llama 3 8B - Meta LLama 3 70B\n- It is now possible to provide clear guidance on the assistant's capabilities, allowing you to add information such as descriptions, features, and example prompts. This configuration can be done from the Backoffice, [Start Page](408.html), or WelcomeData section of the [Assistant API](35.html) and [RAG Assistants API](29.html) endpoints.\n- RAG Assistants - New endpoint in [RAG Assistants API](29.html): using [DELETE/profile/{name}/documents](29.html), all documents associated with the specified RAG Assistant are deleted. ### August 9th - Support of new models - GPT-4o mini\n- RAG Assistants - New option called [CLEANUP](60.html) allows to delete the documents associated to a RAG Assistant. - When creating a new assistant, the following defaults are updated: - [Chunk Count](69.html) updated from 2 to 5. - [Model Name](68.html) from gpt-3.5-turbo to gpt-4o-mini.\n- Data Analyst Assistant - New [Conversational Module](291.html) for Data Analyst Assistant.\n- Considerations - **Important**: [max_tokens integer below minimum value](290.html) error may occur if you are using the _max_tokens_ value as -1; those Assistants must be updated to use a correct positive value. ### July 4th - Permissions by assistants in role-based access control. A new [role and permissions management system](242.html) has been implemented to provide greater flexibility and control. Please read [Migration to the new Roles and Permissions Management System](264.html).\n- Dashboard performance optimizations. Please read [Using the Request Summary Table for Improved Dashboard Performance](273.html).\n- RAG Improvements - New option to customize the [context](205.html). - Support for [Max Marginal Relevance](206.html) - Support for _similarity_hybrid_ and _semantic_hybrid_ for [Azure AISearch](207.html) - Advanced options for SelfQuery retriever prompt case. See [Self Query Use Case](150.html) - Support for Parent Document retrieval option. See [Parent Document Retrieval Strategy](204.html) ### June 10th - [Enterprise AI Proxy](19.html) is deprecated. Use [Chat API](34.html) instead.\n- Support for new LLMs - OpenAI new model GPT-4o - Models in Google Vertex - Gemini 1.0 Pro - Gemini 1.5 Flash preview-0514 - Gemini 1.5 Pro preview-0514 - Claude 3 Haiku - Claude 3 Opus - Claude 3 Sonnet\n- RAG Improvements - New option to initialize RAG Assistant based on another when creating a new RAG Assistant. - New option to export document list in View Documents over a RAG Assistant. - Added filter options when browsing Documents. - SelfQuery RAG retriever partial support for a [customized Prompt](150.html). - Support for text-embedding-004 in Google models to generate the embeddings.\n- Deprecated [Assistant API](35.html) endpoints. - /assistant/text/begin - /assistant/text\n- Support to deploy in Google Cloud Platform. ### May 8th - New [Chat with Data Assistant](159.html).\n- New [Ingestion SDK](172.html) to automate document ingestion in RAG assistants.\n- New models hosted in NVIDIA platform supported. See [Supported Chat Models](200.html) for more details.\n- New option to export information about projects and members available for the organization administrator.\n- New API to extend dataset for [Data Analyst Assistant 1.0](89.html).\n- New filter by user email in Requests.\n- Update default to use _text-embedding-3-small_ OpenAI Embeddings for new RAG assistants.\n- Support for gemini-1.5-pro-preview-0409 model added. ### April 3rd - New [API Assistant](110.html).\n- New [Data Analyst Assistant 1.0](89.html).\n- The user Dashboard displays the email to identify the most active user or the one who generates the most costs. ### March 11th - GeneXus Identity Provider is implemented, expanding the login options in the [Backoffice](42.html) of the production environment. This allows for login not only with Google but also with Apple or GeneXus Account.\n- It is possible to customize the icon for each assistant. ### February 29th - [Frontend](82.html) improvements in UI/UX.\n- Option to get feedback from end users when interacting with [RAG Assistant](44.html).\n- Gemini Pro LLM support.\n- New [Dashboard](562.html) with user metrics.\n\n# Updates Log This log shows the most important fixes or features added to the platform. ## Log ## 2025 ### April 10th - New [Globant Enterprise AI Lab](972.html) - The Lab is designed for defining, managing, and orchestrating autonomous AI agents. It provides a standardized model for representing agents, their capabilities, and their interactions within complex workflows. The core components of the Lab include: - Agents & Tools: This module allows for the definition and management of individual agents and their resources, such as skills and tools. It serves as a central hub for cataloging and managing the agent workforce. - Agentic Processes: This component enables the definition of processes based on tasks executed by Agents. These workflows facilitate collaboration among agents to achieve larger objectives. More details at [How to create an Agentic Process](985.html). - Agent RunTime: This module provides the execution environment for agentic workflows, where agents perform tasks based on their skills and interact with artifacts, driven by events and the flow of knowledge. - The Lab aims to meet the growing demand for intelligent, self-sufficient AI agents capable of collaborating and solving complex problems with minimal human intervention. It offers a flexible and adaptable model, allowing for the creation and management of a diverse range of agents, from co-pilots working alongside humans to fully automated agents executing complex tasks. Implemented as a module of Globant Enterprise AI, the Lab supports the development of intelligent agents that can work autonomously or in collaboration with humans and other agents.\n- New features in Flows - Agent Integration Component: You can now directly integrate agents created with the AI Lab into a Flow. These agents can be exposed through platforms like WhatsApp, Teams, or Slack. - [File Upload Support](1053.html) from WhatsApp: Flows now support receiving file attachments such as documents, images, audio, and video directly from WhatsApp interactions. - Audio and Video Attachment in Web Chat: The Web Chat component now allows users to attach audio and video files, enhancing the interaction experience.\n- New Features for the Data Analyst Agent - Reduced Configuration Requirements: The setup needed to enable the assistant to respond to a wide range of questions has been minimized. - Enhanced Analysis Module: An additional analysis module has been incorporated to complement the responses with relevant business conclusions and interpretations of the obtained data.\n- New metrics to track processed tokens. - A new view has been added to each project dashboard to monitor token-based consumption. More details at [Tokens tab in Dashboard](965.html). - To enable this feature, you must run the initialization process described in [Reprocessing Requests and Loading the Summary Table](1015.html).\n- LLMs: - New Gemini 2.5 Pro (via providers Vertex AI and Gemini): Gemini 2.5 is Google\u2019s latest reasoning model, engineered to tackle increasingly complex challenges. This model is designed for tasks that demand advanced analytical thinking and robust problem-solving capabilities. More details at [LLM API](663.html). - Migration to Gemini 2.0 series (Vertex AI): Based on recommendations from Vertex AI, we have migrated from the legacy Gemini 1.0 and 1.5 models to the more advanced Gemini 2.0 series, offering improved performance, scalability, and integration capabilities. For comprehensive information, please refer to the [Deprecated Models](Deprecated Models). - New Azure OpenAI models (o1, o1-mini and o3-mini): We have expanded our model availability by introducing these models via Azure, providing the same high-quality capabilities as those offered through the OpenAI provider. - DeepSeek-R1 via AWS Bedrock: Recently added through a Serverless API, DeepSeek-R1 offers reliable inference with a substantial 128K token context window and up to 32K maximum output tokens. - OpenAI's new models with built-in web search tool: These specialized models integrate web search capabilities directly into the Chat Completions API, enabling them to both interpret and execute search queries in real time. - openai/gpt-4o-search-preview - openai/gpt-4o-mini-search-preview - New models - Beta only: - gemini/gemma-3-27b-it: Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 has a large, 128K context window, and multilingual support in over 140 languages. - SambaNova: - DeepSeek-R1: This Provider offers the fastest performance for running DeepSeek, processing up to 198 tokens per second per user, with a 16K token context window. The model is hosted in US data centers; privately and securely. - DeepSeek-V3-0324: This model significantly outperforms its predecessor with enhanced reasoning benchmarks, improved code executability and refined web aesthetics, and superior Chinese writing aligned with the R1 style. It also offers better multi-turn interactive rewriting, translation quality, detailed report analysis, and more accurate function calling. - QwQ-32B: SambaNova provides access to QwQ-32B-Preview, the best open source test-time compute model released by Alibaba. - Llama 4 collection by Meta: - Llama 4 Scout: A 17B-parameter multimodal MoE model with 16 experts that excels in text and image understanding. The model is currently in beta and is available via providers Nvidia, Groq and SambaNova. Via Groq, it supports a 128k tokens context window with fast inference at 460 tokens/sec, while SambaNova Cloud runs at 697 tokens/second/user. - Llama 4 Maverick: Available via Nvidia, this 17-billion-parameter model featuring 128 experts supports a 32k-token context window. ### March 10th - New LLMs: - GPT-4.5 - Claude 3.7 Sonnet (Providers Anthropic, Vertex AI and AWS Bedrock) - Updates in Gemini 2.0 series: - vertex_ai/gemini-2.0-flash-lite-preview-02-05 - vertex_ai/gemini-2.0-flash-thinking-exp-01-21\n- RAG Revision #6 - Support for o3-mini, gpt-4.5-preview, claude-3-7-sonnet-20250219, new DeepSeek, Gemini2* and sambanova LLM providers. - New pinecone provider available for [embeddings](447.html) and [rerankers](638.html). - The [CleanUp](60.html) action message has been corrected to clearly specify that it will permanently delete the RAG Assistant files and update the information in the RDS. - Added [usage](712.html) element on every response. - Improvements when changing the [LLM](69.html)/[Embeddings](68.html) settings; all models and providers are normalized to be selected from standard combo-box items; use the [override mechanism](209.html) if you need other options. - Support for [guardrails](840.html). - New [documentAggregation](298.html) property to decide how sources are grouped and returned.\n- It is possible to [provide feedback](170.html) on the response of the Chat with Data Assistant in the Frontend.\n\n- The new [Evaluation APIs](893.html) introduce key functionalities through three interconnected APIs: [DataSet API](839.html), [Evaluation Plan API](849.html), and [Evaluation Result API](856.html). This version is primarily designed for users with a data science profile and is mainly accessed via APIs, complemented by a series of Jupyter notebooks that demonstrate their use. For a comprehensive guide on how to use these APIs, you can refer to [How to evaluate an AI Assistant](881.html) and the [EvaluationAPITutorial.ipynb](https://wiki.genexus.com/enterprise-ai/wiki?8com.gxwiki.wiki?887,,) notebook, which provide practical examples and code for working through the evaluation process.\n- File attachment support in Flows (version 0.9).\n- Support for Full Story integration in the Workspace/Playground to generate user access statistics in Full Story.\n- In the [LLM API](663.html), for models that have descriptions in the specified languages, the `descriptions` property is included in the Response, which contains the descriptions in the available languages, such as `Spanish`, `English` and `Japanese`.\n- [Data Analyst Assistant 2.0](886.html) version presents important improvements, simplifying the interaction with the data by reducing the main components to just two: **Dispatcher** and **Thinker**. In addition, the metadata structure is automatically generated when loading the datasets, streamlining the setup process. For more information, see [How to create a Data Analyst Assistant](105.html).\n- The option to consult version-specific documentation is now available. Articles with versions show the option \u201cOther document versions\u201d in the header. Clicking on \u201cOther document versions\u201d brings up a menu that allows you to choose between the most recent version (\u201cLatest\u201d) or earlier versions (e.g. \u201c2025-02 or prior\u201d). If you select a version other than \u201cLatest\u201d, a message appears: \u201cThis is not the latest version of this document; to access the latest version, click here\u201d. This message provides a direct link to the most up-to-date documentation. [Components Version Update](822.html) ### February 10th - New documentation with details about [Supported Chart Types](731.html).\n- New [Usage Limits API](802.html).\n- Flows - Logs associated with conversations are accessible from the [conversation history](378.html).\n- RAG Revision #5 - Ingestion supports new [file formats](137.html). For on-premises installations, update the _FILE_TYPES_ parameter. - New optional _cleanProcessing_ parameter when using [Parent Document Retrieval Strategy](204.html). - [Embeddings API](422.html) cache support; disabled by default from a [RAG assistant](68.html). - New [mode](68.html) property for using multimodal embeddings and creating a [Multimodal RAG](783.html).\n- New endpoint [GET /accessControl/apitoken/validate](22.html) returns information about the organization and project associated with the provided apitoken.\n- New LLMs: - Already in production - **OpenAI**: o3-mini - Already in Beta - **DeepSeek**: - deepseek/deepseek-reasoner - deepseek/deepseek-chat - azure/deepseek-r1 - nvidia/deepseek-ai-deepseek-r1 - groq/deepseek-r1-distill-llama-70b - sambanova/DeepSeek-R1-Distill-Llama-70B - Updates in Gemini 2.0 series: - gemini-2.0-flash-thinking-exp-01-21 (Via Providers Gemini and Vertex AI) - gemini/gemini-2.0-flash-lite-preview - gemini/gemini-2.0-pro-exp - vertex_ai/gemini-2.0-flash-001 - sambanova/Llama-3.1-Tulu-3-405B [Components Version Update](822.html) ### January 13th - Internationalization, [Backoffice](42.html), and frontend support for Japanese.\n- Invitations now include information about the organization and project in the subject.\n- New LLMs - Already in Production - **OpenAI**: o1 (2024-12-17 version) - Already in Beta - New Providers: **Cohere** - **Cohere**: Cohere-r\n- [Guardrails](682.html) configured by assistant.\n- [Rerank API](625.html) to semantically order a list of document chunks given a query.\n- New optional RAG [Retrieve and Rerank](620.html) adds an extra layer of precision to ensure that only the most relevant information reaches the model used in the generation step. ## 2024 ### December 12th - [Automatic Creation of Default Assistant](640.html)\n- [Organization Usage Limits](666.html): It is possible to set quota limits to control organization expenses or usage.\n- Chat with Data Assistant - [Show details](170.html) about the generated query in the Playground. - Support in [Chat API](34.html) to interact with Chat with Data Assistant.\n- Flows - Support for markdown when showing the response on the different channels supported by Flows (web, Slack, WhatsApp, and Teams). - New component for connecting flows to the agent overflow console (Human-in-the-loop) via B2Chat. Please read [How to connect a Flow to B2Chat](674.html).\n- RAG - [Chroma](537.html), [Elasticsearch](579.html), [OpenSearch](580.html) vectorstore support. - [Improvements](581.html) when processing PDF documents (update to RAG module revision #4 or higher). - Integration with [AWS Knowledge Bases](577.html) at query time.\n- Data Analyst Assistant - Option to update metadata options. - New version by default in new Data Analyst assistants.\n- New LLMs - OpenAI: gpt-4o-2024-11-20 - AWS Bedrock: Anthropic Claude 3.5 Haiku - Amazon Nova models (Micro, Lite, and Pro) - Llama 3.1 405B on Vertex AI - Beta: - Support for providers Cerebras, SambaNova and xAI (Grok models). - All new Gemini Experimental models.\n- Security - It is possible to define a user 'all-members@domain.com' to give access to the Frontend to all users of the domain '@domain.com'. Permissions are enforced based on the roles assigned to this user. Please read [Allowing Frontend Access for Users from Specific Domains](657.html) ### November 12th - Flows execution integrated into the Playground\n- New [LLMs support](200.html) - **OpenAI**: o1-preview and o1-mini - **Claude Sonnet 3.5 v2 - Providers**: Anthropic, Vertex AI, and AWS Bedrock - **Llama 3.2 models - Providers**: Vertex AI and AWS Bedrock\n- Chat with data assistants - Possibility to edit metadata, [entities](170.html), and attribute descriptions. - The Properties tab has been renamed to [Settings](170.html) along with the options that can be configured in it.\n- RAG - New [returnSourceDocuments](298.html) option to disable returning the documents section used to answer the question. - New [step](298.html) option to use the assistant as a retrieval tool. - Support for [custom history](85.html) in conversations using the _chat_history_ variable.\n- Stand-alone Frontend based on the new Playground UI - Options to customize the Frontend to use the client logo, color palette, welcome message, and descriptions. - Feature to collect feedback (thumbs up/down) in each response. - Google Analytics support.\n- Data Analyst Assistant - Support to upload large CSV files.",
          "evaluationResultRowContextDocId": "68ace6a6-8b1b-4736-a466-9ed0a67b97a9",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "2cbfe7a0-a20d-40f7-9978-9c0c6e8dabde",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?31",
          "evaluationResultRowContextName": "31",
          "evaluationResultRowContextPrecision": 0.0
        },
        {
          "evaluationResultRowContextChunk": "#Data Analyst Assistant 2.0 The Data Analyst Assistant is an artificial intelligence assistant in Globant Enterprise AI that allows the end user to interact with a dataset in a similar way as a professional data analyst would. This assistant offers functions such as data summaries, calculations of maximum, minimum and average values, data organization in tables and report generation. To define an assistant of this type, you must provide the following information: * **Metadata**: A description of each dataset, its columns (including data types and possible values), and considerations for the coder and the interpreter (see [[105##Step 3: Define Metadata (JSON)|Define Metadata (JSON)]]). * **Glossary**: A list of terms used within the company and/or the domain of the end user that would be relevant for the LLM to understand the questions. * **Datasets**: A set of CSV files that follows the descriptions added as metadata. When an end user interacts with the Data Analyst Assistant through the Frontend, Globant Enterprise AI performs a series of tasks to identify the purpose of the question, select the appropriate dataset, and generate code to extract the relevant information and present it in a friendly way. This process is based on the following two components that work together to achieve the full functionality of the Assistant: * **Dispatcher**: The Dispatcher is the component in charge of interacting with the end user and managing the selection of data needed to process the query. Its main functions are to: * **Select the appropriate dataset**: The Dispatcher analyzes the end user's query and chooses the appropriate dataset according to the question asked. * **Generate SQL code**: Once the dataset is selected, the Dispatcher creates a SQL query that extracts the necessary information from the database. To perform these tasks, the Dispatcher relies on the following elements: * **Metadata**: To understand the structure and content of the datasets and generate accurate queries. * **Glossary**: To correctly interpret the terminology used in the query and avoid confusion. * **Thinker**: The Thinker is the component that receives the retrieved data and processes it to generate an understandable response for the end user. Its main functions are to: * **Process the data**: It analyzes the data obtained through the SQL query generated by the Dispatcher. * **Generate the final answer**: From the processed data, the Thinker elaborates a clear and useful answer for the end user. To perform these tasks, the Thinker depends on: * **Metadata**: To understand the structure of the selected dataset and process the data appropriately. * **SQL Query**: To know which subset of data is available for analysis and work with the correct information. * **Examiner**: The Examiner is the component that interprets the answer generated by the thinker and provides an analysis of it. * To enable it, you must add \u201cexaminer_enabled\u201d: 1 in the metadata. It defaults to 0 if not defined. To define or update a Data Analyst Assistant, you must use the [[42]] and follow the steps outlined in [[105]]. {{{12}}} {{{17}}}\n##MARKDOWN##",
          "evaluationResultRowContextDocId": "0f2784fb-9b64-476f-8358-efe3972977fe",
          "evaluationResultRowContextExtention": "md",
          "evaluationResultRowContextId": "9524797c-703c-4db3-9bbf-bcade8e1747d",
          "evaluationResultRowContextLink": "https://wiki.genexus.com/enterprise-ai/wiki?886",
          "evaluationResultRowContextName": "886",
          "evaluationResultRowContextPrecision": 0.0
        }
      ],
      "dataMetrics": [
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.5,
          "systemMetricId": "2b32f1ca-3344-4bbf-8f93-6db882a9f946"
        },
        {
          "evaluationResultRowMetricFeedback": "",
          "evaluationResultRowMetricResult": "",
          "evaluationResultRowMetricScore": 0.8,
          "systemMetricId": "8fd20b5c-68a7-42fa-afad-45d17350172f"
        }
      ],
      "dataSetRowId": "a75ace49-98da-4d6f-9f1d-10c2229a0589",
      "evaluationResultRowContextScoreAverage": 0.0,
      "evaluationResultRowCost": "0.0046622600",
      "evaluationResultRowEndDate": "2025-05-12T17:00:43",
      "evaluationResultRowOutput": "To use the Data Analyst Assistant 2.0 in Globant Enterprise AI, you follow these main steps:\n\n1. **Create a New Assistant**\n   - Go to the Globant Enterprise AI Backoffice.\n   - Navigate to the \"Assistants\" menu.\n   - Click on \"CREATE DATA ANALYST ASSISTANT\".\n   - Select the project (like the Default one) where you want to create the assistant.\n\n2. **Add Dataset Files**\n   - Upload CSV dataset files using the \"+Add files...\" button.\n   - Use lowercase letters and underscores in filenames (e.g., `this_notation.csv`).\n\n3. **Define Metadata (JSON)**\n   - Metadata describes the datasets and their columns, including data types and descriptions.\n   - This metadata helps the assistant understand the structure of your data and how to process it.\n   - You define JSON metadata including dataset name, description, column explanations, and considerations for components like Dispatcher and Thinker.\n   - Column names must follow specific rules (start with a letter, lowercase, underscores instead of spaces, no \"id\" column because it's predefined).\n   - Define datatype for each column (`string`, `float`, `integer`, `datetime` with specific formats, `bool`, etc.)\n\n4. **Define Glossary (JSON)**\n   - Create a glossary JSON that includes domain-specific or company-specific terms and their definitions.\n   - This helps the language model correctly interpret user queries with specialized language or abbreviations.\n\n5. **Save Assistant**\n   - After defining metadata and glossary, click \"SAVE\" to create the assistant.\n\n6. **Fill in Assistant Details**\n   - Provide a name, description, and icon for the assistant.\n   - Click \"CONFIRM\".\n\n7. **Wait for Upload and Processing**\n   - On the Assistants page, you will see the status of dataset upload and metadata processing. The assistant will transition from PROCESSING to COMPLETED.\n\n8. **Test Your Assistant**\n   - Use the Playground in the Backoffice to interact with your assistant.\n   - You can ask questions about the data as if you were talking to a professional data analyst.\n   - The assistant will analyze your question, select the appropriate dataset, generate the SQL query, process results, and provide clear answers.\n\nAdditionally, you can:\n- **Extend datasets </B>by adding more records to existing datasets (without changing metadata or glossary).\n- **Replace datasets </B>to update data and modify metadata or glossary.\n- **Update prompts </B>to adjust predefined behaviors, although prompts are mostly controlled through metadata and glossary now.\n\nThe assistant internally works in two main components:\n- **Dispatcher**: Interprets user query, selects dataset, generates SQL query.\n- **Thinker**: Processes SQL results and generates the user-friendly answer.\n\nFor more detailed instructions, you can refer to the step-by-step guide on creating and using the Data Analyst Assistant [here](https://wiki.genexus.com/enterprise-ai/wiki?105) and the assistant concept explained [here](https://wiki.genexus.com/enterprise-ai/wiki?886).\n\n---\n\n**References:**\n\n- How to create and use Data Analyst Assistant (Steps 1 to 8): [105](https://wiki.genexus.com/enterprise-ai/wiki?105)  \n- Data Analyst Assistant 2.0 overview and components: [886](https://wiki.genexus.com/enterprise-ai/wiki?886)",
      "evaluationResultRowStartDate": "2025-05-12T17:00:28",
      "evaluationResultRowStatus": "F",
      "evaluationResultRowTokens": "9719"
    }
  ]
}