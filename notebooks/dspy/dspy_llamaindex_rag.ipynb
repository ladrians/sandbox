{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "456a046f",
   "metadata": {},
   "source": [
    "# DSPy sandbox\n",
    "\n",
    "Initial notebook using the references from [here](https://x.com/jerryjliu0/status/1805626753551155243), complemented from [here](https://github.com/stanfordnlp/dspy/blob/main/intro.ipynb).\n",
    "\n",
    "## Brief Intro\n",
    "\n",
    "DSPy is a new framework for developing LLM programs. It chains LLM calls to build robust systems, where the output of one LLM call becomes the input of the next. Each LLM call acts as a function that takes text as input and produces text as output.\n",
    "DSPy is a new programming model inspired by PyTorch that provides significant control over LLM programs. The Signature abstraction streamlines LLM program codebases by encapsulating prompts and structured input/output data. DSPy's compiler optimizes instructions for each LLM program component while sourcing task examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dspy-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "## Original using a specific version\n",
    "# !pip install llama-index==0.10.44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b85eae8",
   "metadata": {},
   "source": [
    "**DSPy** can be used for various tasks (QA, information extraction, text-to-SQL); the general workflow is:\n",
    "\n",
    "1. **Collect a little bit of data.** Define examples of the inputs and outputs of your program (e.g., questions and their answers). This could just be a handful of quick examples you wrote down. If large datasets exist, the more the merrier!\n",
    "\n",
    "1. **Write your program.** Define the modules (i.e., sub-tasks) of your program and the way they should interact together to solve your task.\n",
    "1. **Define some validation logic.** What makes for a good run of your program? Maybe the answers need to have a certain length or stick to a particular format? Specify the logic that checks that.\n",
    "1. **Compile!** Ask **DSPy** to _compile_ your program using your data. The compiler will use your data and validation logic to optimize your program (e.g., prompts and modules) so it's efficient and effective!\n",
    "1. **Iterate.** Repeat the process by improving your data, program, validation, or by using more advanced features of the **DSPy** compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef7619",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Minimal sample coding\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849dbd89-ce04-4a18-84fb-c19f3db5504a",
   "metadata": {},
   "source": [
    "# Building optimized RAG with LlamaIndex + DSPy\n",
    "\n",
    "This notebook provides a comprehensive overview of LlamaIndex + DSPy integrations.\n",
    "\n",
    "We show **three** core integrations:\n",
    "1. **Build and optimize Query Pipelines with DSPy predictors**: The first section shows you how to write DSPy code to define signatures for LLM inputs/outputs. Then port over these components to overall workflows within LlamaIndex Query pipelines, and then end-to-end optimize the entire system.\n",
    "\n",
    "2. **Build and optimize Query Pipelines with Existing Prompts**: Instead of writing DSPy signatures, you can just define a LlamaIndex prompt template, and our converter will auto-optimize it for you.\n",
    "\n",
    "3. **Port over DSPy-Optimized Prompts to any LlamaIndex Module**: Possible through our `DSPyPromptTemplate` - translate an optimized prompt through DSPy into any module that requires prompts in LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d223313-af5b-4155-8896-c24aa4cb6925",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Define the LLM setting for DSPy (note: this is separate from using the LlamaIndex LLMs), and also the answer signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c50d5e-7046-40c5-bf3e-a8d2a2a2c6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "turbo = dspy.OpenAI(model='gpt-3.5-turbo')\n",
    "dspy.settings.configure(lm=turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d8c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68be4f-36b1-4054-99dd-707ce42f61a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context_str = dspy.InputField(desc=\"contains relevant facts\")\n",
    "    query_str = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2a981-2ee1-497a-b675-15e1029183f6",
   "metadata": {},
   "source": [
    "## [Part 1] Build and Optimize a Query Pipeline with DSPy Modules\n",
    "\n",
    "Use our DSPy query components to plugin DSPy prompts/LLMs, stitch together with our query pipeline abstraction.\n",
    "\n",
    "Any query pipeline can be plugged into our `LlamaIndexModule`. We can then let DSPy optimize the entire thing e2e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f0637-993f-4bef-bd8a-1122951cf7f8",
   "metadata": {},
   "source": [
    "#### Load Data, Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c9d434-c8f6-403e-9499-36059eb09907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# port it over to another index  (paul graham example) \n",
    "\n",
    "!wget https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt -O paul_graham_essay.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367a7985-2f89-47be-b2af-14fef2e845c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=[\"paul_graham_essay.txt\"])\n",
    "docs = reader.load_data()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a68920-6517-465b-8c0c-4c6e0b8390a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d43c6c-9dee-4cbc-b39d-5d8ad617b6ea",
   "metadata": {},
   "source": [
    "#### Build Query Pipeline\n",
    "\n",
    "Replace the synthesis piece with the DSPy component (make sure GenerateAnswer matches signature of inputs/outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7df48b-9369-4f17-8542-0f8afd9e621e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline as QP, InputComponent, FnComponent\n",
    "from dspy.predict.llamaindex import DSPyComponent, LlamaIndexModule\n",
    "\n",
    "dspy_component = DSPyComponent(\n",
    "    dspy.ChainOfThought(GenerateAnswer)\n",
    ")\n",
    "\n",
    "retriever_post = FnComponent(\n",
    "    lambda contexts: \"\\n\\n\".join([n.get_content() for n in contexts])\n",
    ")\n",
    "\n",
    "\n",
    "p = QP(verbose=True)\n",
    "p.add_modules(\n",
    "    {\n",
    "        \"input\": InputComponent(),\n",
    "        \"retriever\": retriever,\n",
    "        \"retriever_post\": retriever_post,\n",
    "        \"synthesizer\": dspy_component,\n",
    "    }\n",
    ")\n",
    "p.add_link(\"input\", \"retriever\")\n",
    "p.add_link(\"retriever\", \"retriever_post\")\n",
    "p.add_link(\"input\", \"synthesizer\", dest_key=\"query_str\")\n",
    "p.add_link(\"retriever_post\", \"synthesizer\", dest_key=\"context_str\")\n",
    "\n",
    "\n",
    "dspy_qp = LlamaIndexModule(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b457f1db-3315-476c-8673-bf19ad6e1657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = dspy_qp(query_str=\"what did the author do in YC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b78501f-e281-4e68-bdb6-1eeca6bf2464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d5d86-1ce0-46fd-9fbc-eca68043c935",
   "metadata": {},
   "source": [
    "#### Optimize Query Pipeline\n",
    "\n",
    "Let's try optimizing the query pipeline with few-shot examples.\n",
    "\n",
    "We define a toy dataset with two examples. We then use our `SemanticSimilarityEvaluator` to define a custom eval function to pass to the DSPy teleprompter.\n",
    "- Because our passing threshold is set to very low, every example should pass with a reasonable LLM. \n",
    "- What this practically means is that all training examples will be added as few-shot examples to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a783c-2025-4b1c-9f5b-cdcde7211ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dspy import Example\n",
    "\n",
    "train_examples = [\n",
    "    Example(query_str=\"What did the author do growing up?\", answer=\"The author wrote short stories and also worked on programming.\"),\n",
    "    Example(query_str=\"What did the author do during his time at YC?\", answer=\"organizing a Summer Founders Program, funding startups, writing essays, working on a new version of Arc, creating Hacker News, and developing internal software for YC\")\n",
    "]\n",
    "\n",
    "train_examples = [t.with_inputs(\"query_str\") for t in train_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43a18c-5569-4947-a02d-08f52331b134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502cc403-8ebc-4bbf-be7c-c15385909b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "from llama_index.core.evaluation import SemanticSimilarityEvaluator\n",
    "\n",
    "evaluator = SemanticSimilarityEvaluator(similarity_threshold=0.5)\n",
    "\n",
    "# Validation logic: check that the predicted answer is correct.\n",
    "# Also check that the retrieved context does actually contain that answer.\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    result = evaluator.evaluate(response=pred.answer, reference=example.answer)\n",
    "    return result.passing\n",
    "\n",
    "# Set up a basic teleprompter, which will compile our RAG program.\n",
    "teleprompter = BootstrapFewShot(max_labeled_demos=0, metric=validate_context_and_answer)\n",
    "\n",
    "# Compile!\n",
    "compiled_dspy_qp = teleprompter.compile(dspy_qp, trainset=train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a147c3e-febe-43fa-bca5-152432d3662b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test this out \n",
    "compiled_dspy_qp(query_str=\"How did PG meet Jessica Livingston?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a2ba4e-6677-44e7-a474-db382ad1a56a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [optional]: inspect history\n",
    "turbo.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895ceff-5aeb-4285-8ede-99bb35a50a45",
   "metadata": {},
   "source": [
    "## [Part 2] Build and Optimize Query Pipelines with Existing Prompts\n",
    "\n",
    "Build a query pipeline similar to the previous section. But instead of directly using DSPy signatures/predictors, we can build DSPyComponent modules from LlamaIndex prompts directly. \n",
    "\n",
    "This allows you to write any LlamaIndex prompt and trust that it'll be optimized in DSPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efff3a9-77d9-4a89-a0f7-207d63e73ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "# let's try a fun prompt that writes in Shakespeare! \n",
    "qa_prompt_template = PromptTemplate(\"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, \\\n",
    "answer the query.\n",
    "\n",
    "Write in the style of a Shakespearean sonnet.\n",
    "\n",
    "Query: {query_str}\n",
    "Answer: \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84aa7b2-b9b8-4740-b862-9073470c31c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline as QP, InputComponent, FnComponent\n",
    "from dspy.predict.llamaindex import DSPyComponent, LlamaIndexModule\n",
    "\n",
    "dspy_component = DSPyComponent.from_prompt(qa_prompt_template)\n",
    "\n",
    "retriever_post = FnComponent(\n",
    "    lambda contexts: \"\\n\\n\".join([n.get_content() for n in contexts])\n",
    ")\n",
    "\n",
    "\n",
    "p = QP(verbose=True)\n",
    "p.add_modules(\n",
    "    {\n",
    "        \"input\": InputComponent(),\n",
    "        \"retriever\": retriever,\n",
    "        \"retriever_post\": retriever_post,\n",
    "        \"synthesizer\": dspy_component,\n",
    "    }\n",
    ")\n",
    "p.add_link(\"input\", \"retriever\")\n",
    "p.add_link(\"retriever\", \"retriever_post\")\n",
    "p.add_link(\"input\", \"synthesizer\", dest_key=\"query_str\")\n",
    "p.add_link(\"retriever_post\", \"synthesizer\", dest_key=\"context_str\")\n",
    "\n",
    "\n",
    "dspy_qp = LlamaIndexModule(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b7662-cb87-4906-bbaf-e2ffd74e20d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the inferred signature\n",
    "dspy_component.predict_module.signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044855f-cdd2-43f0-9d2e-e7ba64a28e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "from llama_index.core.evaluation import SemanticSimilarityEvaluator\n",
    "from dspy import Example\n",
    "\n",
    "output_key = \"sonnet_answer\"\n",
    "train_example_dicts = [\n",
    "    {\"query_str\": \"What did the author do growing up?\", output_key: \"The author wrote short stories and also worked on programming.\"},\n",
    "    {\"query_str\": \"What did the author do during his time at YC?\", output_key: \"organizing a Summer Founders Program, funding startups, writing essays, working on a new version of Arc, creating Hacker News, and developing internal software for YC\"}\n",
    "]\n",
    "train_examples = [Example(**t).with_inputs(\"query_str\") for t in train_example_dicts]\n",
    "\n",
    "evaluator = SemanticSimilarityEvaluator(similarity_threshold=0.5)\n",
    "# Validation logic: check that the predicted answer is correct.\n",
    "# Also check that the retrieved context does actually contain that answer.\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    result = evaluator.evaluate(response=getattr(pred, output_key), reference=getattr(example, output_key))\n",
    "    return result.passing\n",
    "\n",
    "# Set up a basic teleprompter, which will compile our RAG program.\n",
    "teleprompter = BootstrapFewShot(max_labeled_demos=0, metric=validate_context_and_answer)\n",
    "\n",
    "# Compile!\n",
    "compiled_dspy_qp = teleprompter.compile(dspy_qp, trainset=train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fcb28c-c442-4f1d-8876-9c6c6c47eda0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test this out \n",
    "compiled_dspy_qp(query_str=\"How did PG meet Jessica Livingston?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b1b5f7-66d0-49d2-9bae-ca94f12a124d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [optional]: inspect the optimized prompt \n",
    "turbo.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4daae77-bf36-418f-9a80-c9c0fb6f20ec",
   "metadata": {},
   "source": [
    "## [Part 3] Port over Optimized Prompts to LlamaIndex using the DSPy Prompt Template\n",
    "\n",
    "Extract out a prompt from an existing compiled DSPy module, and then port it over to any LlamaIndex pipeline! \n",
    "\n",
    "In the example below we use our `DSPyPromptTemplate` to extract out the compiled few-shot prompt from the optimized query pipeline. \n",
    "\n",
    "We then plug it into a separate query engine over the PG essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180ea2b-412a-41a6-a707-f2945d9dcda0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dspy.predict.llamaindex import DSPyPromptTemplate\n",
    "\n",
    "# NOTE: you cannot do DSPyPromptTemplate(dspy_component.predict_module) - the predict_module is replaced.\n",
    "qa_prompt_tmpl = DSPyPromptTemplate(compiled_dspy_qp.query_pipeline.module_dict[\"synthesizer\"].predict_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c86e63-f225-4540-b1a2-a48a13e29756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(qa_prompt_tmpl.format(query_str=\"hello?\", context_str=\"this is my context\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3097d9c-89e3-4d89-ae8d-6a57bd4fda0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    text_qa_template=qa_prompt_tmpl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b8d58-ddb0-468c-854f-bf475134d653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"what did the author do at RISD?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93b21d-2766-404e-833f-5a1cb3bf8891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
